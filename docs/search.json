[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC1000 2026",
    "section": "",
    "text": "Preface\nStatistics is, at its heart, the language of evidence. In business and economics, it helps us make sense of uncertainty, identify patterns, and make better decisions — not by removing doubt, but by quantifying it. ETC1000: Business and Economic Statistics was designed to introduce students to this way of thinking: one grounded in data, guided by logic, and motivated by real-world questions.\nThis book serves as a companion for the ETC1000 unit, built on the principle that learning statistics is best achieved through doing, interpreting, and communicating — not memorising formulas. You’ll find that each chapter begins with a question or scenario that brings statistical thinking to life: from understanding whether a marketing campaign really boosted sales, to determining if housing prices differ between suburbs, or evaluating the fairness of a hiring process. These are the kinds of questions that analysts, economists, and business leaders face every day — and the kinds of skills you’ll develop throughout this unit.\nAcross the semester, we journey from descriptive to inferential statistics, from raw data to regression modelling, from spreadsheets to reproducible analysis in R. But more importantly, we move from numbers to stories — learning how to interpret, visualise, and explain statistical findings in ways that influence real decisions. This integration of computation and communication is at the heart of what it means to be data-literate in the modern world.\nIn this book, each chapter is designed to build naturally on the one before it, guiding you through the process of solving real-world problems using data. We usually begin with a clearly defined problem—this might be a research question, a business decision, or a practical scenario where evidence is needed to support an answer. From there, we introduce the statistical ideas, tools, and techniques that help move from raw data to meaningful conclusions:\nETC1000 has evolved over many years with input from hundreds of students and teaching staff. Their curiosity, feedback, and creativity have shaped the examples, visuals, and teaching strategies in these pages. Whether you’re studying statistics for the first time or revisiting it after years away, I hope this book helps you see statistics not as a hurdle to clear, but as a toolkit for discovery — one that empowers you to explore, question, and make sense of a complex world through data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html",
    "href": "01-Introduction-to-statistics.html",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "",
    "text": "1.1 Why Statistics?\nOne of the first questions I like to ask when I take on a new unit is:\nIn this case … why is statistics a core part of your degree?\nFor those majoring in econometrics or business analytics, the answer might seem obvious. But many of you come from other sub-disciplines—marketing, management, banking and finance, etc., — and yet, you’re all required to study statistics.\nSo what is it about statistics that makes it essential across so many fields? To answer this question, read the case study regarding the NASA Challenger Disaster below.\nSo I come back to the question:\nNot everyone here is going to work at NASA or be in a role where your decisions could cost lives.\nHowever, every one of you will be expected to make decisions using data — whether you’re managing a budget, launching a product, analyzing customer feedback, or pitching a new strategy to stakeholders. Inaccurate assumptions, poor interpretation of trends, or blindly trusting a single number without questioning where it came from—these mistakes won’t make the news like the Challenger disaster did, but they can cost your company money, credibility, or opportunity. And more importantly, they can damage your own confidence and reputation as a professional.\nStatistics helps you avoid that. It teaches you not just how to calculate but how to think: how to challenge claims, test assumptions, and make sense of messy real-world data. That’s why this unit matters — not just for your degree, but for your ability to make smarter, more informed decisions, wherever your career takes you.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#why-statistics",
    "href": "01-Introduction-to-statistics.html#why-statistics",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "",
    "text": "Why this unit?\n\n\n\n\n\n\n\n\n\n\nThe NASA Challenger disaster\n\n\n\nOn January 28, 1986, the space shuttle Challenger broke apart just 73 seconds after liftoff, killing all seven astronauts on board. The tragedy remains one of the most devastating examples of how misunderstanding data and poor statistical reasoning can have fatal consequences.\n\n\n\n\nPrior to the launch, engineers from Morton Thiokol, the contractor responsible for the shuttle’s O-ring seals, were concerned about the effects of cold temperature on the O-rings’ ability to prevent hot gases from escaping during launch. They had data from previous shuttle launches showing the number of O-ring failures (or “erosions”) and the temperature at launch.\n\n\n\n\n\n\n\n\n\nUnfortunately, when Thiokol engineers presented their findings to NASA, the data were plotted incorrectly. They graphed the number of O-ring failures only for the launches that had experienced a failure—completely omitting all the launches where there were no failures. This meant that the plot showed no clear pattern, appearing to suggest that O-ring performance was random and unrelated to temperature. If the engineers had instead plotted all launches—including those with zero O-ring problems—against temperature (see plot below), the relationship would have been obvious:\n\nas the temperature dropped, the probability of O-ring failure increased sharply.\n\n\n\n\n\n\n\n\n\n\nThe Challenger launch occurred at 51°F, far colder than any previous launch, placing it in the highest-risk zone of all.\nDespite engineers’ warnings and uncertainty, NASA decided to proceed with the launch under immense schedule and political pressure. The misunderstanding of the statistical evidence—and the lack of clear data visualization—played a major role in the decision. When the Challenger lifted off, the cold temperature caused one of the O-rings to fail, leading to the explosion that destroyed the shuttle.\nIn short, if the data had been plotted correctly, the evidence against launching in such cold conditions would have been undeniable. Tragedy might have been averted through better understanding and communication of statistical relationships.\n\n\n\n\nWhy statistics?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#let-him-cook",
    "href": "01-Introduction-to-statistics.html#let-him-cook",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.2 Let him cook",
    "text": "1.2 Let him cook\n\nStatistics is a lot like cooking.\n\nSuppose I want to bake a cake for Mother’s Day.\n\nFirst, I prepare the ingredients.\nThen, I cook them — in this case, using an oven.\nFinally, I (hopefully) end up with the desired end product: a cake.\n\nNow imagine that instead of using an oven, I decided to use a deep fryer. Would I still get a cake? Probably not! The same goes for using a steamer, a blender, or a grill — each appliance is designed for a specific purpose and produces a different result.\nTo make a good cake, I need both the right ingredients and the right tool. Using the wrong one can lead to an unexpected or flawed outcome.\n\nThe same principle applies in statistics. Our tools are the various statistical tests and models available to us. Each is suited to a particular type of problem or research question. Our ingredients are the data we have — their type, structure, and quality.\n\nIf we use the wrong statistical tool for the data at hand, our results may be misleading or invalid, just like trying to bake a cake in a deep fryer.\n\n1.2.1 The Ingredients: Variables\nIn statistics, our ingredients are called variables. A variable is simply something that can vary — it can take on different values across people, objects, or situations. For example,\n\nIf we’re studying athletes, variables might include height, training hours, or reaction time.\nIn a business context, they might be sales revenue, customer satisfaction, or number of website visits.\n\nSome variables are numeric — they involve numbers that can be measured (like weight or income). Others are categorical, describing qualities or groups (like gender, position played, or brand preference). Knowing which kind of variable we have is like knowing whether we’re working with flour or sugar — it determines what “statistical recipe” we should follow.\nIf we treat every variable the same way, we might end up “deep-frying” our data instead of baking it. The secret to good statistical practice is understanding what each variable represents and choosing the right method to work with it.\n\n\n1.2.2 Numerical and Categorical Data\nIn this unit, we simplify our variables down to two types of data:\n\nNumerical\nCategorical\n\nIn analysing data, it is important to be aware of the different types of data. Different sorts of analyses are appropriate for different types of data (see section above on Cooking!). The following categorisation of data will come up throughout the unit:\n\n\nTypeDescriptionExampleNumericData that takes a numeric value.Height, Weight, Income, Exam scoreCategoricalData that can be classified into distinct categories.Country of birth, Eye colour, Employment status\n\n\nWhen distinguishing between data types, a useful strategy is to think about what the units of measurement are — or whether there even are any!\nIf the data have a clear unit (like meters, kilograms, seconds, or dollars), they’re most likely numerical — they involve quantities that can be measured and compared using a number. For instance, a runner’s time in seconds or a student’s test score both have meaningful numerical scales and units.\nIf there are no meaningful units — for example, if the data describe categories like “left-hand/right-hand,” “Yes/No,” or “Apple/Orange/Banana” — then we’re dealing with categorical data. These represent qualities or groupings rather than measurable amounts.\nThinking in terms of units helps guide what analyses are appropriate. You can’t sensibly take an average of “Apple” or “Blue,” but you can average numbers measured in kilograms or seconds. In other words, identifying the presence (or absence) of units tells you whether the data are about how much or what kind.\n\n\n\n\n\n\nPractice Exercise 1.1\n\n\n\nConsider the variables below and choose the type of data associated with that variable:\n\n\n\n\n\n\n\nVariable\nType\n\n\nAnnual Income\nNumericalCategorical\n\n\nEmployment Status\nNumericalCategorical\n\n\nMode of transportation\nNumericalCategorical\n\n\nTime spent at Company\nNumericalCategorical\n\n\nHighest level of Education\nNumericalCategorical",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#understanding-categorical-data",
    "href": "01-Introduction-to-statistics.html#understanding-categorical-data",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.3 Understanding Categorical data",
    "text": "1.3 Understanding Categorical data\nThis week, we will focus our attention towards understanding categorical data analysis. To begin, read through the scenario below, which relates to two categorical variables.\n\n\n\n\n\n\nExercise and Medical Conditions\n\n\n\nSuppose we had some qualitative information on 5,000 people in Australia including information on whether the individual has been diagnosed with a particular medical condition and the amount of exercise they do.\n\nIndividuals were asked to indicate their primary medical condition out of; ‘Asthma’, ‘Cancer’, ‘Depression’, ‘Diabetes’, ‘Heart Disease’ or ‘None of Above’.\nThey were also categorised by the amount of exercise they did each week; either ‘Moderate’ or ‘Minimal’ exercise.\n\nNote: In the real world, people can have multiple medical conditions at a given time. To keep this example simple, we will assume each person in this data set only has one condition.\n\n\nSource: https://www.lifelinehealthcarebd.org\n\n\n\n\n1.3.1 Tabular format\nThe table below is a snapshot of the data (showing just the first 8 people in the sample).\n\n\nPerson_IDConditionExercise1NoneModerate2Heart DiseaseModerate3DiabetesModerate4NoneModerate5NoneMinimal6NoneModerate7AsthmaMinimal8NoneModerate\n\n\nFor each person, we have collected two pieces of information: Condition and Exercise. It’s common the assign our subjects an ID number to make analysis more manageable (here: 1 to 8). Data that is arranged like the table above is considered data in tabular format, where each row corresponds to data for a single person. For example:\n\nPerson 1 has no medical conditions, and does moderate exercise.\nPerson 2 has heart disease and does moderate exercise.\nPerson 7 has asthma and does minimal exercise.\n\nBefore you do any sort of analysis, it’s important to have your data in tabular format.\n\n\n1.3.2 Visualising categories\nIt’s often a good idea to start all analyses by visualising the data. When dealing with categorical data, it’s useful to obtain the frequencies (or counts) for your variable(s) of interest.\nFor example, looking at our snapshot table from above, but only focusing on the Conditon variable, we can obtain the frequencies for each condition in this snapshot:\n\n\n\n\n\n\n\nPractice Exercise 1.2\n\n\n\nComplete the frequency table for the second variable, Exercise:\n\n\nExerciseModerateModerateModerateModerateMinimalModerateMinimalModerate\n\n\n\n\n\n\n\n\n\nExercise\nFrequency\n\n\n\n\nMinimal\n\n\n\nModerate\n\n\n\n\n\n\n\n\n1.3.3 Bar charts\nAnother way to visualise your data is with plots. When plotting frequencies / counts, bar charts are often a good first choice. The height of the bars represent the counts – therefore it’s easy for people to quickly look at the plot and see which groups are high / low.\n\n\n\n\n\n\n\n\n\n\n\n1.3.4 Percentage plots\nWhen dealing with categorical data, sometimes* it’s useful to think of things in terms of percentages (looking at the tables above, why would percentages be a bad idea for small samples)?\n\n\n\n\n\n\n\n\n\n*For condition, one person is 12.5% of the data! This can be very misleading if we don’t know the raw counts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#contingency-tables",
    "href": "01-Introduction-to-statistics.html#contingency-tables",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.4 Contingency Tables",
    "text": "1.4 Contingency Tables\nIn the previous section we looked at the variables one at a time. But we can also look at the variables as pairs (i.e. both at the same time). This is referred to as a contingency table or a cross tabulation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n1\n0\n1\n\n\n    Diabetes\n0\n1\n1\n\n\n    Heart Disease\n0\n1\n1\n\n\n    None\n1\n4\n5\n\n\nTotal\n2\n6\n8\n\n\n\n\n\n\n\nWe can read this table both column-wise and row-wise. For example:\n\nOf the 6 people who do Moderate exercise:\n\n0 have Asthma\n1 has Diabetes\n1 has Heart Disease\n4 have no condition\n\nOf the 5 people who have no condition:\n\n1 does Minimal exercise\n4 does Moderate Exercise\n\n\n\n\n\n\n\n\nPractice Exercise 1.3\n\n\n\nLet’s now have a look at the full data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n260\n166\n426\n\n\n    Cancer\n256\n125\n381\n\n\n    Depression\n323\n170\n493\n\n\n    Diabetes\n146\n56\n202\n\n\n    Heart Disease\n220\n43\n263\n\n\n    None of Above\n2,011\n1,224\n3,235\n\n\nTotal\n3,216\n1,784\n5,000\n\n\n\n\n\n\n\n\nIn this sample, how many people have diabetes?\nOf those who have diabetes, how many do minimal exercise?\nOf those who do minimal exercise, how many have diabetes?\nWhat are the percentages for B. and C.?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n202\n146\n146\n72.28% and 4.54%, respectively\n\n\n\n\n\n\nFrom Question d above we can see that we need to be mindful of how we are interpreting contingency tables (row-wise or column-wise). You might have the same frequency in the table cells; however, the percentages can be completely different!\n\n\n\n\n\n\nCounts and Percentages (row-wise)\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n260 (61%)\n166 (39%)\n426 (100%)\n\n\n    Cancer\n256 (67%)\n125 (33%)\n381 (100%)\n\n\n    Depression\n323 (66%)\n170 (34%)\n493 (100%)\n\n\n    Diabetes\n146 (72%)\n56 (28%)\n202 (100%)\n\n\n    Heart Disease\n220 (84%)\n43 (16%)\n263 (100%)\n\n\n    None of Above\n2,011 (62%)\n1,224 (38%)\n3,235 (100%)\n\n\nTotal\n3,216 (64%)\n1,784 (36%)\n5,000 (100%)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounts and Percentages (column-wise)\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n260 (8.1%)\n166 (9.3%)\n426 (8.5%)\n\n\n    Cancer\n256 (8.0%)\n125 (7.0%)\n381 (7.6%)\n\n\n    Depression\n323 (10%)\n170 (9.5%)\n493 (9.9%)\n\n\n    Diabetes\n146 (4.5%)\n56 (3.1%)\n202 (4.0%)\n\n\n    Heart Disease\n220 (6.8%)\n43 (2.4%)\n263 (5.3%)\n\n\n    None of Above\n2,011 (63%)\n1,224 (69%)\n3,235 (65%)\n\n\nTotal\n3,216 (100%)\n1,784 (100%)\n5,000 (100%)\n\n\n\n\n\n\n\nTo extend this idea, let’s see how the percentages differ when you divide the raw frequencies by different elements.\n\nIn each of these % tables, we can see how dividing by either the row, grand or column total will yield us different results. Note: we’ll be talking about conditional probabilities soon, so it’s better to have these in decimal format, rather than percentages.\nUsing these values, we can now talk about three types of probabilities:\n\nMarginal probabilities are our row and column probabilities.\n\nFor example, the probability that a randomly selected person does moderate exercise, is Pr(Moderate) = 0.3568\nAnd, the probability that a randomly selected person has diabetes, is Pr(Diabetes) = 0.0404\n\nThe cell values (excluding totals) within a contingency table tell us about the joint (or intersection) probabilities.\n\nFor example, the probability that a randomly selected person has diabetes AND does minimal exercise, Pr(Diabetes ∩ Minimal) = 0.0292\nAnd, the probability that a randomly selected person has diabetes AND does moderate exercise, Pr(Diabetes ∩ Moderate) = 0.0112\n\nConditional probabilities represent the probability of an event conditional on another event (and are our cell values divided by the row or column total).\n\nFor example, the probability someone has diabetes given they do minimal exercise, is Pr(Diabetes | Minimal) = 0.0454\nAnd, the probability someone has diabetes given they do moderate exercise, is Pr(Diabetes | Moderate) = 0.0314\n\n\nNote: We can also look at conditional probabilities row-wise as well.\n\nFor example, the probability someone does minimal exercise given they have Diabetes, is Pr(Minimal | Diabetes) = 0.7228\nAnd, the probability someone does moderate exercise given they have Diabetes, is Pr(Moderate | Diabetes) = 0.2772\n\n\n\n\n\n\n\nConditional Probability Formula\n\n\n\nMathematically, conditional probabilities can be derived from the contingency table and the following formula:\n\\[Pr(A|B)=\\frac{Pr(A ∩ B)}{Pr(B)}\\] For example, suppose we wanted to determine the probability that someone has diabetes given they do minimal exercise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n260\n166\n426\n\n\n    Cancer\n256\n125\n381\n\n\n    Depression\n323\n170\n493\n\n\n    Diabetes\n146\n56\n202\n\n\n    Heart Disease\n220\n43\n263\n\n\n    None of Above\n2,011\n1,224\n3,235\n\n\nTotal\n3,216\n1,784\n5,000\n\n\n\n\n\n\n\nThis becomes:\n\\[Pr(Diabetes|Minimal)=\\frac{Pr(Diabetes ∩ Minimal)}{Pr(Minimal)}\\] \\[Pr(Diabetes|Minimal)=\\frac{146/5000}{3216/5000}\\] \\[Pr(Diabetes|Minimal)=0.0454\\]\n(Note: if you look at the “Divide by Column Totals” table above, you’ll see that this matches the answer here)\n\n\n\n\n\n\n\n\nPractice Exercise 1.4\n\n\n\nUsing the formula provide above, determine Pr(None of Above | Moderate).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\nTotal\n\n\nMinimal\nModerate\n\n\n\n\nCondition\n\n\n\n\n\n\n\n\n    Asthma\n260\n166\n426\n\n\n    Cancer\n256\n125\n381\n\n\n    Depression\n323\n170\n493\n\n\n    Diabetes\n146\n56\n202\n\n\n    Heart Disease\n220\n43\n263\n\n\n    None of Above\n2,011\n1,224\n3,235\n\n\nTotal\n3,216\n1,784\n5,000\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\\[Pr(None|Moderate)=\\frac{Pr(None ∩ Moderate)}{Pr(Moderate)}\\] \\[Pr(None|Moderate)=\\frac{1224/5000}{1784/5000}\\] \\[Pr(None|Moderate)=0.6861\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#independence",
    "href": "01-Introduction-to-statistics.html#independence",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.5 Independence",
    "text": "1.5 Independence\nIn statistics, two categorical variables are said to be independent if the distribution of one variable does not depend on the other. In other words, knowing the value of one variable gives you no information about the value of the other. For example, if exercise habits are independent of work type, then the proportion of people who exercise regularly would be the same for both office-based and non-office workers.\nMathematically, there are a few different ways to check for independence. Two methods are shown below using our existing data set (suppose we want to check if having diabetes and amount of exercise is independent).\n\n\n\n\n\n\nChecking Independence (Method 1)\n\n\n\nOne method is to use the formula:\n\\[Pr(A|B_{1})=Pr(A|B_{2})=Pr(A)\\]\nIn this example,\n\nlet A = Diabetes\nB1 = Minimal Exercise\nB2 = Moderate Exercise\n\nHere:\n\nPr⁡(Diabetes)=0.0404\nPr⁡(Diabetes|Minimal)=0.0454\nPr⁡(Diabetes|Moderate)=0.0314\n\nTherefore:\nPr⁡(Diabetes|Minimal) ≠ Pr⁡(Diabetes|Moderate) ≠ Pr⁡(Diabetes)\nAnd since, these are not equal, we cannot assume independence. In other words, the likelihood of developing diabetes depends on how much exercise you do.\n\n\n\n\n\n\n\n\nChecking Independence (Method 2)\n\n\n\nAnother common method for checking independence is shown here.\n\\[Pr(A∩B)=Pr(A)\\times Pr(B)\\]\nIn this example,\n\nlet A = Diabetes\nB = Minimal Exercise (we can use either Minimal or Moderate)\n\nHere:\n\nPr(A) = 202/5000 = 0.0404\nPr(B) = 3216/5000 = 0.6432\nPr(A∩B) = 146/5000 = .0292\n\nAnd:\n\nPr(A) x Pr(B) = 0.0404 x 0.6432 = 0.0260\n\nCompare with joint probability:\n\nDoes Pr(A∩B) = Pr(A) x Pr(B)?\n0.0292 ≠ 0.0260\nTherefore, having diabetes and amount of exercise are not independent\n\nNote: If you had used Moderate exercise as Pr(B) instead of Minimal exercise, you would still make the same conclusion.\n\n\nImportant: This check for independence assumes our probabilities are a good reflection of the population. Most of the time, you’ll only ever be working with sample data. In which case, we are looking for the probabilities to be approximately equal.\nIn later units, you will learn about these different tests and how to use them to check for independence (e.g. Chi-square Test). But for this unit, we will assume probabilities are exactly known and correct.\n\n\n\n\n\n\nPractice Exercise 1.5 (Challenge)\n\n\n\nSuppose you draw two cards from a standard deck of 52 playing cards, with replacement. Are the events “first card is an Ace” and “second card is also an Ace” independent?\nBegin byconstructing a 2x2 contingency table that looks something like:\n\n\n\n\n\n\n\n\n\n\n2nd Card: Ace\n2nd Card: Not Ace\nTotal\n\n\n1st Card: Ace\n\n\n\n\n\n1st Card: Not Ace\n\n\n\n\n\nTotal\n\n\n\n\n\n\nThen you can use either method from above (for practice, try both!) to check if the events “first card is an Ace” and “second card is also an Ace” independent?\nSolutions can be found in the lecture recording",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#excel",
    "href": "01-Introduction-to-statistics.html#excel",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.6 Excel",
    "text": "1.6 Excel\nIn this section we will learn how to use Microsoft Excel to solve some of the concepts covered in this chapter. Begin by downloading the data file below and then follow the instructions by navigating through the screenshots.\n\n\n Medical Condition and Exercise\n\n\n\n1.6.1 Pivot Tables\nWhen your dataset is in tabular format (rows = observations, columns = variables), Excel can quickly summarise relationships between variables using Pivot Tables. A Pivot Table is an interactive summary table that allows you to group, count, and compare categories within your data. It’s particularly useful for:\n\nCreating frequency tables\nBuilding contingency tables\nExploring patterns and relationships between two categorical variables\n\nEssentially, a Pivot Table lets you “pivot” or rearrange your data to look at it from different perspectives — without changing the underlying dataset.\nFollow the instructions below to create PivotTables for this scenario. You will also learn how to change the raw counts to conditional probabilities.\n\n123456\n\n\nAfter you have loaded the data, click on the Insert Tab, then click PivotTable. This will open up the Create PivotTable dialog box. Excel usually does a good job of identifying the range of our data automatically (in this case cells A1 to C5001). We can then just click OK.\n\n\n\n\n\n\n\n\n\n\n\nA new sheet will be created in your workbook, with PivotTable Fields appearing on the right of the window. To begin, let’s move Medical Condition into the Rows and Values boxes. This now creates a PivotTable that shows the counts of the conditions within our data.\n\n\n\n\n\n\n\n\n\n\n\nTo create a contingency table, we can move a second variable into the columns field box. Let’s move Exercise into this box. Notice here that your pivot table will update automatically. Try moving the variables to different field boxes to see what happens!\n\n\n\n\n\n\n\n\n\n\n\nBy default the PivotTable shows the counts or frequencies. We can very easily change these to row or column totals to show conditional probabilities. Right-click on any number with the table, then select Show Values As and select % of Column Total.\n\n\n\n\n\n\n\n\n\n\n\nOur table now shows the conditional probability (column-wise). We can also change the format of the table and how the values are presented (e.g. %, decimal, etc.). Click on the Home tab, highlight the data, then switch to decimal. You can also increase / decrease decimal places as well.\n\n\n\n\n\n\n\n\n\n\n\nYou can also copy and paste a PivotTable quite easily to show different things. Have a go at creating two PivotTables - one that shows the conditional probability (column-wise), and one that shows the conditional probability (row-wise). Use the screenshot here to check your answer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.6.2 Excel formulas (Challenge Task)\nWhile PivotTables are a fast and flexible way to summarise categorical data, it’s also valuable to know how to perform these calculations manually using Excel formulas. Understanding the underlying formulas helps you see how the summaries are generated and gives you more control when customising reports or creating automated dashboards. This can be a valuable skill to add to your CV. Note: there are many ways to complete this task. The screenshots below suggest only one of these methods.\n\n1234567891011\n\n\nUse the UNIQUE function to find all levels of the Medication Condition variable.\n\n\n\n\n\n\n\n\n\n\n\nNext, let’s remove the formulas from these cells. Copy the cell contents and paste over it as “values only”.\n\n\n\n\n\n\n\n\n\n\n\nOptional: Switch to the Data tab and arrange the data alphabetically.\n\n\n\n\n\n\n\n\n\n\n\nWe can do the same for the levels of Exercise as well (although because we only have 2 categories it’s faster just to type them out).\n\n\n\n\n\n\n\n\n\n\n\nUse the COUNTIFS function to count how many cases have asthma and minimal exercise.\n\n\n\n\n\n\n\n\n\n\n\nDrag this formula down for all medical conditions.\n\n\n\n\n\n\n\n\n\n\n\nRepeat for the Moderate Exercise column\n\n\n\n\n\n\n\n\n\n\n\nUse the SUM function to calculate the row totals\n\n\n\n\n\n\n\n\n\n\n\nRepeat for the column totals\n\n\n\n\n\n\n\n\n\n\n\nCopy and Paste the tables twice to two different places on the workbook. Remove the values, such that only the row and column labels remain.\n\n\n\n\n\n\n\n\n\n\n\nCalculate the conditional probabilities for each table by dividing the cell value by either their respective row / column totals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#summary",
    "href": "01-Introduction-to-statistics.html#summary",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nIn this chapter, we built the foundations for understanding data in statistics — beginning with the concept of a variable. A variable is any characteristic or property that can take on different values across individuals or observations. Recognising what kind of variable you’re dealing with is essential because it determines the type of analysis and visualisation that are appropriate.\nWe classified variables into two main types:\n\nNumerical variables, which represent quantities that can be measured and expressed with meaningful units (e.g., height in centimeters, income in dollars, reaction time in seconds).\nCategorical variables, which describe qualities, groups, or classifications without meaningful numerical scales (e.g., gender, country of birth, eye color).\n\nUnderstanding whether a variable has units of measurement helps determine which type it is — and consequently, which statistical “recipe” to use.\nWe also introduced the idea of tabular data, where each row represents one observation (e.g., one person) and each column represents a variable. This structure is fundamental for statistical analysis because it allows data to be organised, summarised, and visualised systematically.\nFrom there, we examined how to explore relationships between two categorical variables using contingency tables (or cross-tabulations). These tables display the frequency of each possible combination of categories, helping us observe patterns and associations between variables.\nThis led to the introduction of key probability concepts:\n\nMarginal probabilities: the overall probabilities of a single event occurring (e.g., the proportion of people who exercise regularly).\nJoint probabilities: the probability that two events occur together (e.g., someone both has diabetes and exercises minimally).\nConditional probabilities: the probability of one event occurring given that another has occurred (e.g., the probability of having diabetes given minimal exercise).\n\nFinally, we explored the idea of independence. Two categorical variables are independent if the probability of one event does not change based on the outcome of the other. In other words, knowing one provides no information about the other. We learned two ways to check for independence — by comparing conditional probabilities or by testing whether\n\\[Pr(A∩B)=Pr(A)×Pr(B)\\]\nUnderstanding these concepts — how variables are defined and structured, how relationships between them are summarised in tables, and how probability underpins their interpretation — provides the essential groundwork for all statistical reasoning and inference that follows.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-statistics.html#exercises",
    "href": "01-Introduction-to-statistics.html#exercises",
    "title": "1  Introduction to Statistics & Categorical Data",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe table below is the same one presented in the readings regarding medical condition and amount of exercise\n\n\n\n\n\n\n\n\n\nCondition\nMinimal Exercise\nModerate Exercise\nTotal\n\n\nAsthma\n260\n166\n426\n\n\nCancer\n256\n125\n381\n\n\nDepression\n323\n170\n493\n\n\nDiabetes\n146\n56\n202\n\n\nHeart Disease\n220\n43\n263\n\n\nNone of the above\n2011\n1224\n3235\n\n\nTotal\n3216\n1784\n5000\n\n\n\nDetermine if Heart Disease and Exercise are independent, using the product rule\n\\[Pr(A∩B)=Pr(A)\\times Pr(B)\\]\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nDefine A and B\n\nLet A: Undergraduate\nLet B: Minimal exercise\n\nObtain probabilities\n\nPr(A) = 263/5000 = .053\nPr(B) = 3216/5000 = .643\nPr(A ∩ B) = 220/5000 = .044\n\nMultiply marginal probabilities\n\nPr(A) x Pr(B) = .053 x .643 = .034\n\nCompare with joint probability\n\nDoes Pr(A ∩ B) = Pr(A) x Pr(B)\n.044 ≠ .034\nTherefore not independent\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nThe table below show the proportion of females and males in each category of education level for a sample of Australian adults.\n\n\n\nRow Labels\nMale\nFemale\nTotal\n\n\nLess than year12\n19.4%\n17.9%\n18.6%\n\n\nPostgraduate\n9.5%\n6.5%\n7.9%\n\n\nTAFE\n22.7%\n19.6%\n21.1%\n\n\nUndergraduate\n19.4%\n23.2%\n21.4%\n\n\nYear 12\n29.1%\n32.8%\n31.0%\n\n\nTotal\n100.0%\n100.0%\n100.0%\n\n\n\n\nHow has the % values within the table been calculated? (i.e. row-wise or column-wise?)\nWhat does the number 29.1% tell us in this table?\nWhat proportion of the sample had achieved less than Year 12 level of education?\nApply the conditional probability rule to test whether having an undergraduate degree is independent of sex.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nColumn-wise\n29.1% of the sample are male and completed year 12\n18.6%\nPr(Undergraduate) = 21.4%\nPr(Undergraduate | Male) = 19.4%\nPr(Undergraduate | Female) = 23.2%\nPr(Undergraduate | Male) ≠ Pr(Undergraduate | Female) ≠ Pr(Undergraduate)\nTherfore, not independent\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nIn this exercise, you will look at some data on poverty rates for different types of households. The data is based on the 2015 Australian Census, and covers the Metropolitan Melbourne region.\nThe table below shows the poverty rates for Melbourne for different types of households, in terms of number of parents and number of children.\n\n\n\n\n\n\n\n\n\n\n\nHousehold\nN children = 1\nN children = 2\nN children = 3\nN children = 4\nTotal\n\n\nSingle parent\n16.9%\n23.5%\n27.6%\n34.3%\n20.6%\n\n\nTwo parents\n6.7%\n6.3%\n7.9%\n15.4%\n7.2%\n\n\n\n\nWhat is the probability that a Melbourne household with a single parent and two children will be poor?\nWhat does the number 20.6% in the last column mean?\nDiscuss the patterns in this table – how does the poverty rate vary with number of parents and number of children? Give some intuition for the patterns you find.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n0.235\n20.6% of all single-parent households have incomes that fall below the poverty line\nN Parents: The poverty rate is much lower for two-parent households compared to single-parent households. It could be that single-parent households may need to trade off some (or all) of their working capacity to take care of their children (i.e. work part-time and earn less). Whereas in two-parent households, the second parent may be more able to work full-time.\nN Children: The poverty rate appears to increase as the number of children increases. Perhaps those who have fewer children are more career-oriented rather than family-oriented. This trade-off would see these households earning more and therefore less likely to fall under the poverty line.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nThe tables below relate to the scenario from the previous question, and show the percentage of poverty according to household type.\nTable 1. Joint probabilities\n\n\n\n\n\n\n\n\n\n\n\nHousehold\nN children = 1\nN children = 2\nN children = 3\nN children = 4\nTotal\n\n\n\n\nSingle parent\n22.1%\n16.5%\n6.7%\n3.1%\n48.3%\n\n\nTwo Parents\n16.4%\n19.0%\n9.9%\n6.3%\n51.7%\n\n\nTotal\n38.5%\n35.5%\n16.5%\n9.5%\n100.0%\n\n\n\nTable 2. Conditional probabilities\n\n\n\n\n\n\n\n\n\n\n\nHousehold\nN children = 1\nN children = 2\nN children = 3\nN children = 4\nTotal\n\n\n\n\nSingle parent\n45.7%\n34.1%\n13.8%\n6.4%\n100.0%\n\n\nTwo Parents\n31.8%\n36.8%\n19.1%\n12.3%\n100.0%\n\n\n\n\nWhat does the value 48.3% in the last column of the first table mean?\nWhat is the probability that a poor household has two parents and two children?\nWhat is the probability that a poor, two-parent household will have more than two children?\nAmong poor households, show that the number of children in a household is NOT independent of whether there are two parents or one. Explain your reasoning. Give some intuition for why this dependence is likely to be present.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n48.3% of households living below the poverty line consist of a single parent.\nUsing Table 1: The probability is 0.190 (or 19.0%).\nUsing Table 2: Pr(3 children | 2 parents) + Pr(4 children | 2 parents) = 19.1 + 12.3 = 31.4%\nSince we already have Table 2 (conditional probabilities), we will use that approach\nP(1 child | single parent) = 45.7%\nP(1 child | two parents) = 31.8%\nTherefore not independent\nIntuitive explanation: Single-parent households are more likely to have only one child, while two-parent households are more likely to have multiple children. This dependence likely exists because: (1) Raising children is more manageable with two parents, (2) Two-parent households typically have higher combined incomes and shared responsibilities.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nDownload the file below and open it in Excel.\n\n\n Travellers\n\n\nThe sheet “Travellers” contains a worksheet of raw data. The data have been collected from 3999 travellers as they arrived at Melbourne airport. The sheet contains the country (region) they came from and the main purpose of their visit (work, study or tourism), so there are two categorical variables to be examined: one is “Region” and the other is “Purpose”.\n\nCreate a univariate pivot table which looks at the purpose of travel – i.e. study, work or tourism. Examine and comment on the frequency of these three travel purposes.\nConstruct a contingency table which shows the percentage of travellers with different purposes by their region. Which region has the highest proportion of people coming for the purposes of study?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\n\n\n\n\nRow Labels\nCount of purpose\n\n\nstudy\n1559\n\n\ntourism\n1247\n\n\nwork\n1193\n\n\nGrand Total\n3999\n\n\n\nIt appears that travellers tend to visit Melbourne mainly for the purpose of study, and they are less likely to visit Melbourne for work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRow Labels\nAmericas\nAsia\nEurope\nMiddle East\nGrand Total\n\n\nstudy\n45.82%\n52.13%\n18.25%\n41.44%\n38.98%\n\n\ntourism\n46.08%\n24.47%\n48.42%\n10.04%\n31.18%\n\n\nwork\n8.10%\n23.40%\n33.33%\n48.52%\n29.83%\n\n\nGrand Total\n100.0%\n100.0%\n100.0%\n100.0%\n100.0%\n\n\n\nIt is clear that in terms of traveling purpose, more than half of the Asian travellers visit Melbourne for study, while almost half of European travellers are tourists. To sum up, travellers from different regions visit Melbourne for different purposes.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nThis question will use the same file from the previous exercise. Begin by switching to the sheet labelled “HDI & Schooling”. It contains data from the Human Development Index (HDI). The HDI is an index produced by the United Nations that ranks countries from highest to lowest human development. The index rank is determined based on a number of different measures of development of an economy-based on measures of life expectancy, education and income. The data sheet provides the 2013 rankings for countries with complete data on the relevant development measures, geographical region within which the country is located, along with average years of schooling.\nConstruct a pivot table with “Region” on the rows and “HDI Category” on the columns. From your pivot tables, determine the following:\n\nPr(East Asia & Pacific)\nPr(Sub-Saharan Africa)\nPr(Very High HDI)\nPr(East Asia & Pacific ∩ Medium HDI)\nPr(Sub-Saharan Africa ∩ Low HDI)\nPr(North America ∩ Very High HDI)\nPr(Medium HDI | East Asia & Pacific)\nPr(Low HDI | Sub-Saharan Africa)\nPr(Very High HDI | North America)\nPr(East Asia & Pacific | Medium HDI)\nPr(Sub-Saharan Africa | Low HDI)\nPr(North America | Very High HDI)\nCheck whether the probability of having a “Low HDI” is independent of “Region”.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nPr(East Asia & Pacific) = 0.1444\nPr(Sub-Saharan Africa) = 0.246\nPr(Very High HDI) = 0.2513\nPr(East Asia & Pacific ∩ Medium HDI) = 0.0802\nPr(Sub-Saharan Africa ∩ Low HDI) = 0.1925\nPr(North America ∩ Very High HDI) = 0.0107\nPr(Medium HDI | East Asia & Pacific) = 0.5556\nPr(Low HDI | Sub-Saharan Africa) = 0.7826\nPr(Very High HDI | North America) = 1\nPr(East Asia & Pacific | Medium HDI) = 0.3191\nPr(Sub-Saharan Africa | Low HDI) = 0.7826\nPr(North America | Very High HDI) = 0.0426\nPr(Low HDI | East Asia & Pacific) = 0.1111\nPr(Low HDI | Europe & Central Asia) = 0.0000\nPr(Low HDI) = 0.2460\nNot independent",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Statistics & Categorical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html",
    "href": "02-Analysing-numerical-data.html",
    "title": "2  Analysing Numerical Data",
    "section": "",
    "text": "2.1 Introduction\nIn the previous chapter, we explored categorical data — data that sort people or things into distinct groups such as “left-hand/right-hand,” “yes/no,” or “sport type.” This chapter turns our attention to numerical data — data that represent quantities, such as income, test scores, or reaction times.\nUnderstanding numerical data is essential because it allows us to move from simple description to measurement, comparison, and eventually inference. Our journey in this chapter begins with a key principle—standardisation—and moves through concepts of centre, spread, and shape, before ending with the foundations of probability distributions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#standardising-data",
    "href": "02-Analysing-numerical-data.html#standardising-data",
    "title": "2  Analysing Numerical Data",
    "section": "2.2 Standardising Data",
    "text": "2.2 Standardising Data\nWhen analysing data from different sources, one of the first questions we must ask is:\n\nAre these values comparable?\n\nRaw data often exist on very different scales — for instance, GDPs across countries, mortality rates, or sports performance metrics. Without standardisation, direct comparisons can be misleading.\n\n\n\n\n\n\nThe Trump Interview that broke the internet\n\n\n\n\nIn 2020, journalist Jonathan Swan interviewed then-President Donald Trump about America’s handling of the COVID-19 pandemic. Trump presented charts showing the United States “lowest in numerous categories,” claiming success. But Swan quickly realised that Trump was looking at deaths per case rather than deaths per population (which placed the United States at a very different position with regards to deaths):\n\nSwan: “Oh, you’re doing death as a proportion of cases. I’m talking about death as a proportion of population.”\n\nMeasuring deaths per case highlights case fatality rates, while measuring per population captures how widely the disease spread. This exchange demonstrates that the choice of denominator fundamentally shapes interpretation.\nAnd whilst both measures are correct, it was Swan’s facial reactions (see image above) to each of Donald Trump’s responses that stole the interview.\n\n\n\n\n\n\n\n\nStandardising for Population: CO₂ Emissions Example\n\n\n\nAnother example of standardisation is carbon dioxide (CO2) omissions. In the tables below, the first three columns have the total CO2 emissions in 2020 for the 20 countries which have the highest emissions. China is at the top of the list followed by the United States, India and so forth. Australia comes in at 16th place. While it may be of interest to examine total emissions, depending upon the precise question that is being addressed, it will often be more meaningful to normalise the data by population. This makes the quantity of emissions more comparable across countries.\n\n\nOrderCountryCO2_Emissions_kt1China10,944,6862United States4,320,5333India2,200,8364Russian Federation1,618,2715Japan1,014,0656Iran, Islamic Rep.616,5617Germany603,3518Korea, Rep.569,6829Indonesia563,19710Canada516,87411Saudi Arabia513,55612Brazil414,13913Turkey407,40614South Africa393,24215Mexico383,13116Australia378,99717Viet Nam355,32318United Kingdom308,65019Italy281,28720Poland279,224\n\n\nChina has the largest emissions in main part because it has a very large population. However, each Chinese person’s emissions are actually relatively low. If we calculate CO2 emissions per capita then we get the data in the two right hand columns of the table. We can see that this tells quite a different story. China is at the bottom of the list. Qatar heads the list along with some smaller countries – Bahrain, Brunei Darussalam and Kuwait. The US and Australia rank highly on this table indicating that not only do they have high total emissions, but their emissions per person are also high.\n\n\nOrderCountryCO2_Emissions_Standardised1Qatar31.732Bahrain21.983Brunei Darussalam21.714Kuwait21.175United Arab Emirates20.256Oman15.647Australia14.788Saudi Arabia14.279Canada13.5910United States13.0311Luxembourg12.4612Kazakhstan11.3013Russian Federation11.1414Korea, Rep.10.9915Turkmenistan10.1816Trinidad and Tobago10.1617Palau8.8018Czechia8.3019Japan8.0320China7.76\n\n\n\n\n\n2.2.1 Adjusting for Inflation\nA particularly important form of standardising data is to adjust for inflation. This is relevant for data which is measured across time and that measures monetary values. The process of adjusting for the effects of inflation is necessary because money changes value over time. For example, 100 dollars in 1990 was worth more (in the sense that you could buy more goods and services with it) than is 100 dollars now, because prices have (generally) risen in that time.\nThis is a really important observation because a lot of series are measured in dollar values. One of the most important is Gross Domestic Product (GDP). This measures the total value of all goods and services produced in the economy in a given period. In Figure 2.2, Australia’s GDP per capita is shown (we have divided GDP by the population to make it more meaningful along the lines of the previous discussion above). This shows a massive rise in nominal GDP per capita. It has risen from around \\(23,000\\) in 1990 to almost \\(100,000\\) in 2024. This is a rise of around 4 times. But does this mean we are 4 times wealthier than in 1990? No, because prices also rose over this period.\nIt turns out that prices have more than doubled over that period, so most of the increase in GDP has just been keeping up with price increases. The Statistics Office collects a lot of information about prices each quarter, and constructs what is called a “Price Index”, that tells us about the change in prices from one quarter to the next. We won’t go into the details of how that price index is constructed – there are a few of them, but the most common is the “Consumer Price Index” (CPI), which tells us average prices for everyday goods and services people buy, and is used to calculate the annual inflation rate.\n\n\n\nIn order to standardise the GDP data to allow for price increases over time, we take the value of GDP in a given quarter, and divide it by the price index for that quarter. This removes the effect of inflation from the GDP value.\nWe will use the formula:\n\\[\\text{Real GDP per capita}=\\frac{\\text{Nominal GDP per capita}}{\\text{Price Index for GDP}}\\times 100\\]\nNow look at the “real” GDP values in Figure 2.2 – real GDP per capita has risen from \\(23,000\\) dollars in 1990 to about \\(40,000\\) dollars in 2024. This is a much less spectacular rise than we first thought, because now we have removed the effect of inflation. GDP per capita has increased, so it has less than doubled over 34 years.\nThe standardisation of data to remove the effect of inflation is a common task – we will do it when looking at wages, share prices and any measure that evolves over time. Here is some of the jargon people might use to describe the variables used in these situations.\n\nNominal/Actual/Current Prices: the $ value of a series as actually measured at each point in time.\nReal/Constant Prices: the value a series would have taken if prices remained fixed at some point in history - the ‘base’ period (this is March 1990 in the Figure).\nPrice Index (e.g. Consumer Price Index (CPI)): A weighted average of prices of goods and services indexed to 100 in the ‘base’ period (this is 1990 in the Figure).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#measures-of-central-tendency",
    "href": "02-Analysing-numerical-data.html#measures-of-central-tendency",
    "title": "2  Analysing Numerical Data",
    "section": "2.3 Measures of Central Tendency",
    "text": "2.3 Measures of Central Tendency\nBack when I was working in Sport Analytics, we would collect a lot of data, and build various models. However, when presenting to the Coach or Sporting Manager, quite often they only have time to listen to one piece of information about the data. In that situation, that one piece of information, should probably be a measure of central tendency:\n\nThe center of a series of data points is usually a good example of the type of data we can expect from the group as a whole.\n\nIn general, there are 3 measures of center that people focus on: mean, median and mode.\n\n2.3.1 Mean\nThe mean (average) is the most common summary measure. It adds all values and divides by the total number of observations. It provides an overall sense of the dataset’s “center.”\nHowever, the mean is most informative when the data are normally distributed — that is, when values are symmetrically clustered around the middle, with fewer observations at the extremes (forming the familiar “bell curve”).\nFor example, in a class of 25 students with heights roughly balanced around 175 cm, the mean height gives a good sense of the group’s overall stature.\n\n\n\n\n\n\n\n\n\nThe middle height of this distribution is 175cm, and we can see that we have approximately the same amount of data on either side of this middle. For normal distributions, most of the data is around the middle and then becomes scarcer on either side (forming the famous bell-shaped curve you’re probably familiar with)\nNow, suppose the heights of students from the classroom next door shared a normal distribution similar to the previous one. If you had to guess the height of a random student in the class (without looking at them), then selecting 175cm is your safest bet (because that’s where most of the data is).\nBe careful however, as the MEAN can sometimes be misleading. For example, the mean life expectancy back in the middle ages was around 30 years. Based upon the previous example, this might lead you think that most people [in the middle ages] died around their 30s, and forming a normal distribution around this age (see the left panel below). Back then however, there was an incredibly high rate of child mortality in the days before modern medicine. But, those who managed to make it to 30, were able to, on average, live relatively long lives after that (well, long for the time anyway). Recall that the MEAN life expectancy in the middle ages was about 30. But if we examine this distribution (right panel below), hardly any of the data points are at the mean! Therefore, if we were to randomly select a person from the Middle Ages and guess their age of death, using the mean of the data would not be the best method.\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Median\nThe median is the middle value when data are ordered from smallest to largest. It divides the dataset in half — 50% of values fall below, 50% above. Because it ignores extreme values, the median is more robust when the data are skewed.\nSuppose you’re a wealthy Australian student with a net worth of about $60,000. And you find yourself in a room with seven other similarly wealthy Australian students, and Elon Musk, who is currently estimated to be worth around 400 billion dollars.\n\n\n\nThe mean, or average, worth of the people in the room would by slightly over 45 billion dollar. (A value that doesn’t correctly tell us anything about Musk’s actual wealth, or anyone else’s for that matter).\n\\[\\text{Mean}_{\\text{net worth}}=\\frac{60000+59450+63510 \\:+\\:...+\\:405,000,000,000}{9}=45,000,052,357\\]\nWhen there are extreme values that can inflate / deflate the center of your data, the median would be a better choice than the mean. To determine the median worth of the people in the room, you first list each individual’s worth in order. The median figure is half-way down the list. In this case, $60,000.\n\n\nPersonNetWorthElon Musk450,000,000,000Person 265,200Person 363,510Person 461,000Person 560,000Person 659,450Person 755,800Person 855,060Person 951,200\n\n\nMathematically, we use the following formulas to determine the median position of a set of numbers (where n = number of data points):\n\\[\\text{Median}_{\\text{odd}}=(\\frac{n+1}{2})^{th}\\]\n\\[\\text{Median}_{\\text{even}}=\\frac{(n/2)^{th}+(n/2+1)^{th}}{2}\\]\nFor example:\n\nNote: Just like the mean, the median can also be misleading depending on the nature of your data.\nSuppose we had sale prices for 5 homes in Clayton (arranged in descending order). And, suppose we also have sale prices for 5 homes in Wantirna South.\n\nClayton\n\n\n\nHousePrice11,480,00021,470,00031,460,00041,320,00051,270,000\n\n\n\nWantirna South\n\n\n\nHousePrice12,000,00021,850,00031,460,00041,450,00051,450,000\n\n\nHere, both suburbs have the same median sale price. However, it’s very clear that the houses in Wantirna South are more costly than those in Clayton! (Information about the range of sale prices are not considered when computing the median).\n\n\n2.3.3 Mode\nThe mode is the value that occurs most frequently in a dataset. Unlike the mean or median, which describe the centre of the data, the mode identifies the most common observation. It is particularly useful when analysing categorical data (for example, determining the most popular product or preferred transport mode), though it can also be applied to numerical data. A dataset can have one mode (unimodal), more than one mode (bimodal or multimodal), or no mode at all if all values occur with equal frequency. While the mode provides insight into what is typical, it does not reflect how data are distributed around that value.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#data-disributions",
    "href": "02-Analysing-numerical-data.html#data-disributions",
    "title": "2  Analysing Numerical Data",
    "section": "2.4 Data Disributions",
    "text": "2.4 Data Disributions\nOften the mean will be different from the median, even though both are designed to describe where the data is centered. The difference is usually due to skewness in the data.\n\n\n\n\nWhen the mean is lower than the median, the data generally displays negative skewness. This is often due to a few small values in the left tail of the distribution.\nWhen the mean is approximately equal to the median, the data displays a symmetrical distribution. There is approximately equal number of small and large values the decrease in frequency from the center of the data.\nWhen the mean is larger than the Median, the data generally displays positive skewness. This is often due to a few large values in the right tail of the distribution.\n\n\n\n\n\n\n\nPractice Exercise 2.1\n\n\n\nIn a 2008 study, researchers compared monthly online gambling expenditure across several locations. The table below summarises the findings:\n\n\nCountryMeanMedianUnited States23790Canada13390United Kingdom6561Asia9555Australia3009\n\n\n\nWhich country has the highest mean monthly gambling expenditure?\nWhich country has the lowest median monthly gambling expenditure?\nGiven the results of A. and B., what can be say about gambling expenditures in Australia?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nAustralia (mean = $300)\nAlso Australia (median = $9)\nMost likely, there are some extreme cases (outliers) that are skewing the distribution to the right and increasing the mean.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#measures-of-spread",
    "href": "02-Analysing-numerical-data.html#measures-of-spread",
    "title": "2  Analysing Numerical Data",
    "section": "2.5 Measures of Spread",
    "text": "2.5 Measures of Spread\nWhile measures of centrality (such as the mean or median) tell us where the centre of the data lies, measures of spread show us how tightly or loosely the data cluster around that centre. Two datasets can have the same average but very different variability — one might have values that are all similar, while another might include both very small and very large numbers. Understanding spread helps us interpret the consistency, reliability, and diversity within data. In the sections that follow, we’ll explore four key ways to describe spread: the range, interquartile range (IQR), variance, and standard deviation, each offering a different perspective on how dispersed the data are.\n\n2.5.1 Range\nSuppose you work in the Human Resources department for a particular organisation. You’ve been informed that the organisation is losing employees rapidly and you need to find a way to keep them. Your first task is to learn more about your employees.\nTo begin with, you could calculate the range of the current employees’ salaries.\n\nThe range takes the largest number in the data set and subtracts the smallest number to give the distance between these two extremes.\n\nSuppose you take a random sample of 10 employees and inspect their current annual salaries (here arranged in sequential order on a number line):\n\n\n\nSubtracting the smallest number from the largest gives us the range (here: 122K – 75K = 47K)\nIn this case, the two larger values (115K and 122K) appear further away than the rest of the data. This might represent employees at different levels compared to the others. Including these, increases the range.\n\n\n\nNote: the range is unreliable as a measure of spread when we have extreme cases. We’re going to need a different approach!\n\n\n2.5.2 Interquartile Range\nThe interquartile range (IQR) measures the spread of the middle 50% of your data. It focuses on where most observations lie by looking at the distance between the 1st quartile (Q1) — the value marking the 25th percentile — and the 3rd quartile (Q3) — the value marking the 75th percentile. The IQR is calculated as \\(IQR = Q3-Q1\\). Unlike the range, which can be heavily influenced by extreme values, the IQR is resistant to outliers and provides a clearer picture of the typical variation in your dataset. In practice, it helps us understand whether most data points are tightly clustered around the median or spread more widely across the scale.\n\n\n\nOne of the advantages of the IQR, is that it is not affected by extreme cases (like the Range). In this example, we’ve captured the middle 50% of our data. However, we’ve completely ignored the data in the tails! (i.e. the bottom and top 25% of the data)\n\n\n2.5.3 Variance\nThe variance measures how much the values in a dataset differ from the mean. It does this by calculating the average of the squared deviations — that is, how far each data point is from the mean, squared to remove negative signs and emphasise larger differences. Mathematically, the variance is given by:\n\\[\\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\] Click on the steps below to see how the variance is calculated:\nThe formula looks more complicated than it really is. In essence what we are doing is:\n\nComputing the mean of the data set\nSubtracting the mean from each value (this is called a deviation)\nSquaring eaching deviation\nAdding up these squared deviations\nDividing by n - 1\n\nThe image below shows how the variance is computed for our current data set using these steps:\n\nNote that because we squared the deviations, our units are squared as well! (Most people don’t think of things in squared terms, so we need a way to convert this back to original units)\n\n\n2.5.4 Standard deviation\nThe Standard Deviation (SD) is the square root of the variance, which gives back the units that we’re comfortable with and you can think of the SD as the average amount we expect a point to differ (or deviate) from the mean.\n\n\n\n\n\n\n\nPractice Exercise 2.2\n\n\n\nSuppose we randomly sampled the final grades of ETC1000 students from different years (and arranged in ascending order for each year):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022:\n24\n56\n57\n66\n66\n68\n75\n78\n84\n90\n\n\n2023:\n45\n46\n46\n47\n49\n51\n53\n54\n55\n55\n\n\n2024:\n65\n65\n65\n67\n69\n70\n72\n72\n74\n75\n\n\n\n\nBased on these samples, which year (2022, 2023 or 2024) had the most spread in final grades?\nBased on the final grades across the three years, which year appears to have been the most difficult for students?\nSuppose I’m a repeat student and scored a 45 in each of the three years. In which year would I have performed the worst relative to my peers?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n2022\n2023\n2024",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#visualisations",
    "href": "02-Analysing-numerical-data.html#visualisations",
    "title": "2  Analysing Numerical Data",
    "section": "2.6 Visualisations",
    "text": "2.6 Visualisations\n\n2.6.1 Boxplots\nWhen exploring numerical data, one of the most effective ways to summarise and visualise the distribution is through a boxplot (also known as a box-and-whisker plot). Boxplots are built from a set of five key statistics — collectively known as the five-number summary.\nThe five-number summary provides a concise overview of your dataset’s spread and shape. It consists of:\n\nMinimum – the smallest observed value (excluding outliers).\nFirst Quartile (Q1) – the value below which 25% of the data fall.\nMedian (Q2) – the middle value, dividing the dataset into two equal halves.\nThird Quartile (Q3) – the value below which 75% of the data fall.\nMaximum – the largest observed value (excluding outliers).\n\nTogether, these five values tell us where the data begin, where they end, and how they are distributed around the middle. The interquartile range (IQR), which we discussed earlier, is simply the distance between Q3 and Q1 — representing the spread of the middle 50% of the data.\nBoxplots are particularly useful when comparing groups side-by-side. They allow you to quickly see differences in central tendency, spread, and symmetry. For instance, if one group’s box is much taller, it has more variability; if the median line is closer to one end of the box, the data are skewed.\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Histograms\nLast week, we learnt how to transform raw data into a frequencies table, which in turn could be visualised as a bar chart. This works well when you have categorical data, because you are determining how often a category occurs.\n\n\n\n\n\n\n\n\n\nIt’s a bit more challenging when you have numerical data. For example, let’s consider our salary case study again, where we have a range of numeric values (instead of categories), and each value only has a count of 1.\n\n\nSalaryFrequency75,000181,000186,000187,000191,000193,000199,0001102,0001115,0001112,0001\n\n\nNow, you could treat each value as it’s distinct category, and then get the frequencies of each. You can then plot a bar chart to visualise this. Unfortunately, this plot doesn’t tell us much at all!\n\n\n\n\n\n\n\n\n\nInstead of using every single value in the data set as a category, people bin values together. This means to group data points close to each other into groups.\n\n\n\nIn this sample we have 125 employee salaries in total, and we can see that the most frequent salaries were between \\(63,243\\) and \\(86,121\\). We can also say that very few employees (n = 4) have salaries less than \\(55,617\\). Likewise, very few employees (n = 3) have salaries above \\(93,747\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#probability-distributions",
    "href": "02-Analysing-numerical-data.html#probability-distributions",
    "title": "2  Analysing Numerical Data",
    "section": "2.7 Probability Distributions",
    "text": "2.7 Probability Distributions\nAnother way to think about these plots is not in terms of how often a value actually occurred, rather – how likely is it that a value would occur (i.e. the probability). Imagine you were part of the HR team and you were doing a campaign that awards prizes to random employees. Your manager asks you the following question:\n\nHow likely is it that a randomly selected employee will have a salary greater than $93,747?\n\nIf we look at the frequency distribution, we’d probably respond “not very likely” (\\(\\frac{3}{125} = .024\\)).\nOn the other hand, if your manager asked you\n\nHow likely is it that a randomly selected employee will have a salary between $63,243 and $86,121?\n\nIf we look at the frequency distribution, we’d probably respond “very likely” (\\(\\frac{24+42+33}{125}=.792\\)).\nFor any distribution of values, we can (in theory) calculate the probability of obtaining a value of any given size. For each distribution, statisticians have computed probability density functions, that specify idealised versions of each distribution. Plotting these, provides us with curves (see above), known as probability distributions. A probability distribution is similar to a histogram, however the lumps and bumps have been smoothed out.\n\n\n\nLet’s just focus on our salary data for now, which appears to approximate a normal distribution based on its histogram:\n\n\n\n\n2.7.1 Properties of the normal distribution\n\nThe Normal curve is bell shape and approximately symmetric about the mean\nThe mean, median and mode are approximately equal\nThe total area under the curve is equal to 1\n\nThe standard deviation can be used to express how far from the mean the data values are. We usually discuss the normal distribution with respects to 1SD above and below the mean, 2SD above and below the mean, and 3SD above and below the mean. However we could also talk about 4SD, 5SD, etc. above and below the mean. This is because the normal distribution never touches the x-axis (i.e. it approaches ± ∞).\nRemember earlier that the area under the normal distribution equals 1? This is useful because the area (probability) under the curve will always be the same for different SD above / below the mean for the standard normal distribution.\n\n\n\nFrom the image above, we can see that:\n\n68.3% of the area is covered within 1 std. deviation of the mean\n95.4% of the area is covered within 2 std. deviations of the mean\n99.7% of the area is covered within 3 std. deviations of the mean\n\nLet’s reconsider our salary example from before, shown below as normal probability density functions (PDF), for office and warehouse workers.\n\n\n\nWe can see that the center of the distributions for both group is approximately the same (~ 50,000), however we can see that there is very different spread for both distributions. On average, warehouse and office workers have similar salaries, however, the variability in office worker’s pay is much larger. Let’s assume:\n\nOffice ~ N(50000, 10000)\nWarehouse ~ N(50000, 5000)\n\nNow suppose we randomly selected an employee from both groups who have annual salaries of \\(60,000\\). In both cases, we’re looking at an employee who’s 5000 above the mean salaries for their groups. But relative to each group’s spread, the \\(60,000\\) Office worker is not that much higher compared to other Office workers. Whereas the \\(60,000\\) warehouse worker is much higher than other warehouse workers.\n\n\n\nThis is why we standardise values - so that we can interpret these values in context. By standardising, we convert the data to a common scale: the standard normal distribution, which has a mean of 0 and a standard deviation of 1. This is called a Z transformation.\nFor any value X, the z-score is calculated as:\n\\[z=\\frac{X-\\mu}{\\sigma}\\]\nWhere:\n\nX: the value of interest\n\\(\\mu\\): the population mean\n\\(\\sigma\\): the population standard deviation\n\nThis formula standardises the value X, putting it on a common scale with mean 0 and standard deviation 1. For example, using our scenario from above (selecting an office worker with a salary of \\(60,000\\)), we can standardise this using the Z formula. Recall, the \\(\\mu=50,000\\) and \\(sigma=10,000\\).\n\\[Z=\\frac{60000-50000}{10000}=1.00\\]\nOnce you have converted a raw value X into its corresponding z-score, the next step in many statistical problems is to determine how likely that value is under the assumption of a normal distribution. In practice, this means translating the z-score into an area under the standard normal curve. You might recall a table similar to the one below from high school:\n\nIn our example, the Z score is 1.00. Using the table, we can see that this corresponds to a probability of 0.8413. This tells us that for an office worker with a salary of 60,000:\n\n0.8413 (or 84.13%) of office workers have salaries below 60,000\n0.1587 (or 15.87%) of office workers have salaries above 60,000. This is because probabilities need to sum to 1.\n\nIf we repeated this for warehouse workers as well:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#excel",
    "href": "02-Analysing-numerical-data.html#excel",
    "title": "2  Analysing Numerical Data",
    "section": "2.8 Excel",
    "text": "2.8 Excel\nIf you need to develop complex statistical or engineering analyses, you can save steps and time by using the Analysis ToolPak. You provide the data and parameters for each analysis, and the tool uses the appropriate statistical or engineering macro functions to calculate and display the results in an output table. Some tools generate charts in addition to output tables.\n\n2.8.1 Analysis ToolPak in Excel\n\nWindowsmacOS\n\n\n\nClick the File tab, click Options, and then click the Add-Ins category.\nIn the Manage box, select Excel Add-ins and then click Go.\nIf you’re using Excel for Mac, in the file menu go to Tools &gt; Excel Add-ins.\nIn the Add-Ins box, check the Analysis ToolPak check box, and then click OK.\n\nIf Analysis ToolPak is not listed in the Add-Ins available box, click Browse to locate it.\nIf you are prompted that the Analysis ToolPak is not currently installed on your computer, click Yes to install it.\n\n\n\n\n\nClick the Tools menu, and then click Excel Add-ins.\nIn the Add-Ins available box, select the Analysis ToolPak check box, and then click OK.\n\nIf Analysis ToolPak is not listed in the Add-Ins available box, click Browse to locate it.\nIf you get a prompt that the Analysis ToolPak is not currently installed on your computer, click Yes to install it.\nQuit and restart Excel.\n\nNow the Data Analysis command is available on the Data tab.\n\n\n\n\n\n\n2.8.2 Descriptive Statistics\nFor this exercise we will use the Data Analysis ToolPak (make sure you have enabled it using the instructions above) to generate descriptive statistics for the data file below.\n\n\n\n Household income\n\n\n\n\n1234\n\n\nOpen the data file in Excel. It should only have 1 column (called Income).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Data tab and select Data Analysis. From there, choose Descriptive Statitics and click OK.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’ll now need to select the input range (i.e. the data). In this file, it’s cell A1 to A5086. Make sure ‘Labels in first row’ is selected (so that it knows A1 is just the column name). Specify an output (usually within the same sheet or within a new sheet). And make sure ‘Summary statistics’ is selected.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf done correctly, you will now have generated a range of descriptive statistics for this variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.8.3 Histograms\nHistograms are useful ways to visualise the distribution of a numeric variable. There are two ways in which Excel can create histograms.\n\nMethod 1 (Excel Charts)Method 2 (Data Analysis ToolPak)\n\n\nHighlight the column where your data is (in this case: column A), switch to the Insert tab, then choose Insert Statistics Chart then choose Histogram. Note here that Excel will choose the range for each bin automatically for you (see the chart’s X-axis). You can modify this by Right-clicking on the X-axis and choosing format axis. From there you can customise the bins as you see fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe second method is to use the ToolPak. This method requires a bit more work, as you have to specify your own bins.\n\n1234\n\n\nLet’s begin by creating a new column called “Bins”, which has values from 0 to 120000.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the Data tab, select Data Analysis and choose Histogram. From there specify the input range (our data) and the bin range (which we created in step 1). Make sure Labels is selected and specify where you want the output to appear.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcel will compute a frequency table for your bins (see image 1 below). From here, switch to the Insert tab and insert a column chart.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can then customise your chart as desired (recommendation is to right-click the columns, select Format data series and reduce the gap width to 5%).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#summary",
    "href": "02-Analysing-numerical-data.html#summary",
    "title": "2  Analysing Numerical Data",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nIn this chapter, we moved from describing what numerical data look like to understanding how to compare, summarise, and interpret them responsibly. We began with standardisation, showing why raw totals can mislead and how adjusting for population, time (inflation), or scale (Z-scores) enables fair comparisons. We then examined measures of central tendency—mean, median, and mode—highlighting when each is most informative and how distribution shape (symmetry vs skew) affects their interpretation. Next, we quantified variability with measures of spread—range, IQR, variance, and standard deviation—to distinguish consistent datasets from volatile ones. We translated these summaries into visuals: boxplots (built from the five-number summary) to compare groups at a glance, and histograms to reveal distribution shape. Finally, we introduced probability distributions, especially the normal, and used Z-scores to put values on a common scale and read probabilities from the standard normal model. Together, these tools—center, spread, shape, standardisation, and probability—form the core toolkit for analysing numerical data and set the stage for inference in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "02-Analysing-numerical-data.html#exercises",
    "href": "02-Analysing-numerical-data.html#exercises",
    "title": "2  Analysing Numerical Data",
    "section": "2.10 Exercises",
    "text": "2.10 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nBelow are two histograms showing the distribution of weekly incomes for a sample of Australian adults.\n\n\n\n\n\n\n\n\n\n\nWhat are two features of the second histogram that make it a much better graph than the first? Explain why they make things better.\nBased on the second histogram, what is the approximate percent of people with incomes below $1200 per week? N.B. There are 1000 people in the sample\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\n\n\nBetter chart title, which includes units to make interpreting the values easier\nBetter bin ranges, which make the horizontal axis easier to read and compare\n\n\n\n\n(6+13)/1000 = 1.9%\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nThe normal distribution is often used to approximate the true distribution of a random variable such as individuals’ incomes. Suppose we have weekly income for a sample of Australian adults. The sample mean is 1728.32, and the standard deviation is 261.47. If we assume incomes are normally distributed, use the Excel calculations below and other properties of the normal distribution to find:\n\nThe probability a person will earn below $1,500 per week.\nThe probability a person will earn more than $2,000 per week.\nThe probability a person will earn less than the mean ($1728.32).\n\nExcel calculations\n\\[=\\text{NORM.DIST(1500, 1728.32, 261.47, TRUE)}\\]\n\\[=1-\\text{NORM.DIST(2000, 1728.32, 261.47, TRUE)}\\]\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nPr(X &lt; 1500) = 0.191\nPr(X &gt; 2000) = 1 – Pr(X &lt; 2000) = 1 – 0.851\nPr(X &lt; 1728.32) = 0.50\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nIn this exercise you will look at results based on a database with details of every death in road accidents in Australia over the past 31 years. Below are descriptive statistics for the age of the people who died in accidents. The first set covers the whole period January 1989 to March 2020, the second set covers 1989 to 2003, and the last column is for 2004 to 2020.\n\n\nStatisticFull_SampleYear_1989_2003Year_2004_2020Mean39.5437.6442.08Standard Error0.100.130.15Median34.0031.0038.00Mode18.0018.0018.00Standard Deviation21.8021.5621.85Sample Variance475.07465.02477.23Kurtosis-0.62-0.52-0.69Skewness0.570.660.47Range110.00108.00110.00Minimum-9.00-9.00-9.00Maximum101.0099.00101.00Sum2,024,661.001,101,459.00923,195.00Count51,202.0029,261.0021,940.00\n\n\n\nCompare the mean and median for the full sample. What does this suggest about the shape of the distribution of ages? Explain how you draw that conclusion about the shape.\nCompare the means across the 3 periods. What does this suggest about trends in age over time?\nWhat does the mode tell you? Is it a sensible measure in this case? Explain.\nNote the minimum value in each case is -9. It turns out that when the age of the person is missing in the data, a value of -9 is entered. There are a few of these in the data set, and the analysis would have excluded these values first. If we took out the missing values (-9) from the data, would be mean increase or decrease? Explain your reasoning.\nLook at the standard deviation for the full sample. Interpret what this value tells you about the spread of ages.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nThe mean is 5 years older than the median. This suggests positive skewness. Some extreme large values of age are pulling up the mean, but they don’t affect the median in the same way.\nThe mean has increased by 4.5 years from the first half to the second half of the sample. Average age is clearly getting older with time, more older people, fewer young people.\nMost common age is 18. This is useful, indicates the high-risk young driver.\nMean will increase. Remove values below the mean, and you increase the mean of the values that remain.\nRoughly speaking, average variation in age around the mean is 21.8 years. So, it is a reasonably spread out age range.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nSuppose X is distributed as Normal with a mean 40 and standard deviation of 15, and we calculate another variable Z using the formula Z = (X – 40) / 15.\n\nDo some simple maths to derive the mean of Z.\nWhat is the standard deviation of Z? No need to derive this result, just state it.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\overline{Z}\n&= \\frac{1}{n} \\sum_{i=1}^{n} Z_i \\\\[6pt]\n&= \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i - 40}{15} \\\\[6pt]\n&= \\frac{1}{15} \\times \\frac{1}{n} \\sum_{i=1}^{n} (X_i - 40) \\\\[6pt]\n&= \\frac{1}{15} \\left[\n\\frac{1}{n} \\sum_{i=1}^{n} X_i\n-\n\\frac{1}{n} \\sum_{i=1}^{n} 40\n\\right] \\\\[6pt]\n&= \\frac{1}{15} \\left[ \\overline{X} - 40 \\right] \\\\[6pt]\n&= \\frac{1}{15} \\left[ 40 - 40 \\right] = 0\n\\end{aligned}\n\\]\n\n\n\nThe formula used converts a normal random variable, X, with a given mean and variance / standard deviation into a standard normal random variable, Z, with a mean of 0 and variance / standard deviation of 1\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nIn the table below, we include some annual CO2 emissions data for China for the years 1990 and 2016.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nCO2\nPopulation\nGDP\nGDP per capital\nGDP (% change)\nGDP per capital (% change)\nCO2 (% change)\n\n\n1990\n2,421\n1.18 B\n2.79 T\n2,364\n\n\n\n\n\n2016\n9,553\n1.19 B\n16.90 T\n14,202\n506\n501\n295\n\n\n\n\nWrite down the formula used to calculate GDP per capita for 2016 and outline the units of measurements of this variable.\nIn the table, we have also calculated the percentage change in GDP, GDP per capita and CO2 emissions between 1990 and 2016. Discuss these numbers: what do we learn by comparing them?\nCalculate the average annual amount of growth in GDP each year in the period from 1990 to 2016. Show your working.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nGDP per capita = GDP/Population. Units is $ thousands.\nGDP and GDP per capita % change are very similar, which means population has hardly changed. CO2 emissions grew by much less than GDP (in percentage terms), so they are now more efficient – emissions per GDP has gone down.\n(16.9 – 2.79)/26 = 14.11/26, which is around 0.543. Units is $trillion per year. i.e. Average annual growth is about $543 billion per year.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nOutline in words how the range and the standard deviation are calculated. Discuss one advantage of the standard deviation over the range as a measure of spread of a set of data.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nRange = maximum value – minimum value SD = square root of the average of the squared deviations from the mean.\nSD has benefit of using all values and averaging squared deviations. So no one value has undue influence on the measure. Range is based just on 2 numbers so can be misleading, e.g. if the Max or Min values are extreme. E.g. Suppose all data are from 0-50, except one value is say 200. The Range is 200, but that overstates the spread.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nThis exercise uses real data from Timor-Leste looking at children’s heights. Child heights can be compared across ages by calculating standardised heights – subtract the global mean (or median) for children that age, and divide by the standard deviation. This gives a height Z-score (zHeight). E.g. A child with zHeight = 0 is exactly average / normal height for their age. zHeight = +2 means this child is 2 Standard deviations taller than the average child their age. A child is considered “Stunted” if their zHeight value is below -2.0.\nThe dataset has data on almost 9000 children in Timor-Leste aged between 0 and 5 years old. Here are some summary statistics and graphs for the variable zHeight.\n\n\n\nBased on this information, write a few paragraphs explaining what we learn about the heights of children in Timor-Leste. Make sure you cover the important information covered in this unit – central tendency, spread, shape, etc – and relate it well to the subject (comparative heights of children).\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nThe variable we are describing is a Z-score for height, based on worldwide data. So, an average child in the world will have a zHeight of 0, and the distribution of zHeight has a SD of 1.0.\nFor Timor-Leste children, taking an average of all the children in the sample we get an average well below 0, at -1.95, almost 2 SDs below the world average. Similarly, the median is also very low, at -2.05. This means half the children have scores below -2.05. Since the definition of stunted is zHeight below -2, this means a little more than half the children are stunted.\nThe middle 50% of children have zHeight values between -1.16 and -2.86, indicating the middleheight range is from around 1 SD below world average to 3 SDs below the world average. This confirms how the majority of these children are very short. -The SD of 1.4 suggests Timor-Leste children have more spread of heights than the world data, where SD is 1.0. There are more extremes of very short and even quite tall children. This is common in low income countries with much poverty – poor nutrition amongst the poorest children means there are many standardised heights well below zero and even well below -2. On the other hand, there are some privileged children who have privileged diet and living conditions, and who grow at more normal rate for world data, so get positive Z-scores. This produces the wider spread.\nThere is little evidence of skewness in the data, with mean and median very low, and not much skewness evident in the histogram. If anything, there is weak positive skewness (since mean is slightly bigger than the median).\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nDownload the Marks workbook below and open it in Excel. This file contains results from a a class of first year students. We will use Excel’s Data Analysis, Histogram tool to analyse the student results.\n\n\n\n Marks\n\n\n\nNote: Make sure you have the Data Analysis Toolpak enabled in your Excel.\n\nConstruct a histogram for the distribution of marks using the standard Australian grading system: High Distinction (HD) 80-100, Distinction (D) 70-79, Credit (C) 60-69, Pass (P) 50-59 and Fail (N) below 50. Make sure to tidy the histogram up with appropriate axis labels, title and so forth. Present the results in percentage form rather than as total number of people in each category.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nIn this exercise we will explore the link between the standard normal distribution, Z ~ N(mean = 0, variance = 1), and the Student’s t distribution, X ~ Student’s t(d.o.f. = n-1).\nBegin by downloading the Distributions workbook and oepning it in Excel;\n\n\n\n Distributions\n\n\n\n\nOnce you have opened up this workbook:\n\nIn cell B3, calculate the probability associated with the Standard Normal distribution Z value that’s in cell A3, using the “=NORM.S.DIST(A3,false)” function. Drag this formula down to calculate the probabilities for the rest of the Z values.\nIn cell E3, generate the probability associated with the X value from a Student’s t distribution with n = 5 (thus the degrees of freedom will equal 4), using the “=T.DIST(D3,4,false)” function. Drag this formula down to calculate the remaining probabilities.\n\n\nCalculate the values of Z, where Z ~ N(mean = 0, variance = 1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail. Use the “=NORM.INV()” function.\nCalculate the values of X, where X ~ Students t(d.o.f.=n-1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail, and where n=10 such that X ~ Students t(d.o.f.=9). Use either the “=T.INV.2T()” or “=T.INV()” functions. As a bonus, see if you can work out how to use both functions to give you the same answer!\nCalculate the values of X, where X ~ Students t(d.o.f.=n-1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail, and where n=20 such that X ~ Students t(d.o.f.=19).\nCalculate the values of X, where X ~ Students t(d.o.f.=n-1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail, and where n=50 such that X ~ Students t(d.o.f.=49).\nCalculate the values of X, where X ~ Students t(d.o.f.=n-1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail, and where n=100 such that X ~ Students t(d.o.f.=99).\nCalculate the values of X, where X ~ Students t(d.o.f.=n-1), for which 2.5% of the data lies in the lower tail and 2.5% lies in the upper tail, and where n=1000 such that X ~ Students t(d.o.f.=999).\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n1.96\n2.26\n2.09\n2.01\n1.98\n1.96",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analysing Numerical Data</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html",
    "href": "03-Understanding-uncertainty.html",
    "title": "3  Understanding uncertainty",
    "section": "",
    "text": "3.1 Introduction\nSuppose it’s the not too distant future. And you wake up to the following breaking news:\nIgnoring the fact that this would never happen … what is the news headline referring to? Basically, a polling company has conducted a survey to find out who people intend to vote for during the upcoming election. In this case, their results show that 50% of the survey responders intend to vote for me when voting time comes around.\nNow, suppose they had surveyed a reasonable number of people (n = 600) to get these results. So a result of 50% means that 300 / 600 responders said that they would vote for me. Keep in mind that there are approximately 17,000,000 enrolled voters in Australia. So if we assume the results of the poll don’t change (given that people can lie, or change their mind), the only thing we can say with confidence is:\nNow, 0.002% is very different to that of 50%. Given this fact, how can the polling companies possibly conclude that my chances of winning are so high?\nIn this case, they have made an inference. Based on the results of these 600 survey responders, they believe the overall voting population will behave in a similar way.\nIf on the actual day I won by 48% or 53%, then that is pretty close to the polling results so no one would be concerned. But what if I had lost? What if I had only obtained 21% of the votes? This is a massive difference to what the poll had estimated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#introduction",
    "href": "03-Understanding-uncertainty.html#introduction",
    "title": "3  Understanding uncertainty",
    "section": "",
    "text": "Click to see breaking news\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\frac{300}{17,000,000}= 0.002\\)% of Australians will vote for Minh on election day.\n\n\n\n\n\n\n\n\n\nThis is where understanding the concept of uncertainty is essential in making valid statistical conclusions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#populations-and-samples",
    "href": "03-Understanding-uncertainty.html#populations-and-samples",
    "title": "3  Understanding uncertainty",
    "section": "3.2 Populations and Samples",
    "text": "3.2 Populations and Samples\nIn the previous section, I used the example of using political polls to estimate how people will vote on election day. Although a bit outdated now, one of my favourite cases of political polls has to be the 2016 US Presidential election. For those of you who can’t remember, the US President would be decided between two candidates: Hillary Clinton and Donald Trump.\nWhat makes this case interesting is that dozens upon dozens of political polls had Clinton as the clear winner. And as we all know Trump won that election (as well as the one in 2024). So what did the polling companies actually do, and why were they so wrong?\nI’m going to adjust the example a little bit, because by now we all know the outcome of the 2016 US Presidential election. Suppose you are a market researcher investigating the Australian people’s opinion towards Donald Trump. Suppose the research question is:\n\nIf Australians had a vote in the US presidential elections, would they have voted Donald Trump in for a second term?\n\nSuppose I made a list of ALL enrolled voters in Australia and asked each and everyone of them the same (Yes/No) question: “If Australians had a vote in the US presidential elections, would they have voted Donald Trump in for a second term?” After I had asked each and every single person, I can then tally up the results.\nIf the total number of YES’s are greater than the total number of NO’s (or vice versa), then we would have a definitive answer to this question.\n\nWhy is this approach not possible?\n\n\n\n\n\n\n\nClick to check your answer\n\n\n\n\n\nYou cannot survey every single member of a target population due to time and resource constraints. The closet you will get would be conducting a census, and even then, this would not capture 100% of the target population.\n\n\n\nSo what can we do? How do we go about making inferences and answering research questions?\nTo answer a research question, we collect a sample to represent the population of interest. If we want accurate results, then we would want our sample to be as similar to the population as possible. We then conduct our experiments / survey / etc. with our sample and make an inference about the population based upon the results of the sample. This process is known as statistical inference:\n\n\n\n\n\n\n\n\n\n\nIs your sample reliable?\n\n\n\nBut how do we know that our sample is reliable? Consider the image below (assume blue dots represent population and red dots represent sample). Would (a) and (b) be considered good / bad samples based upon the research question?\n\n\n\n\n\n\n\n\n\nClick to check your answer\n\n\n\n\n\nRecall here that the population of interest = Australian voters, so our sample should try and capture this as best as possible.\n\nwould be a bad sample because it only captures those living on the East coast of Australia.\nwould be a bad sample because it is only capturing those living in major cities (ignoring rural / regional areas)\n\n\n\n\n\n\nBack to our research question (If Australians had a vote in the US presidential elections, would they have voted Donald Trump in for a second term?). Suppose three different market research firms collected one sample each to address this question:\n\n\n\nDepending on which sample was selected, we would have a different conclusion for the population!\n\nIf we follow the top one then we would infer that most of the people in the Australian population think “Yes.”\nIf we follow the bottom one, we would infer that most of the people in the Australian population think “No.”\nIf we follow the last one, we would infer that most of the people in the Australian population are “undecided.”\n\nNow, because we are only ever working with samples, we need to acknowledge that all of our inferences are subject to error.\nBack in 2016, the majority of the political polls estimated (inferred) that Clinton would win based upon the people they had surveyed / sampled. This is a list of the polls with the largest samples back in 2016 for this election:\n\n\n\nAs impressive as these samples are, they come nowhere close to capturing ALL of data from the target population, which means, these polls / inferences were subject to uncertainty.\nFortunately, there are ways to minimise this uncertainty - which are explored below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#sampling-bias",
    "href": "03-Understanding-uncertainty.html#sampling-bias",
    "title": "3  Understanding uncertainty",
    "section": "3.3 Sampling Bias",
    "text": "3.3 Sampling Bias\nSampling bias occurs when the method used to select participants leads to a sample that does not accurately represent the population of interest. In other words, certain individuals or groups in the population have a higher or lower chance of being included in the sample, and these differences are systematic, not due to random variation. As a result, any conclusions drawn from the sample may be misleading because they reflect the characteristics of the biased sample rather than the true characteristics of the population.\nSampling bias can arise in several ways. It may occur through inappropriate sampling frames (e.g., surveying only people who attend a gym when studying national exercise habits), voluntary response (where people who choose to participate differ from those who do not), or convenience sampling (selecting individuals who are easy to reach rather than those who are most representative). In each case, the key issue is that the sampling design systematically favours some outcomes over others.\nUnderstanding and preventing sampling bias is essential because even the most sophisticated statistical methods cannot “fix” a fundamentally unrepresentative sample. Careful planning, clear definitions of the target population, and appropriate random sampling techniques help ensure that the sample accurately reflects the population, allowing researchers to make valid and trustworthy inferences.\nThe idea is for each object within the population to be equally likely to be chosen as part of the sample. This is called an unbiased sample. When this is violated, we call it selection bias.\nThere are manay different types of Selection bias, with a few examples being provided below.\n\n\nTypeDescriptionSampling biasSome member of the target population are less likely to be included than others.Attrition biasParticipants who drop out of a study are different from the ones who remain.Volunteer biasParticipants who choose to participate in studies are generally different to those who do not.Survivorship biasSuccessful observations are more likely to be represented than unsuccessful ones.Non-response biasParticipants who refuse to participate or drop out of a study may be different to those who take part.Undercoverage biasParticipants who are inadequately represented.\n\n\nRead through the 2 case studies below to learn more about bias.\n::: {.callout-note collapse = “true”} ## Bias in Sampling: The Raymond Pearl Case Study\nIn 1929, Raymond Pearl of Johns Hopkins University set out to test the hypothesis:\n\n“Does tuberculosis (TB) protect against cancer?”\n\nUsing hospital records, he compared the rates of cancer among patients with and without tuberculosis and concluded that TB appeared to have a protective effect.\n\n\nStatusCancerNo_CancerTotalTB54133187No TB7626831,445\n\n\n\n\nStatusCancerNo_CancerTotalTB0.00620.93380.1146No TB0.16300.83700.8854\n\n\nHowever, this finding was not the result of biology—it was the result of bias in the sampling process.\nAt the time of Pearl’s study, tuberculosis was one of the leading causes of hospitalisation at Johns Hopkins Hospital. This meant that the hospital population—Pearl’s sampling frame—contained a disproportionately large number of patients with TB compared with the general population. When he selected his “control” group (patients without cancer), this control sample was already overrepresented with TB cases simply because so many hospital admissions were TB-related.\nThis introduces a classic case of selection bias: the method used to select participants systematically distorted the groups being compared. The control group was not representative of people without cancer; it was representative of people without cancer who happened to be hospitalised at Johns Hopkins during an era when TB dominated admissions. As a result, the comparison between the “TB group” and the “non-TB group” was fundamentally flawed from the start.\nBecause TB was so common among hospitalised patients, especially those in the “non-cancer” group, Pearl’s analysis made it appear as though TB patients developed cancer less often. In reality, his sampling method had artificially inflated the number of TB cases in his controls. The result was an illusion of a protective effect where none existed.\nPearl’s study highlights how selection bias can make a false relationship appear real. When the sampling frame is distorted—such as relying solely on hospitalised patients during an epidemic of a particular disease—any conclusions drawn about disease interactions become unreliable. Careful consideration of where, how, and from whom data are collected is essential to avoid repeating the same mistakes in modern research.\n:::\n::: {.callout-note collapse = “true”} ## Survivorship Bias: The Abraham Wald Case Study\nDuring World War II, the U.S. military faced a critical problem: many aircraft were being lost in combat, and they needed to strengthen the planes to improve their chances of returning safely. Engineers examined the bullet holes on planes that had returned from missions and noticed consistent patterns—certain parts of the aircraft were riddled with damage, while other areas appeared largely untouched. The initial recommendation was straightforward: reinforce the sections that showed the most bullet holes.\nHowever, statistician Abraham Wald, working with the Statistical Research Group, identified a fundamental flaw in this reasoning. The analysis was based solely on the planes that survived and made it back to base. The aircraft that did not return—the ones shot down—were missing from the dataset. This oversight created a textbook example of survivorship bias: drawing conclusions only from the units that remain in the sample, while overlooking those that have been lost, destroyed, or removed.\n\n\n\nWald realised that the absence of bullet holes in certain areas did not mean those areas were unimportant. Instead, it suggested the opposite: planes hit in those regions were not surviving long enough to be observed. These “missing” planes carried the crucial information. Wald therefore suggested reinforcing the areas with least observed damage, correctly reasoning that these were the true vulnerable points. His insight dramatically improved aircraft survivability and became one of the most famous examples of statistical reasoning under uncertainty.\n:::",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#sampling-methods",
    "href": "03-Understanding-uncertainty.html#sampling-methods",
    "title": "3  Understanding uncertainty",
    "section": "3.4 Sampling Methods",
    "text": "3.4 Sampling Methods\nChoosing an appropriate sampling method is essential for obtaining data that accurately reflect the population of interest. Different sampling strategies have different strengths, limitations, and practical uses. Below are five commonly used methods in introductory statistics.\n\n3.4.1 Convenience Sampling\nConvenience sampling involves selecting individuals who are easiest to reach or most readily available. For example, surveying students walking past the library or collecting responses from friends and colleagues. While this method is quick, inexpensive, and practical, it often produces highly biased samples because the participants may differ systematically from the broader population. Convenience samples are useful for piloting surveys or generating early insights but are generally not suitable for drawing strong conclusions.\n\n\n3.4.2 Simple Random Sampling (SRS)\nIn a simple random sample, every individual in the population has an equal chance of being selected. This can be achieved using random number generators, lottery-style draws, or sampling software. SRS is considered the gold standard of sampling because it minimises selection bias and allows the use of standard statistical inference techniques. However, it can be impractical for very large or geographically dispersed populations.\n\n\n3.4.3 Systematic Sampling\nSystematic sampling involves selecting individuals at regular intervals from an ordered list. For example, choosing every 10th person on a class roll or every 5th customer entering a store. After a random starting point is chosen, the interval (called the sampling step) remains fixed. This method is efficient and easy to implement. However, if the list has an underlying pattern or periodicity that aligns with the interval, the sample may become biased.\n\n\n3.4.4 Cluster Sampling\nCluster sampling involves dividing the population into clusters—often naturally occurring groups such as schools, suburbs, or departments—and then randomly selecting entire clusters to include in the sample. Once selected, all individuals within the chosen clusters (or a random subset of them) are surveyed. This approach is cost-effective for large or geographically spread populations, though it may introduce more variability because people within a cluster tend to be similar to one another.\n\n\n3.4.5 Stratified Sampling\nStratified sampling divides the population into meaningful subgroups (called strata), such as age groups, income brackets, or geographic regions. A random sample is then drawn from each stratum. This method ensures that all key subgroups are represented, often improving precision compared with simple random sampling. Stratified sampling is particularly useful when researchers want to compare outcomes across specific categories or when certain groups are very small but important.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#sampling-distributions",
    "href": "03-Understanding-uncertainty.html#sampling-distributions",
    "title": "3  Understanding uncertainty",
    "section": "3.5 Sampling Distributions",
    "text": "3.5 Sampling Distributions\nSo far we’ve been talking about a population in a fairly intuitive way – for example, all firms listed on the ASX, all households in Victoria, or all daily returns for a particular stock. That’s how an applied researcher in business, finance, or economics naturally thinks: the population is a real collection of economic units “out there in the world”.\nStatisticians, however, like to take one extra step towards abstraction. For them, it’s often more convenient to think of a population not as a big spreadsheet of actual values, but as a probability distribution that could, in principle, have generated those values. This is similar to how we might think of a data-generating process in econometrics.\nFor example, suppose we are interested in weekly returns on a particular share index. To an applied researcher, the population might mean “all weekly returns over an infinite horizon” or “all weeks in some long-run stable period.” A statistician operationalises this by saying: assume the returns follow some probability distribution with certain characteristics – for example, a mean return of \\(0.2\\%\\) per week and a standard deviation of \\(2\\%\\) per week, perhaps approximated by a normal distribution.\nThose underlying numerical characteristics – the true mean, the true variance, the true proportion, and so on – are called population parameters. We typically denote them by Greek letters, for example:\n\npopulation mean: \\(\\mu\\)\npopulation standard deviation: \\(\\sigma\\)\n\nNow imagine you run a study: you take a random sample of 200 weeks of returns and compute the sample mean and sample standard deviation. You might find a sample mean of \\(0.18\\%\\) and a sample standard deviation of \\(2.1\\%\\). These are sample statistics: they are calculated from your data and will vary from one sample to the next.\nThe important distinction is:\n\nPopulation parameters: fixed but unknown numbers (for example, the true long-run mean return), usually denoted by Greek letters.\nSample statistics: numbers you can actually compute from your sample (for example, the sample mean), usually denoted by Latin letters such as \\(\\bar{X}\\) and \\(s\\).\n\nIn practice, we observe statistics and try to infer parameters. Much of this chapter is about how we use sample statistics to estimate population parameters and how uncertain those estimates are.\n\n3.5.1 The law of large numbers\nIn the previous discussion, we saw that with a sample of moderate size, our sample mean might be “in the right ballpark” but not exactly equal to the population mean. A natural question for any business analyst or econometrician is:\n\nIf I want my sample average to be closer to the true population average, what can I do?\n\nThe most obvious answer is: collect more data.\nTo see why this works, imagine we are studying monthly sales revenue for a particular retail chain. Suppose the true (but unknown) population mean revenue per month is $2.5 million. If we sample just 12 months, the sample mean might be $2.3 million, or $2.8 million, or something else quite different – because with small samples, random fluctuation matters a lot.\nIf instead we observe 1,200 months (across many stores or many periods), the sample mean tends to sit much closer to the true \\(\\mu\\). This is not just common sense; it is formalised in a fundamental result called the law of large numbers.\nInformally, the law of large numbers (LLN) says:\n\nAs the sample size \\(n\\) gets larger and larger, the sample mean \\(\\bar{X}\\) tends to move closer and closer to the true population mean \\(\\mu\\).\n\nMore precisely, as \\(n \\to \\infty\\), then \\(\\bar{X} \\to \\mu.\\)\nWe don’t need the formal proof here, but the implication is extremely important:\n\nAny single finite sample will almost always give us a sample mean that is not exactly equal to the population mean.\nHowever, if we keep increasing the sample size, the discrepancy between \\(\\bar{X}\\) and \\(\\mu\\) tends to shrink.\n\nThis is the mathematical justification for a belief that every data analyst already holds intuitively: large samples give more reliable information than small samples.\nThe law of large numbers is reassuring, but it’s a “long-run promise”: it tells us what happens as sample size becomes very large. In real business and economic applications, we usually work with finite samples: perhaps 80 firms, 200 households, 20 years of quarterly GDP, and so on.\nSo we need a more detailed understanding of how sample statistics behave for realistic sample sizes. That’s where the ideas of sampling distributions and the central limit theorem come in.\n\n\n3.5.2 Sampling distribution of the mean\nConsider a simple experiment. Suppose the monthly log returns on a stock are drawn from some fixed distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Now:\n\nYou randomly select \\(n = 5\\) months and compute the sample mean return \\(\\bar{X}_1\\).\nYou repeat the experiment with 5 different months and get another sample mean \\(\\bar{X}_2\\).\nYou keep going, drawing many fresh random samples of 5 months each, and recording each sample mean.\n\nAfter doing this, you would have a long list of numbers:\n\\[\n\\bar{X}_1,\\ \\bar{X}_2,\\ \\bar{X}_3,\\ \\dots\n\\]\nIf you make a histogram of these sample means, you get a sampling distribution of the mean – the distribution of \\(\\bar{X}\\) across repeated samples of the same size from the same population.\nSome observations:\n\nFor \\(n = 5\\), the sample mean is quite volatile from one replication to another.\nThe sampling distribution is typically centred near the true mean \\(\\mu\\), but has noticeable spread.\nIf we increase the sample size (say to \\(n = 30\\) or \\(n = 100\\)), the histogram of sample means becomes more tightly concentrated around \\(\\mu\\).\n\nThis distribution of sample means is not just a thought experiment: it is a key theoretical tool. It tells us how much variability we should expect in \\(\\bar{X}\\) purely due to sampling, even when the data-generating process is unchanged.\n\n\n3.5.3 Sampling distributions exist for any sample statistic\nWe’ve focused on the sample mean, but the same idea applies to any statistic you might compute repeatedly from new samples:\n\nthe maximum monthly return in the sample,\nthe proportion of firms with negative profit,\nthe sample variance,\nregression coefficients, and so on.\n\nFor example, suppose that in each replication of our “5 months” experiment, instead of recording the sample mean return, we record the maximum monthly return. Repeating this many times and plotting those maxima would give the sampling distribution of the maximum.\nThis distribution will look very different from the sampling distribution of the mean:\n\nThe maximum of 5 returns will typically be above the population mean.\nThe shape is often skewed, because extreme values are more likely to appear as the maximum when you draw several observations.\n\nThe general lesson is:\n\nEvery statistic you compute from a sample has a sampling distribution that tells you how that statistic would vary across repeated samples of the same size.\n\nUnderstanding these distributions is at the heart of statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#the-central-limit-theorem",
    "href": "03-Understanding-uncertainty.html#the-central-limit-theorem",
    "title": "3  Understanding uncertainty",
    "section": "3.6 The central limit theorem",
    "text": "3.6 The central limit theorem\nNow we turn to one of the most important results in statistics: the central limit theorem (CLT). It describes how the sampling distribution of the mean behaves as the sample size increases.\nIntuitively, you already know one part of the story: as sample size \\(n\\) increases, the sample mean becomes less variable from sample to sample, so the sampling distribution of the mean becomes narrower.\nWe can formalise this. Suppose the population has mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then the sampling distribution of the mean has:\n\nmean equal to \\(\\mu\\); and\n\nstandard deviation equal to \\(\\sigma / \\sqrt{n}\\).\n\nThis standard deviation of \\(\\bar{X}\\) is called the standard error of the mean (SEM). As \\(n\\) grows, \\(\\sigma / \\sqrt{n}\\) shrinks, so the sampling distribution becomes more concentrated around \\(\\mu\\).\nThe remarkable part of the CLT is about shape:\n\nNo matter what the shape of the original population distribution is, as the sample size \\(n\\) becomes large, the sampling distribution of the mean becomes approximately normal.\n\nThat is, even if the underlying data are skewed (e.g. income, firm size, or sales distributions), the distribution of \\(\\bar{X}\\) is close to normal for reasonably large \\(n\\).\nSo the CLT tells us that for the sample mean:\n\nCentre:\n\\[\nE[\\bar{X}] = \\mu\n\\]\nSpread:\n\\[\nSD(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nShape: approximately normal for large \\(n\\).\n\nThis is incredibly useful in business and econometrics because it justifies the widespread use of normal-based methods (z-scores, t-tests, confidence intervals, regression inference) even when the raw data are not perfectly normal, as long as the sample size is not tiny.\n\n\n3.6.1 Estimating population parameters\nUp to now, we’ve often talked as if the population parameters were known: for example, “suppose returns have mean \\(\\mu\\) and standard deviation \\(\\sigma\\).” In real data analysis, the exact values of \\(\\mu\\) and \\(\\sigma\\) are almost never known.\nInstead, we:\n\ncollect a sample,\ncompute sample statistics (like \\(\\bar{X}\\) and \\(s\\)), and\ntreat these as estimates of the underlying population parameters.\n\nFor example, suppose we collect data on daily returns of a stock over 250 trading days (roughly one year) and compute:\n\nsample mean return \\(\\bar{X} = 0.05\\%\\)\nsample standard deviation \\(s = 1.3\\%\\)\n\nWe don’t know the true long-run mean and volatility of this stock. But if we had to give a “best guess” for the population mean, we would naturally choose \\(\\bar{X}\\). Likewise, we want a good procedure for estimating \\(\\sigma\\).\nIn the rest of this section, we distinguish clearly between:\n\nSample statistics: exact numbers computed from the sample.\nEstimators/estimates: rules and resulting values we use to guess the corresponding population parameters.\n\n\n\n\n3.6.2 Estimating the population mean\nLet \\(\\mu\\) be the true population mean (for example, the long-run average return), and let \\(\\bar{X}\\) be the sample mean from a random sample.\nA natural estimator of \\(\\mu\\) is:\n\\[\n\\hat{\\mu} = \\bar{X}.\n\\]\nHere:\n\n\\(\\bar{X}\\) is the sample statistic – a known quantity once we have the data.\n\\(\\hat{\\mu}\\) is the estimate of the population mean – conceptually a “best guess” about \\(\\mu\\) based on \\(\\bar{X}\\).\n\nIn simple random sampling, these two numbers coincide: numerically, \\(\\hat{\\mu} = \\bar{X}\\). But conceptually they are different:\n\n\\(\\bar{X}\\) describes the sample.\n\\(\\hat{\\mu}\\) is a statement about the population.\n\nWhy is \\(\\bar{X}\\) a good choice as an estimator? Because, as we saw earlier, it is unbiased:\n\\[\nE[\\bar{X}] = \\mu.\n\\]\n\n\n\n\n\n\nClick for the mathematical proof\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\overline{X}\n&= \\frac{1}{n} \\sum_{i=1}^{n} X_i\n   = \\frac{1}{n}(X_1 + X_2 + \\cdots + X_n) \\\\[12pt]\nE[\\overline{X}]\n&= E\\left[ \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right]\n= \\frac{1}{n} \\sum_{i=1}^{n} E[X_i]\n= \\frac{1}{n} \\sum_{i=1}^{n} \\mu\n= \\frac{n}{n} \\mu\n= \\mu\n\\end{aligned}\n\\]\n\n\n\nOn average across many possible samples, the sample mean hits the right target. That’s exactly what we want from a good estimator.\nYou can think of it this way for business data: if you repeatedly sampled 100 firms from a very large population and each time computed the sample mean of annual profit, the average over all those sample means would equal the true population mean profit.\n\n\n3.6.3 Estimating the population standard deviation\nEstimating the population standard deviation \\(\\sigma\\) is a bit more subtle.\nWe know the formula for the sample standard deviation often used in practice:\n\\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2},\n\\]\nand also the version with \\(n\\) in the denominator:\n\\[\ns_n = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}.\n\\]\n\n\n\n\n\n\nClick for the mathematical proof\n\n\n\n\n\nTo derive the variance of \\(\\overline{X}\\) mathematically, let’s start by denoting the variance of \\(X\\) as\n\\(\\mathrm{Var}(X) = \\sigma^2\\). With a random sample, we can derive the variance of \\(\\overline{X}\\) as a function of \\(\\mathrm{Var}(X)\\).\n\\[\n\\begin{aligned}\n\\mathrm{Var}(\\overline{X})\n&= \\mathrm{Var}\\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) \\\\[6pt]\n&= \\frac{1}{n^2} \\mathrm{Var}\\left( \\sum_{i=1}^{n} X_i \\right) \\\\[10pt]\n&= \\frac{1}{n^2} \\left( \\mathrm{Var}(X_1) + \\mathrm{Var}(X_2) + \\cdots + \\mathrm{Var}(X_n) \\right) \\\\[10pt]\n&= \\frac{1}{n^2} (\\sigma^2 + \\sigma^2 + \\cdots + \\sigma^2) \\\\[6pt]\n&= \\frac{1}{n^2} (n \\sigma^2) \\\\[6pt]\n&= \\frac{1}{n} \\sigma^2\n\\end{aligned}\n\\]\nSo:\n\\[\n\\mathrm{Var}(\\overline{X}) = \\frac{\\sigma^2}{n}.\n\\]\nN.B. This result required a random sample, so that each \\(X_i\\) is independent of the others.\nDividing by the sample size \\(n\\), a positive value which is generally larger than 1, makes it clear that\n\\(\\overline{X}\\) has a smaller variance than \\(X\\). This makes intuitive sense: averaging a set of values reduces variation.\nPutting this all together:\n\\[\nX \\sim N(\\mu, \\sigma^2) \\quad \\Longrightarrow \\quad\n\\overline{X} \\sim N\\!\\left(\\mu,\\; \\frac{\\sigma^2}{n}\\right).\n\\]\n\n\n\nWhich one should we use as an estimator of the population standard deviation?\nTo understand the issue, imagine extremely small samples. Suppose your sample size is \\(n = 1\\). Then your sample consists of a single observation, say a single firm with profit \\(X_1 = 2.3\\) million.\n\nThe sample mean is \\(\\bar{X} = 2.3\\).\nThe sample standard deviation (no matter how you define it) is \\(0\\), because all observations equal the mean.\n\nAs a description of the sample, SD \\(= 0\\) is correct – there is no variation within a single observation. But as an estimate of the population standard deviation, “0” is clearly absurd. We don’t really believe that all firms in the economy have exactly the same profit.\nEven when \\(n = 2\\), we get very unstable information about variability. With only two observations (say, profits of 1.8 and 2.8 million), the sample standard deviation tends, on average, to underestimate the true variability in the population. Intuitively, with very little data, we haven’t given the population enough opportunity to show us its true spread.\nMathematically, for small samples it turns out that the “naïve” variance\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\]\nis a biased estimator of the population variance \\(\\sigma^2\\): on average it is too small. The standard fix (which you’ve probably seen in earlier courses) is to divide by \\(n-1\\) instead of \\(n\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2.\n\\]\nThis adjusted quantity \\(s^2\\) is an unbiased estimator of \\(\\sigma^2\\), and the corresponding \\(s\\) is our usual estimator of \\(\\sigma\\).\nThat’s why in software and textbooks you almost always see the denominator \\(n-1\\) for variance and standard deviation: it corrects the systematic downward bias that occurs if we divide by \\(n\\).\nConceptually, it’s worth keeping the roles clear:\n\nThe sample standard deviation as a description of the sample’s spread would naturally use division by \\(n\\).\nThe estimated population standard deviation uses division by \\(n-1\\) because that produces an estimator whose expected value equals the true \\(\\sigma\\).\n\nIn practice, people often blur this distinction and casually refer to “the sample standard deviation” when they actually mean the \\(n-1\\) version (the estimator of \\(\\sigma\\)). For most applied work in business and econometrics, this imprecision in language is not a major problem, as long as you remember:\n\nYou are almost always using the version that is designed to estimate the population standard deviation, not merely to describe the sample.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#confidence-intervals",
    "href": "03-Understanding-uncertainty.html#confidence-intervals",
    "title": "3  Understanding uncertainty",
    "section": "3.7 Confidence intervals",
    "text": "3.7 Confidence intervals\nSo far, we have focused on point estimation: using a single number such as \\(\\hat{\\mu} = \\bar{X}\\) as our best guess for the population mean \\(\\mu\\). In practice, though, every business analyst and econometrician knows that a single number never tells the full story. We also want to know how uncertain that estimate is.\nThis is where confidence intervals come in. Instead of reporting just a point estimate, we report a range of plausible values for the population mean, based on our sample and a chosen level of confidence (often 95%).\n\n3.7.1 The basic idea\nSuppose we have a random sample of size \\(n\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), and we compute the sample mean \\(\\bar{X}\\). We know:\n\n\\(\\bar{X}\\) is centred around \\(\\mu\\).\nThe standard deviation of \\(\\bar{X}\\) (its standard error) is \\[\nSE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}.\n\\]\n\nBecause of the central limit theorem, for reasonably large \\(n\\) the sampling distribution of \\(\\bar{X}\\) is approximately normal. That means we can use the standard normal fact that:\n\\[\nP(-z_{0.975} \\le Z \\le z_{0.975}) \\approx 0.95,\n\\]\nwhere \\(Z\\) is a standard normal variable and \\(z_{0.975} \\approx 1.96\\) is the 97.5th percentile of the standard normal distribution.\nIf \\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}},\n\\] then\n\\[\nP\\!\\left(\n-1.96 \\le \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\le 1.96\n\\right) \\approx 0.95.\n\\]\nRearranging this inequality to solve for \\(\\mu\\) gives\n\\[\nP\\!\\left(\n\\bar{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le\n\\bar{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\n\\right) \\approx 0.95.\n\\]\nThis suggests a 95% confidence interval for \\(\\mu\\) of the form\n\\[\n\\bar{X} \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}.\n\\]\nOf course, in real life we almost never know \\(\\sigma\\), so this is mainly a theoretical starting point.\n\n\n\n3.7.2 Unknown \\(\\sigma\\): using the \\(t\\)-distribution\nIn practice, we replace the unknown population standard deviation \\(\\sigma\\) with its estimate \\(s\\). This introduces extra uncertainty, especially for smaller \\(n\\), so instead of using the standard normal distribution we use the Student \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nThe standard error is now estimated as\n\\[\nSE(\\bar{X}) \\approx \\frac{s}{\\sqrt{n}},\n\\]\nand a \\((1 - \\alpha)\\times 100\\%\\) confidence interval for \\(\\mu\\) becomes\n\\[\n\\bar{X} \\pm t_{n-1,\\,1-\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}},\n\\]\nwhere \\(t_{n-1,\\,1-\\alpha/2}\\) is the critical value from the \\(t\\)-distribution with \\(n-1\\) degrees of freedom.\nFor a 95% confidence interval, we use \\(t_{n-1,\\,0.975}\\).\n\n\n3.7.3 A business example\nSuppose a retailer wants to estimate the average weekly online revenue. They collect a simple random sample of \\(n = 52\\) weeks and calculate:\n\nsample mean revenue: \\(\\bar{X} = \\$410{,}000\\)\nsample standard deviation: \\(s = \\$80{,}000\\)\n\nThe estimated standard error of the mean is\n\\[\nSE(\\bar{X}) = \\frac{s}{\\sqrt{n}}\n= \\frac{80{,}000}{\\sqrt{52}}\n\\approx 11{,}111.\n\\]\nWith \\(n = 52\\), the degrees of freedom are \\(51\\). The 95% \\(t\\)-critical value is approximately \\(t_{51,\\,0.975} \\approx 2.01\\).\nThe 95% confidence interval for the population mean weekly revenue is then\n\\[\n\\bar{X} \\pm t_{51,\\,0.975} \\cdot SE(\\bar{X})\n= 410{,}000 \\pm 2.01 \\times 11{,}111.\n\\]\nSo the margin of error is about\n\\[\n2.01 \\times 11{,}111 \\approx 22{,}344,\n\\]\nand the 95% confidence interval is approximately\n\\[\n[410{,}000 - 22{,}344,\\ 410{,}000 + 22{,}344]\n= [387{,}656,\\ 432{,}344].\n\\]\nA sensible way to report this might be:\n\nWe estimate that the true average weekly online revenue lies between $388k and $432k, with 95% confidence.\n\n\n\n3.7.4 How to interpret a confidence interval (and how not to)\nA 95% confidence interval does not mean there is a 95% probability that \\(\\mu\\) lies in the specific interval you just calculated. The population mean is a fixed (though unknown) number; it either is or is not in that interval.\nThe correct interpretation is:\n\nIf we were to repeat this entire sampling process many times, and each time construct a 95% confidence interval in exactly the same way, then about 95% of those intervals would contain the true population mean \\(\\mu\\).\n\nYour particular interval is one of those many possible intervals. You don’t know whether yours is one of the 95% that contain \\(\\mu\\) or one of the 5% that miss it, but the method you are using has a 95% “success rate” in the long run.\n\n\n3.7.5 Factors that affect the width of a confidence interval\nFor practical decision-making in business and econometrics, it is useful to understand what controls the width of a confidence interval for the mean:\n\nSample size \\(n\\)\nThe standard error is proportional to \\(1/\\sqrt{n}\\).\nLarger \\(n \\Rightarrow\\) smaller \\(SE \\Rightarrow\\) narrower interval.\nThis is the statistical payoff to collecting more data.\nVariability \\(s\\) in the data\nHigher variability in the underlying data (larger \\(s\\)) makes the interval wider.\nHighly volatile markets or very heterogeneous firms require larger samples to achieve a given precision.\nConfidence level\nA 99% confidence interval uses a larger critical value than a 95% interval, so it is wider.\nThere is a trade-off: higher confidence \\(\\Rightarrow\\) wider intervals.\n\nIn practice, analysts must balance precision (narrow intervals) against costs of data collection and desired confidence level.\n\n\n3.7.6 Why confidence intervals matter\nConfidence intervals are crucial because they:\n\nquantify uncertainty around an estimate,\n\nallow comparisons across groups (e.g., mean sales in region A vs region B),\n\nprovide a basis for formal hypothesis tests (e.g., does a CI for a mean difference include 0?), and\n\ncommunicate results to decision-makers in a more informative way than a single point estimate.\n\nFor example, telling a manager that “average weekly revenue is estimated at $410k” is less informative than saying “we are 95% confident the true average falls between $388k and $432k.” The interval directly conveys both magnitude and uncertainty, which is exactly what good business and econometric analysis should aim to provide.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#excel",
    "href": "03-Understanding-uncertainty.html#excel",
    "title": "3  Understanding uncertainty",
    "section": "3.8 Excel",
    "text": "3.8 Excel\nBelow we walk through how to use the ToolPak to generate descriptive statistics—including Excel’s estimate of the mean, standard deviation, and a confidence level for the mean—and how to convert those results into a confidence interval. Begin by downloading the data file below and then follow the instructions by navigating through the screenshots.\n\n\n\n Salary\n\n\n\n\n12\n\n\nSwitch to the Data tab and select Data Analysis. Select Descriptive Statistics and enter in the input range, select labels, output location, and make sure to select “summary statistics” and “confidence level.” By default the confidence level is set at 95%, and we can adjust this as needed.\n\n\n\n\n\n\n\n\n\n\n\nThe summary statistics will provided us with confidence levels (this is like the margin of error) in the table. To obtain confidence intervals we subtract and add this confidence level from the mean. See the screenshot below.\nwe can adjust this as needed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#summary",
    "href": "03-Understanding-uncertainty.html#summary",
    "title": "3  Understanding uncertainty",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis chapter introduced the core ideas behind statistical inference—how we use information from a sample to draw conclusions about a larger population. We began by distinguishing between population parameters (the true values we care about) and sample statistics (the values we calculate from data). Because we almost never observe an entire population, our conclusions always involve some degree of uncertainty, and our samples must be collected carefully to avoid bias. Real-world examples, such as political polling errors and famous case studies like Pearl’s TB research and Wald’s aircraft analysis, showed how flawed sampling methods can lead to misleading or even completely incorrect conclusions.\nWe also explored the behaviour of sample statistics across repeated sampling through the idea of sampling distributions. The law of large numbers tells us that larger samples tend to produce more accurate estimates, while the central limit theorem explains why the sample mean is approximately normally distributed even when the population is not. These results justify many of the common inferential tools used in business, economics, and data analysis.\nFinally, we introduced the concept of estimating population parameters, including why the sample mean is an unbiased estimator of the population mean and why the \\(n-1\\) version of variance is used to estimate the population standard deviation. We concluded with confidence intervals, which provide a practical way to express uncertainty by giving a range of plausible values for the population mean rather than a single point estimate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "03-Understanding-uncertainty.html#exercises",
    "href": "03-Understanding-uncertainty.html#exercises",
    "title": "3  Understanding uncertainty",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nExplain briefly why voluntary participation is an unreliable way to collect a sample to be used for estimation.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nThe aim of a sample is to be representative of the population, so that same estimates are unbiased estimates of population parameters. Voluntary participation will likely mean certain types of people are more likely to respond, and these types may represent one set of views in the survey, not representative of the whole population.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nExplain the idea of a biased sample and a representative sample, and discuss whether this method of taking a sample is appropriate.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nA biased sample is one that is chosen so that it does not represent the characteristics of the population. It is called biased because some characteristics may be over-represented, others missing. A representative sample reflects the characteristics of the population well. This method is appropriate because it is based on a random sample of claims, meaning it should be representative.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nIf you wanted a 99% confidence interval instead of a 95% confidence interval, would it be wider or narrower? Explain your reasoning.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n99% is wider, because we need to have a more conservative range, to give greater certainty that the true value is in that interval.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nThe 95% confidence interval for mean is given by the following formula:\n\\[\n\\bar{X} - t \\cdot \\frac{s}{\\sqrt{n}}\n\\quad \\text{to} \\quad\n\\bar{X} + t \\cdot \\frac{s}{\\sqrt{n}}\n\\]\n\nUse the formula to explain what happens to my confidence interval if I am able to get more data and my sample doubles in size.\nImagine somehow that the population standard deviation was known instead of having to be estimated. What would change in the confidence interval formula, and would my interval be narrower or wider?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nWhen n doubles: bottom line of the s/sqrt(n) is bigger, so the whole expression is smaller, so the interval is narrower. This makes sense: more data, estimate more accurately, and hence can get same level of confidence with a narrower interval.\nReplace t with normal distribution value, and Z value is smaller than t, so interval gets narrower. This makes sense – less uncertainty, narrower interval for same level of confidence. Also replace s (sample SD) with σ (population SD).\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nThis question uses data from a numeracy test performed with Secondary School students in Timor-Leste in 2019. The data can be found in the “Test Data” sheet.\n\n\n\n Test Data\n\n\n\n\nMore than 1,800 students were tested across 60 schools, with around 30 students per school. The schools were randomly chosen, and then 30 students were randomly chosen from the class lists of Grade 8 and 9 students enrolled at the school. Independent researchers then supervised the tests with those 30 students. The numeracy test comprised 19 questions. A range of other information was also collected from each child, including family income and parents’ education. Let us consider the data on family income.\nUse Excel to get the descriptive statistics for the monthly family income per person for the full dataset, then answer the following questions:\n\nCalculate a 95% confidence interval for the mean income per person. Interpret that interval in everyday language.\nIn this case, the confidence interval you calculated will turn out to be very narrow. What does this say about the accuracy of your estimate? Knowing how confidence intervals are calculated, what is the main reason the interval is so narrow in this case?\nThe Official Poverty Line for Timor-Leste is $57 per person per month. Is it valid to use a poverty rate calculated from this particular sample as an estimate of the poverty rate for the whole country? Explain your logic – think carefully about who the sample used in this study includes.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n70.96 - 1.24, to 70.96 + 1.24. This is $69.72 to $72.20. We are 95% sure that the true average monthly family income per person is between $69.72 and $72.20.\nIt is a very accurate estimate. The accuracy depends on standard deviation (s) and sample size (n). Standard deviation is not that small ($27), but it’s a huge sample of 1838 people (n), so this allows us to get a very accurate estimate.\nNot strictly valid, as this sample only comprises household with children in Grade 8 or 9. So, for example, households with only young children would not be included. So, the sample does not represent all households in the country.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nThis question uses real data from Timor-Leste looking at children’s heights. The dataset has data on almost 9000 children in Timor-Leste aged between 0 and 5 years old. Each row of the table provides information on one child.\nThe following statistics relate to the heights of mothers of the children in the sample.\n\n\nStatisticmum_heightMean150.830400Standard Error0.057403Median150.650000Mode149.500000Standard Deviation5.390395Sample Variance29.056360Kurtosis1.741754Skewness0.353652Range74.399990Minimum124.500000Maximum198.900000Sum1,330,022.000000Count8,818.000000Confidence Level (95%)0.112524\n\n\nThe formula for confidence intervals for the mean is given here:\n\\[\n\\bar{X} - t \\cdot \\frac{s}{\\sqrt{n}}\n\\quad \\text{to} \\quad\n\\bar{X} + t \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nUse this output to construct a 95% confidence interval for the average height of mothers of young children, and give a technical interpretation of this interval, based on the theory of repeated samples. Also refer to the formula for confidence intervals to explain why this interval is very narrow in this case.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nInterval: 150.83 - 0.11 to 150.83 + 0.11, so 150.72cm to 150.94cm\nIf we took many samples of size 8818 from the population of all mothers, and calculated a 95% confidence interval for each using the method above, we would find that 95% of these intervals would contain the true mean height. So, we can conclude that there is a 95% chance that this interval contains the true mean height. i.e. We are 95% confident the true mean height for mothers is between 150.72cm and 150.94cm.\nThe interval is narrow mainly because of the large sample size, n = 8818. The formula requires dividing by sqrt(n), so a large n will mean we divide by a big number, making the interval narrower. This makes intuitive sense: if you have a big sample, you can estimate the mean more accurately, as you have more data on which to base the estimate\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nFor this exercise use the sheet “Timor-Leste”. This dataset represents a census – a list of all the households in this particular village. Suppose we want to find out more about the characteristics of households in this village, but do not have the budget to re-interview everyone – we want to take a sample that is representative of the whole population. How do we take a representative sample (of say, 50 households) from this list (population) of households?\n\nUse Excel’s random number generator to take a ‘good’ (i.e. random) sample of size 50 of this population.\nAlso, think of some ‘bad’ (i.e. non-random) ways of taking a sample.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nThere are a few ways we could approach this, some a lot better than others:\n\nTake the first 50 households?\n\nIn this case, taking (‘sampling’) the first 50 households in the list would produce a particularly BIASED sample. Why? Take a look at the characteristics of the first 50 households in the spreadsheet. Household ID is a unique identifier – just a number – allocated to the household. But it turns out that this was essentially the order in which households were interviewed. Imagine the census interviewer arriving in the village, interviewing house by house. Looking at the “Time to Road” column, the interviewer clearly started interviewing along the main road. So, if we were to take the first 50 households, we would essentially be taking the 50 households close to the main road. And we know that those on the main road are quite different to those who do live away from it (much more likely to have electricity, for example).\n\nTake Every j’th House?\n\nThis is an approach that is often used in practice, as it can be administered easily. We have 374 households in this village, so selecting every 7-8th house (374/50=7.5) will give us 50. If there is some kind of systematic ordering of the data, this method will not produce a representative sample.\n\nUse a Random Number Generator\n\nExcel can be used to draw a random sample. There are a few ways of doing this in Excel; we will use the random number generator in the Data Analysis Toolpak. The process for drawing a random sample of 50 households from our population of 374 is as follows: we assign each household a number randomly. What is important is that each household (row in our spreadsheet) has equal chance of being selected. Once each household has been assigned a number, we rank (sort) households based on their number, smallest to largest. The top 50 households in the list become our sampled households.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding uncertainty</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html",
    "href": "04-Models-of-relationships-between-data.html",
    "title": "4  Models of relationships between data",
    "section": "",
    "text": "4.1 Introduction\nStatistics is fundamentally about making sense of data. At its core, modern statistics blends art and science to help us extract meaningful patterns, uncover relationships, and generate useful predictions. One of the primary ways we do this is through models—simplified representations of complex real-world processes.\nWhenever we analyse data to answer a question, solve a problem, or understand a system, we are implicitly or explicitly building a model. And although statistical models may sound technical, the idea of modelling is something we all practice constantly. Every time we interpret information, anticipate what might happen next, or make a decision based on past experience, we are relying on informal mental models of the world.\nTo illustrate, imagine a team of designers developing a new smartphone. They would never jump straight into manufacturing without planning. Instead, they study existing phones, gather data on battery performance, screen durability, camera quality, and user behaviour, and then construct a prototype. The prototype is not a perfect replica of the final product, but a simplified version that captures the essential features. It can then be tested—perhaps by running stress tests or user trials—to predict how the final device will perform in real-world conditions.\nIn statistics, this same idea appears in the concept of model fit: how well a model represents the patterns in the data.\nA well-fitting model behaves much like the real system, producing reliable predictions. A moderately fitting model captures some aspects of reality but misses others, leading to weaker predictions. A poorly fitting model bears little resemblance to the real world and provides unreliable or misleading results.\nThis concept is just as important in business and economics. Consider a company using historical sales data (2017–2020) to forecast sales for 2021. A model that simply predicts the average of past sales ignores trends and seasonality (Model 1) — clearly a poor fit. A trending-only model (Model 2) improves slightly but still misses seasonal peaks. A model that incorporates both trend and seasonal patterns fits the data more closely and produces more credible forecasts.\nThese examples emphasise a key insight: a model is only as useful as its ability to reflect the underlying data-generating process.\nIn the chapters ahead, we will gradually work our way toward the most fundamental and widely used statistical modelling tool: linear regression. Before we get there, we first need a solid understanding of how to model relationships between variables, how to assess model fit, and how to use models responsibly to answer questions about the world.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#covariance-and-correlation",
    "href": "04-Models-of-relationships-between-data.html#covariance-and-correlation",
    "title": "4  Models of relationships between data",
    "section": "4.2 Covariance and Correlation",
    "text": "4.2 Covariance and Correlation\nSo far we’ve talked about how to summarise a single variable. In practice, though, we are usually interested in how two variables move together. Do people who spend more time on TikTok also spend more money online? Do higher advertising budgets go hand-in-hand with higher sales? To answer questions like these we need tools that describe relationships between variables, not just each variable on its own. :contentReferenceoaicite:0\n\n4.2.1 A motivating example\nImagine a participant, Charles, who is taking part in a marketing study. At the start of each week, researchers assign him a certain number of hours of marketing videos to watch. Over the same week they record:\n\nhow much money he spends on online shopping; and\n\nhow many minutes of exercise he does.\n\nAcross several weeks a pattern emerges:\n\n\n\n\n\nIn weeks with more marketing, Charles tends to spend more online.\nIn those same weeks, he tends to exercise less.\n\nIf we plot “time spent watching marketing videos” against “amount spent online”, the points slope upwards: large values of one tend to go with large values of the other. If we instead plot “time spent watching marketing videos” against “exercise minutes”, the points slope downwards: large values of one tend to go with small values of the other.\nWhat we are noticing informally is that some pairs of variables rise and fall together, while others move in opposite directions. We now want a numerical measure that captures this idea.\n\n\n4.2.2 What is covariance?\nCovariance is a measure of how two variables vary jointly around their means. For two variables \\(X\\) and \\(Y\\) measured on the same \\(n\\) individuals, the sample covariance is\n\\[\n\\operatorname{Cov}(X,Y)\n= \\frac{1}{n-1} \\sum_{i=1}^{n}\n\\bigl(x_i - \\bar{x}\\bigr)\\bigl(y_i - \\bar{y}\\bigr).\n\\]\nThe logic mirrors the variance:\n\nFor variance, we take deviations from the mean \\((x_i - \\bar{x})\\), square them, and average to see how a single variable spreads around its mean.\nFor covariance, we multiply paired deviations\n\\((x_i - \\bar{x})(y_i - \\bar{y})\\) and average them to see whether the two variables wander away from their means in the same direction or in opposite directions.\n\nThis leads to three broad possibilities:\n\nPositive covariance: when \\(x_i\\) is above \\(\\bar{x}\\), \\(y_i\\) also tends to be above \\(\\bar{y}\\); when one is below its mean, the other tends to be below as well. The product of deviations is mostly positive, so the average is positive. In Charles’ case, “time watching marketing videos” and “online spending” have positive covariance.\nNegative covariance: when \\(x_i\\) is above \\(\\bar{x}\\), \\(y_i\\) tends to be below \\(\\bar{y}\\), and vice versa. Products of deviations are mostly negative, giving a negative covariance. This describes the relationship between Charles’ “time watching marketing videos” and his “exercise minutes”.\nCovariance near zero: there is no consistent pattern; sometimes the deviations have the same sign, sometimes opposite, and they cancel out on average. In that case the variables are said to have (approximately) no linear relationship.\n\n\n\n\n\n\n\nA worked example - Positive Covariance\n\n\n\nIn the example below, each pair is moving in the same direction relative to their means, i.e. negative-negative, positive-positive. When this is applied to the formula, we obtain a positive covariance.\n\n\n\n\n\n\n\n\n\n\n\nA worked example - Negative Covariance\n\n\n\nIn this example, most of the pairs are moving in opposite directions, e.g. negative-positive, positive-negative. When this is applied to the formula, we receive a negative covariance.\n\n\n\n\n\nA useful way to think about covariance is:\n\nAre high values of \\(X\\) paired with high values of \\(Y\\) (positive), low values of \\(Y\\) (negative), or a bit of both (around zero)?\n\n\n\n4.2.3 Why raw covariance is hard to interpret\nCovariance is excellent for telling us the direction of a relationship, but much less helpful for telling us the strength of that relationship.\nThe problem is that covariance depends on the units of measurement:\n\nIf online spending is measured in dollars, the covariance might be \\(42.5\\).\nIf we re-express the same spending in cents, every spending value is multiplied by 100, and the covariance is multiplied by 100 as well (e.g. \\(4250\\)).\n\nThe underlying relationship between time and spending has not changed at all, but the numerical value of the covariance has. There is no natural upper or lower bound, and so it is difficult to look at a covariance of, say, \\(150\\) and know whether that is “large” or “small”. We need a standardised version.\n\n\n4.2.4 From covariance to correlation\nTo obtain a measure that is independent of units, we standardise the covariance by dividing by the standard deviations of each variable:\n\\[\nr_{XY} =\n\\frac{\\operatorname{Cov}(X,Y)}{s_X s_Y},\n\\]\nwhere \\(s_X\\) and \\(s_Y\\) are the sample standard deviations of \\(X\\) and \\(Y\\). This standardised measure is the correlation coefficient.\nSome key properties of the correlation coefficient:\n\nIt always lies between \\(-1\\) and \\(+1\\).\nThe sign (positive or negative) tells us the direction of the relationship, just like covariance.\nThe magnitude tells us the strength of the linear relationship:\n\nvalues near \\(\\pm 1\\): very strong linear relationships;\nvalues near 0: weak or no linear relationship.\n\nBecause it is unit-free, a correlation of \\(0.8\\) means the same “strength” whether we measure income in dollars or euros, or height in centimetres or inches.\n\nVisually, if we plot \\(X\\) and \\(Y\\) in a scatterplot:\n\ncorrelations close to \\(+1\\) produce points that hug an upward-sloping straight line;\ncorrelations close to \\(-1\\) hug a downward-sloping line;\ncorrelations near 0 produce a diffuse cloud with no obvious linear pattern.\n\n\n\n\n\n\n\n\n\n\nA worked example - Correlation\n\n\n\nTo illustrate how correlation is computed, consider the following 10 paired observations for variables \\(X\\) and \\(Y\\):\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(y_i\\)\n\n\n\n\n1\n12\n10\n\n\n2\n15\n14\n\n\n3\n9\n11\n\n\n4\n18\n16\n\n\n5\n11\n9\n\n\n6\n20\n19\n\n\n7\n14\n13\n\n\n8\n7\n8\n\n\n9\n16\n15\n\n\n10\n10\n9\n\n\n\nStep 1 — Compute the means\n\\[\n\\bar{x} = \\frac{1}{10}\\sum_{i=1}^{10} x_i = \\frac{12+15+9+18+11+20+14+7+16+10}{10} = 13.2\n\\]\n\\[\n\\bar{y} = \\frac{1}{10}\\sum_{i=1}^{10} y_i = \\frac{10+14+11+16+9+19+13+8+15+9}{10} = 12.4\n\\]\nStep 2 — Compute deviations from the mean\nFor each pair:\n\\[\ndx_i = x_i - \\bar{x}, \\qquad dy_i = y_i - \\bar{y}\n\\]\nWe also compute the product \\(dx_i \\cdot dy_i\\):\n\n\n\n\\(i\\)\n\\(dx_i\\)\n\\(dy_i\\)\n\\(dx_i dy_i\\)\n\n\n\n\n1\n\\(-1.2\\)\n\\(-2.4\\)\n\\(2.88\\)\n\n\n2\n\\(1.8\\)\n\\(1.6\\)\n\\(2.88\\)\n\n\n3\n\\(-4.2\\)\n\\(-1.4\\)\n\\(5.88\\)\n\n\n4\n\\(4.8\\)\n\\(3.6\\)\n\\(17.28\\)\n\n\n5\n\\(-2.2\\)\n\\(-3.4\\)\n\\(7.48\\)\n\n\n6\n\\(6.8\\)\n\\(6.6\\)\n\\(44.88\\)\n\n\n7\n\\(0.8\\)\n\\(0.6\\)\n\\(0.48\\)\n\n\n8\n\\(-6.2\\)\n\\(-4.4\\)\n\\(27.28\\)\n\n\n9\n\\(2.8\\)\n\\(2.6\\)\n\\(7.28\\)\n\n\n10\n\\(-3.2\\)\n\\(-3.4\\)\n\\(10.88\\)\n\n\n\nSum of products:\n\\[\n\\sum dx_i dy_i = 127.08\n\\]\nStep 3 — Compute the sum of squared deviations\nFor \\(X\\):\n\\[\n\\sum dx_i^2 =\n(-1.2)^2 + (1.8)^2 + \\cdots + (-3.2)^2 = 149.6\n\\]\nFor \\(Y\\):\n\\[\n\\sum dy_i^2 =\n(-2.4)^2 + (1.6)^2 + \\cdots + (-3.4)^2 = 112.4\n\\]\nStep 4 — Compute the correlation\nCorrelation is defined as:\n\\[\nr =\n\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y}) }\n{\\sqrt{\\sum (x_i - \\bar{x})^2 } \\;\n\\sqrt{\\sum (y_i - \\bar{y})^2 }}\n\\]\nSubstituting:\n\\[\nr =\n\\frac{127.08}{\\sqrt{149.6} \\, \\sqrt{112.4}}\n\\]\nCompute denominators:\n\\[\n\\sqrt{149.6} = 12.228,\n\\qquad\n\\sqrt{112.4} = 10.602\n\\]\nThus:\n\\[\nr = \\frac{127.08}{12.228 \\times 10.602}\n\\]\n\\[\nr = \\frac{127.08}{129.62} \\approx 0.98\n\\]\nInterpretation\n\nThe correlation \\(r = 0.98\\) indicates a very strong positive linear relationship between \\(X\\) and \\(Y\\).\nAlmost all the variation in one variable corresponds to variation in the other.\nThis example demonstrates how correlation is simply a standardised version of covariance, built from deviations from the mean and scaled by the spread of each variable.\n\n\n\n\n\n4.2.5 Correlation Does Not Imply Causation\nA correlation tells us how strongly two variables move together, but it tells us nothing about why they move together or whether changes in one variable cause changes in the other.\nMany students (and many adults!) fall into the trap of assuming that a strong correlation means one variable causes the other. This is often incorrect and can lead to misleading conclusions.\n\n4.2.5.1 What causation actually means\nCausation reflects an if–then relationship:\n\nIf X happens, then Y becomes more likely.\n\nImportantly, this is probabilistic, not absolute.\nFor example:\n\nIf athletes use steroids, then their muscle mass increases the chances of improving.\nThis does not mean every athlete will necessarily experience the same effect.\n\nWe express causal claims using language like “increases the likelihood of”, precisely because real-world data always contain variability.\n\n\n4.2.5.2 Why strong correlations can mislead us\nIt is possible for two variables to move together simply because a third factor influences both.\nA classic example: Ice-cream sales and drownings both rise over summer. When graphed together, the relationship looks compelling—high sales coincide with more drownings.\n\n\n\nHowever:\n\nEating ice-cream does not cause drowning.\nBoth trends are caused by a lurking variable: temperature.\nWarm weather increases both swimming activity and ice-cream consumption.\n\n\n\n\nThe key lesson is simple:\n\nJust because two things move together does not mean one causes the other.\n\nThis principle is essential before we introduce regression modelling.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#simple-linear-regression",
    "href": "04-Models-of-relationships-between-data.html#simple-linear-regression",
    "title": "4  Models of relationships between data",
    "section": "4.3 Simple Linear Regression",
    "text": "4.3 Simple Linear Regression\nCorrelation summarises a relationship numerically, but regression provides a model of that relationship.\nSimple linear regression is the most widely used statistical modelling framework for understanding how one variable predicts another.\n\n4.3.1 The basic regression model\nSimple linear regression models a dependent variable \\(Y\\) as a straight-line function of one predictor \\(X\\):\n\\[\n\\hat{Y} = b_0 + b_1 X\n\\]\nWhere:\n\n\\(b_0\\) = intercept (predicted value of \\(Y\\) when \\(X = 0\\))\n\\(b_1\\) = slope, describing how much \\(Y\\) changes for a one-unit change in \\(X\\)\n\n\n\n\nThe regression line is chosen to come as close as possible to all the points in a scatterplot. In this sense, it is the “best fitting” straight-line model for the data.\n\n\n4.3.2 A worked example\nSuppose we had some data relating to number of close friends and measured depression for 10 people.\n\n\nN_FriendsDepression8219541035584635564101\n\n\nWe can visualise this data by pairing each observation:\n\n\n\n\n\n\n\n\n\nUsing linear regression, we can construct a model that fits this data. This is a time-consuming process so it’d standard practice to use software to perform this task for us. However, I have also provided the mathematical formulas below so that you are familiar with the steps as well.\n\n\n\n\n\n\nSimple linear regression\n\n\n\n\n\n\nFrom the model above:\n\nThe slope \\(b_1 = -0.71\\):\nFor each additional friend, depression score decreases on average by 0.71 points.\n\nThe negative slope implies an inverse relationship: more friends → lower depression.\n\nHowever, even with a clear model, we must remember:\nThis is still correlational data, not causal evidence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#extending-to-multiple-regression",
    "href": "04-Models-of-relationships-between-data.html#extending-to-multiple-regression",
    "title": "4  Models of relationships between data",
    "section": "4.4 Extending to Multiple Regression",
    "text": "4.4 Extending to Multiple Regression\nSimple regression uses one predictor. But most real-world outcomes are influenced by many variables.\nMultiple regression extends the simple model by including more predictors: :contentReferenceoaicite:7\n\\[\n\\hat{Y} = b_0 + b_1 X_1 + b_2 X_2 + \\cdots + b_k X_k\n\\]\nThis allows us to:\n\ncontrol for additional variables,\nisolate the effect of each predictor,\nimprove prediction accuracy,\nand reduce the risk of misleading correlations caused by omitted variables.\n\n\n4.4.1 Example: Predicting income\nSuppose we have some data on worker’s: (1) years of education and (2) age, and we want to build a regression model for income:\n\\[\n\\hat{\\text{Income}} = b_0 + b_1 \\text{Education} + b_2 \\text{Age}\n\\]\nAnd suppose we have used software to determine our \\(b\\) coefficients as:\n\\[\n\\hat{\\text{Income}} = -117{,}110 + 4540(\\text{Education}) + 3368(\\text{Age})\n\\]\nIn the model above, the \\(b\\) coefficient for education is\n\\[b_1=4540\\]\nThis means that for each additional year in education, income increases by $4540, when we hold all other variables constant.\nThis is like saying, if you take two people of the same age (i.e. holding age constant), but have one unit of difference in education, then the predicted difference in income would:\n\n1 unit difference2 unit difference\n\n\nThe difference in the predictions for the two people ($76,21 - $72,081 = $4,450) is equal to the b coefficient for education:\n\n\n\n\n\nThe difference in the predictions for the two people ($81,161 - $72,081 = $9,080) is equal to the 2 x b coefficient for education:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#excel",
    "href": "04-Models-of-relationships-between-data.html#excel",
    "title": "4  Models of relationships between data",
    "section": "4.5 Excel",
    "text": "4.5 Excel\nIn this section we will learn how to use Microsoft Excel to solve some of the concepts covered in this chapter. Begin by downloading the data file below and then follow the instructions by navigating through the screenshots.\n\n\n\n Amount Spent\n\n\n\n\nScatter plots are one of the most useful tools for exploring relationships between two numerical variables. Each point on the plot represents an individual observation, with its position determined by the values of the two variables being compared. By looking at the overall pattern of points, we can quickly identify whether the relationship appears positive, negative, or unrelated, and whether it is roughly linear or more curved. Scatter plots also help reveal clusters, unusual observations, or potential outliers that may influence further analysis. Because they provide an immediate visual sense of how two variables move together, scatter plots are the natural starting point before calculating numerical measures such as covariance, correlation, or fitting a regression model.\n\n4.5.1 Scatter plots\n\n12345\n\n\nSuppose we wanted to generate a scatter plot to visualise the relationship between Time and Amount spent. Begin by highlighting these two columns.\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Insert tab, select Insert Scatter or Bubble, then choose Scatter.\n\n\n\n\n\n\n\n\n\n\n\nAfter the chart has been generated, click on the + icon next to the chart and select Axis titles. You should enter in the variable names as your axis titles.\n\n\n\n\n\n\n\n\n\n\n\nIt’s also useful to add a trendline.\n\n\n\n\n\n\n\n\n\n\n\nPractice creating the following scatter plots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Correlation (Excel formulas)\nExcel’s CORREL function is a quick way to obtain the correlation coefficient between two variables. In the screenshot below, the function is used to calculate the correlation between:\n\nTime and Amount Spent\nHeight and Amount Spent\nExercise and Amount Spent\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Correlation (Data ToolPack)\nYou can also use Excel’s Data Analysis ToolPack to determine the correlation coefficient between variables. This is particularly useful if you would like to compute several correlations at the same time.\n\n123\n\n\nOptional: I like to put the main variable in the first column (here I’ve moved Amount spent to Column A). This doesn’t change the results, it just places the variables in different positions.\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Data tab, select Data Analysis then choose Correlation. From there, select the data (here we’re choosing all 4 variables).\n\n\n\n\n\n\n\n\n\n\n\nThis will produce a correlation matrix that tells us what the correlation coefficient is for each pair. For example, the correlation between height and Amount spent is 0.049; the correlation between Time and Amount spent is 0.783, etc.\nNote: Correlation matrixes are useful when you want to display 2 or more correlation coefficients. if you’re only displaying 1 correlation coefficient, it’s not efficient (and not recommended) to use a matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4 Simple Linear Regression\nYou can use Excel’s Data Analysis ToolPack to quickly run regression models for your data. Suppose we would like to create a model for Amount Spent using only Time as a predictor:\n\n12\n\n\nSwitch to the Data tab, select Data Analysis and choose Regression. From there, specify your input Y (here: Amount Spent, so cells D1 to D42) and input X (here: Time, cells B1 to B42) ranges. Make sure “Labels” is selected, and specify your output range.\n\n\n\n\n\n\n\n\n\n\n\nExcel will generate several tables (more on these next week). For now, we’re just interested in last table, which will allow us to determine the regression model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5 Multiple Linear Regression\n\n12\n\n\nWe will use the same steps as Simple Linear Regression, excep when it comes to specifying the Input X range, we’re going to select columns A to C (which indicates 3 different variables)\n\n\n\n\n\n\n\n\n\n\n\nThe same output is generated, except the coefficients table will have more rows (for each predictor). Similar to simple linear regression, we can use this table to determine the regression equation/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#summary",
    "href": "04-Models-of-relationships-between-data.html#summary",
    "title": "4  Models of relationships between data",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nIn this chapter, we moved from describing single variables to modelling relationships between variables. We began by introducing covariance, which captures the direction in which two variables move together, and then developed this into correlation, a standardised measure that allows us to assess both the direction and strength of a linear relationship. Importantly, we emphasised that correlation alone does not establish causation—variables may move together because of a third unseen factor, or purely by coincidence.\nBuilding on this foundation, we introduced simple linear regression as a formal modelling framework. Regression enables us to estimate a line of best fit, quantify the change in an outcome for a change in a predictor, and make predictions based on observed patterns in the data. Throughout, we discussed how to interpret slopes, intercepts, fitted values, and residuals, and highlighted that regression still describes associations, not causal effects.\nFinally, we introduced the idea of multiple regression—a powerful extension that allows us to model outcomes using more than one predictor at a time. At this stage, we focused only on intuition and interpretation: understanding how each variable’s effect is estimated while holding the others constant and why this leads to better modelling and prediction. In the next two weeks, we will examine multiple regression in much greater detail, including model building, diagnostics, interactions, categorical predictors, and the assumptions required for reliable inference.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "04-Models-of-relationships-between-data.html#exercises",
    "href": "04-Models-of-relationships-between-data.html#exercises",
    "title": "4  Models of relationships between data",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nThe simple regression model is given as follows:\n\\[Y_i=\\beta_0+\\beta_1X_i+e_i\\]\n“The estimates of the intercept and slope (\\(\\beta_0\\) and \\(\\beta_1\\)) are obtained by minimising the sum of squared errors.” Give a simple intuitive explanation for what this statement means. It might help to draw a scatter diagram to make your explanation clear.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nIntuitively, the best fitting model to the data will be the model with the least amount of error. The error is the difference between the actual value of Y and the model’s predictions of Y. The errors are then squared so that large negative errors don’t cancel out large positive errors, giving the false impression of a well-fitted model. The total error in the model is measured using the sum of these squared errors for each observation. The sum of squared errors is then minimised in order to obtain values for the intercept and slope that provide the best fitting model.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nHow is the correlation calculated from the covariance and how do they differ?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nCorrelation = \\(\\frac{\\text{covariance}}{sd(X)sd(Y)}\\). Covariance can take any value (units depend on units of X and Y), but correlation is unitless, standardised to be between -1 and +1. So we can judge the strength of the correlation by how far from zero it is.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhen you use Excel to calculate the correlation of two variables, and one of them is constant for all values in the sample, the answer comes back as #DIV/0!. What does this value mean and based on how a correlation is calculated, explain intuitively why Excel was unable to calculate a correlation.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nIf a variable is constant for all values, the SD is zero, so correlation tries to divide by zero. This is impossible, hence the error from Excel. This makes sense – correlation measures how much variation in one variable is related to variation in the other. If a variable is constant in the sample, we learn nothing about this variation, so cannot calculate a correlation.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nThis question is based around annual CO2 emissions for countries around the world over many years. Of particular interest is how CO2 per capita (million of tonnes) and GDP per capita ($) are related to each other. To investigate this we construct a scatter plot between these two variables for data from 2016. This is show below.\n\n\n\n\nWhat does the scatter plot tell us about the relationship between GDP per capita and CO2 emissions per capita?\nTo understand the relationship between these two variables we estimated the following simple linear regression using data for 2016: \\(CO_2 \\text{ per capita} =0.23607+0.00025 \\times GDP \\text{ per capita}\\). Interpret the sign and magnitude of the coefficient on GDP per capita.\nThe correlation between GDP per capita and CO2 per capita is 0.7966. What is the range of possible values of the correlation coefficient, and what does this value of 0.7966 tell us?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nThere is a pattern, seems to be a relationship.\n\nIt is positive, higher CO2 usually means higher GDP per capita.\nIt is roughly linear, so a linear model would work ok in modelling it.\n\nSign is positive, which means countries with higher GDP per capita have higher CO2 emissions per person. An extra $1 of GDP per capita increase CO2 emissions per capita by 0.00025 million tonnes. A better interpretation might be: Increase of GDP per capita of $1000 means increase in CO2 emissions of .25 million tonnes.\nCorrelation ranges from -1 to +1.\n\n0.7966 means positive correlation, and a quite strong one as it is reasonably close to 1.\nNot “very strong” though.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nContinuing from question 4, let us focus on a sample of data from 1990. We estimate a model with the following:\n\nY variable: \\(CO_2\\) emissions (millions of tonnes)\nX variables: (1) Population (millions) and (2) GDP per capita\n\nThe results of the model across countries for 1990 is shown below:\n\n\n\n\nReport and interpret the coefficient on Population.\nReport and interpret the coefficient on GDP per capita.\nIt has been claimed that the increase in CO2 emissions can be stopped if we stopped population growth. Does your estimated model support that claim? Explain.\nThe intercept for the model is negative. Discuss what this means.\nConsider a country with a population of 25 million and GDP per capita of 30,000. Write the formula to give the predicted level of CO2 emissions.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nComparing two countries with the same GDP per capita, but one has an extra million people, the model predicts the country with the higher population will have 4.116 million tonnes of CO2 emissions per year more than the other.\nComparing two countries with the same population, but one has an extra $1 of GDP per capita, the model predicts the country with the higher GDP per capita will have 20, 301 tonnes of CO2 emissions per year more than the other.\nNo it doesn’t. Even if population is held constant, the model estimates show that increases in GDP per capita also will lead to increased emissions.\nIt is estimated that countries with no GDP and no people have negative emissions of 181 million tonnes pa. Clearly not realistic since no countries are like that.\nPredicted CO2 = -180.7 + 4.116 * 25 + .020*30,000\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nLet us now estimate the same regression model as previously (question 5) but using data for 2016. The results are show below.\n\n\n\n\nThis model, estimated on 2016 data, has a smaller coefficient on GDP per capita than the model estimated on 1990 data. What does this mean?\nTwo regressions have been estimated using the 2016 data for 161 countries. They have the same independent variables (Population, measured in millions of people, and GDP, measured in billions of dollars), but different dependent variables. The estimated equations are:\n\\[CO_2=-15.6 + 0.838Population + 0.281GDP\\] \\[CO_2(per capita)=4.73 -0.016Population+0.0012GDP\\]\nCompare the estimates across the two equations, commenting on two things: (1) explain why it could make sense for the coefficient on Population to be negative in the second equation, and (2) explain why the coefficient of GDP is so much smaller in the second equation compared to the first.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nIt means an increase in GDP per capita in 2016 have a smaller effect on emissions than if the same increase took place in 1990. This suggests the world may be using better technologies, as more economic activity has a smaller effect on CO2 emissions than in 1990.\n\nThe second model accounts for the effect of increase in population by modelling CO2 per capita. Negative effect of population in that model tells us that countries with bigger populations have lower emissions per person, presumably they get ‘economies of scale’ in their economic activity and hence are more efficient, producing less emissions per person.\nThe second model predicts CO2 per capita, so the estimated effect of a $1 billion increase in GDP is .0012 per person. Model 1 measures the effect on total CO2 emissions. To be comparable, we have to take the second estimate and multiply by population to get total effect on emissions. So the coefficient in Model 2 ought to be much smaller.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nDownload the file below and open it in Excel.\n\n\n Test Scores\n\n\n\nProduce a scatter plot of the two variables – Monthly Family Income per person (US$) and Test Scores. Is the relationship positive or negative?\nEstimate a Simple Linear Regression Model with Monthly Family Income per person (US$) as the X variable, and Test Scores as the Y variable. Is the estimated slope coefficient consistent with your conclusion for part (a)?\nEstimate a Multiple Linear Regression Model with age and Monthly Family Income per person as the X variables, and Test Scores as the Y variable. Interpret the \\(\\beta\\) coefficients for each variable.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nThe two series appear to be slightly positively related (almost zero), in the sense that the Test Score tends to increase as the value of Monthly Family Income increases. In other words, the value of Y can be inferred from the knowledge of X\nThe slope coefficient on Monthly Family Income is 0.014 which is slightly positive. This supports our previous evaluation of the scatter plot that there is a slight positive relationship between Monthly Family Income and Test Scores.\n\nEvery additional year of age decreases test score by 0.1619, when controlling for monthly family income.\nEvery additional dollar increase in monthly family income increases test score by 0.0133, when controlling for age.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models of relationships between data</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html",
    "href": "05-Multiple-regression-and-hypothesis-testing.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "5.1 Introduction\nLast time we learnt how to build models to estimate things about a population. Specifically, we explored regression models, which allow us to quantify relationships between variables and generate predictions based on observed data. With tools such as covariance, correlation, and simple linear regression, we can now construct models that summarise how an outcome changes as predictors change.\nBut building a model is only the first step. Once we have a regression line or an estimated effect, a natural question follows:\nIn other words, whilst we now know how to construct these models, the deeper question becomes: can we make reliable conclusions from them?\nThis is where hypothesis testing, and the focus for this week, comes in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#introduction",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#introduction",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "is this relationship real, or could it simply be due to random chance in our sample?\n\n\n\n\n\n\n\n\n\nClass Experiment\n\n\n\nBefore diving into the formal ideas, we will explore one concrete example throughout this chapter. It’s a simple question — one you can respond to in class:\n\nIs there a relationship between how much time you spend on your phone and your performance on a colour-perception test?\n\nThe Data\nFor each student, we record:\n\nPhone usage: average minutes of screen time last week (most modern phones can check this quite quickly)\nKOLOR test score: We will do this test in the lecture (link here)\n\nThe intuition\nImagine the regression line for students in this class shows a negative slope: i.e. more screen time is associated with worse KOLOR performance.\n\n\n\n\n\n\n\n\n\n\n\nAt this moment, we can already ask our central questions:\n\nCould this slope just be random noise?  Or is the relationship consistent enough in our data that we would expect it to appear again if we repeated the study?\n\nBy the end of this Chapter, you’ll be able to answer the bigger question: How do we decide if an observed effect is “real”?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#introduction-to-hypotheses",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#introduction-to-hypotheses",
    "title": "5  Hypothesis Testing",
    "section": "5.2 Introduction to hypotheses",
    "text": "5.2 Introduction to hypotheses\nBefore we can formally test whether a relationship is “real” or just random noise, we first need a structured way to express the competing ideas we want to evaluate. This structure is the foundation of all statistical testing and begins with two statements:\n\na null hypothesis\nan alternative hypothesis\n\nThese are not random guesses, nor are they predictions. They are carefully defined statements about how the world might work. Every statistical test compares these two possibilities.\nA hypothesis is simply a claim about a population. Not the sample we collected (see Chapter 3), but the entire population we care about. For example, based on our KOLOR data set, we might believe:\nStudents who spend more time on their phones tend to score lower on the KOLOR test.\nBut that belief is just a conjecture. A hypothesis test gives us a systematic way to evaluate that conjecture using data. To do this, we first define our two hypotheses.\n\nThe Null HypothesisThe Alternative Hypothesis\n\n\nThe null hypothesis, written as \\(H_0\\), represents:\n\nThe assumption of “no effect,” “no difference,” or “nothing interesting happening.”\n\nIt is the default position—the claim we assume is true unless the evidence strongly contradicts it. In the context of our KOLOR example, we might say:\n\n\\(H_0\\): There is no relationship between phone usage and KOLOR score in the population.\n\nTranslated into regression language:\n\n\\(H_0\\): The true slope (β₁) = 0\n\nIf \\(H_0\\) is true, any trend we see in our sample is just random variation, not a meaningful pattern. The null is not meant to be exciting. It is designed to be challenged, not believed.\n\n\nThe alternative hypothesis, written as \\(H_1\\), represents:\n\nThe effect we believe might be present — the idea we want evidence for.\n\nIt is the scientific claim we are trying to support with data. For our example:\n\n\\(H_1\\): There is a relationship between phone usage and KOLOR score.\n\nTranslated into regression language:\n\n\\(H_1\\): The true slope (β₁) ≠ 0\n\nThis is called a two-sided alternative, because it allows the slope to be:\n\npositive (more phone time → higher KOLOR score), or\nnegative (more phone time → lower KOLOR score)\n\nIf we had a strong prior belief that more screen time harms perceptual accuracy, we could specify a one-sided alternative:\n\n\\(H_1\\): β₁ &lt; 0\n\n(more on this later)\n\n\n\nIt may feel strange to set up two competing statements when we really only “believe” one of them. But this is crucial for two reasons:\n\nWe must consider the possibility that our observed effect is just noise. Without a null hypothesis, everything would seem statistically significant.\nThe statistical machinery needs a baseline. To compute p-values, test statistics, and sampling distributions, we must make explicit what “no effect” looks like.\n\nThe entire logic of hypothesis testing is built on evaluating how inconsistent our sample data are with the null hypothesis.\nIf we look at the plot from above (Relationship between phone usage and KOLOR performance), we observe a slight negative trend - higher phone usage is associated with lower KOLOR performance.\nBut before we conclude anything, we need to ask:\n\nIf the true effect in the population were zero (β₁ = 0), how likely is it that we would see a slope as strong, or stronger, than the slope we found?\nIf this likelihood is very small, do we have enough evidence to reject \\(H_0\\) and conclude that an effect probably exists?\n\nThese question, translated into test statistics, sampling distributions, and p-values, form the backbone of formal hypothesis testing, and is what we will explore in the subsequent sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#test-statistic-decisions",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#test-statistic-decisions",
    "title": "5  Hypothesis Testing",
    "section": "5.3 Test Statistic & Decisions",
    "text": "5.3 Test Statistic & Decisions\nOnce our hypotheses are set up, we need a way to judge whether our sample provides strong enough evidence against the null hypothesis. This is where the test statistic comes in.\n\nA test statistic is a numerical summary that measures how far our sample result is from what we would expect if the null hypothesis were true.\n\nWe then use this number to make a statistical decision. In simple linear regression, when testing whether a slope is zero, the test statistic we use is the t-statistic.\n\n5.3.1 The t-statistic\nFor our slope estimate, \\(\\hat{\\beta}_1\\), is given by:\n\\[\nt = \\frac{\\beta_1}{SE(\\beta_1)}\n\\]\nThis measures how many standard errors the estimated slope is away from zero (the value stated by the null hypothesis).\n\nA t-statistic near 0 → sample slope is close to zero → consistent with \\(H_0\\)\nA large |t-statistic| → sample slope far from zero → evidence against \\(H_0\\)\n\n\n\n5.3.2 Degrees of Freedom\nWhen we compute the t-statistic for the slope in a simple linear regression model, we compare it to a t-distribution rather than a normal distribution (See Chapter 2). This is because, in real data, we do not know the true standard deviation of the errors, therefore we estimate it from the sample.\nEstimating parameters introduces uncertainty. The t-distribution accounts for this extra uncertainty through a quantity called degrees of freedom (\\(df\\)).\nIn simple linear regression, we have two parameters:\n\nthe intercept \\(\\beta_0\\)\nthe slope \\(\\beta_1\\)\n\nOnce these two numbers are estimated, the residuals (the “errors”) must fit around that line. This leaves us with:\n\\[df = n-2\\]\nIn our KOLOR example, suppose we had 100 students take the KOLOR test. This mean mean that\n\\[df = 100-2=98\\] So our t-statistic is compared against a t-distribution with 98 degrees of freedom.\nThis is important because the \\(df\\) determines the shape of the t-distribution. When \\(df\\) is small, the distribution has heavier tails. When \\(df\\) is large, it approaches the normal distribution.\n\n\n\n\n\n\n\n\n\nWith 98 \\(df\\), our distribution is already very close to normal, but still slightly wider to account for residual uncertainty.\n\n\n5.3.3 Critical Regions\nOnce we understand the shape of the t-distribution and how degrees of freedom affect it, the next question becomes:\n\nHow do we decide whether our observed test statistic is “large enough” to reject the null hypothesis?\n\nThe answer comes from defining critical regions: parts of the t-distribution that represent values so extreme that they are unlikely to occur if the null hypothesis were true.\nA critical region (\\(t_\\text{crit}\\)) is the set of t-values that would lead us to reject the null hypothesis.\nIt depends on:\n\nthe significance level (\\(\\alpha\\))\nwhether the test is one-sided or two-sided,\nand the degrees of freedom (\\(df\\)).\n\nThe significance level is a threshold that determines how much evidence we require before we are willing to reject the null hypothesis. Because we want to limit the chance of falsely declaring a relationship that does not exist, α is usually chosen to be small. Common choices are:\n\n\\(\\alpha = 0.01\\) (more conservative)\n\\(\\alpha = 0.05\\) (most common)\n\\(\\alpha = 0.10\\) (more lenient)\n\nChoosing \\(\\alpha = 0.05\\) within the context of this example would mean:\n\n“I am willing to tolerate a 5% chance of incorrectly concluding that phone usage affects KOLOR test performance, when in reality the relationship is zero.”\n\nIn other words, α controls how strong the evidence must be before we reject \\(H_0\\).\nFor a typical two-sided test with \\(\\alpha=0.05\\), \\(\\frac{\\alpha}{2}=0.025\\) of the probability lies in the left tail, and 0.025 lies in the right. These two tails form the rejection region. If our observed t-statistic falls in either shaded tail, the data are considered too inconsistent with \\(H_0\\) to be explained by chance.\n\n\n\n\n\n\n\n\n\n\n\nTraditionally, to determine the values for the boundary for the critical region we use the t-tables. These are similar to the Z-tables we saw in Chapter 2. Below is a two-tailed t-table that shows the boundary values (\\(\\pm t_\\text{crit}\\)) for different \\(df\\).\n\n\n\n\n\n\n\n\ndf\n0.20\n0.10\n0.05\n0.02\n0.01\n0.001\n\n\n\n\n1\n3.078\n6.314\n12.710\n31.820\n63.660\n636.620\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n6.869\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n4.587\n\n\n15\n1.341\n1.753\n2.131\n2.602\n2.947\n4.073\n\n\n20\n1.325\n1.725\n2.086\n2.528\n2.845\n3.850\n\n\n25\n1.316\n1.708\n2.060\n2.485\n2.787\n3.725\n\n\n30\n1.310\n1.697\n2.042\n2.457\n2.750\n3.646\n\n\n40\n1.303\n1.684\n2.021\n2.423\n2.704\n3.551\n\n\n60\n1.296\n1.671\n2.000\n2.390\n2.660\n3.460\n\n\n80\n1.292\n1.664\n1.990\n2.374\n2.639\n3.416\n\n\n100\n1.290\n1.660\n1.984\n2.364\n2.626\n3.390\n\n\n1000\n1.282\n1.646\n1.962\n2.330\n2.581\n3.300\n\n\nz\n1.282\n1.645\n1.960\n2.326\n2.576\n3.291\n\n\n\n\n\n\n\nRecall that we have \\(df =98\\), so we will use the nearest row in the table (\\(df=100\\)) to obtain the boundary values. Here (\\(\\pm t_\\text{crit}=1.984\\)) for \\(\\alpha=0.05\\). This means that for our current example, the critical t values are:\n\n\n\n\n\n\n\n\n\n\n\nNow, we just need to compute the t-statistic and see whether or not it lays within the critical region for our t-distribution. Recall from the section above that the t-statistic is given by:\n\\[\nt = \\frac{\\beta_1}{SE(\\beta_1)}\n\\]\nMathematically, the values of \\(\\hat{\\beta}_1\\) and \\(SE(b_1)\\) are given by:\n\\[\n\\beta_1\n= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}\n       {\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\]\n\\[\nSE(b_1) =\n\\sqrt{\n\\frac{\nRSS\n}{\n(n-2)\\left(\\sum x^2 - \\frac{(\\sum x)^2}{n}\\right)\n}\n}\n\\]\nWhere the residual sum of squares (RSS) can be computed with:\n\\[\nRSS =\n\\sum y^2 - b_0\\sum y - b_1\\sum xy\n\\]\nAnd the intercept \\(b_0\\) is:\n\\[\nb_0 = \\bar{y} - b_1\\bar{x}\n\\]\nRecall from last week, we learnt how to use software (Excels Data Analysis Tool Pack) to quickly obtain the regression output for a given data set. Suppose the coefficients table for this example looked like:\n\n\n.CoefficientsSEt_statP_valueIntercept73.35122.862925.620.00001phone usage-0.02140.0137-1.560.12200\n\n\nFrom this output, we can see that \\(\\beta_1=-0.0214\\) and \\(SE(\\beta_1)=0.0137\\). Therefore:\n\\[\nt = \\frac{\\beta_1}{SE(\\beta_1)}=\\frac{-0.0214}{0.0137}=-1.56\n\\]\nNote: Our software usually provides the value of the t-statistic with the coefficient estimates (check that the t-stat column matches this calculation).\nWith the calculated t-statistic, we can now see if it lies within the critical region:\n\n\n\n\n\n\n\n\n\n\n\nThe observed t-statistic of −1.56 does not exceed the critical cut-offs, meaning it is not sufficiently extreme to indicate a real departure from the null hypothesis. In practical terms, the sample does not provide strong enough evidence of an association between phone usage and KOLOR score. In statistics this can also be refereed to as failing to reject the null hypothesis.\n\n\n\n\n\n\nCommon Misconception: Failing to Reject \\(H_0\\) Means the Null Is True\n\n\n\nIt is extremely important to understand what our conclusion does not mean.\nWhen we fail to reject \\(H_0\\), this does not imply:\n\nthat the null hypothesis is true\n\nthat there is no relationship in the population\n\nthat the alternative hypothesis is false\n\nA non-significant result simply means:\n\nOur sample did not provide strong enough evidence to rule out the null hypothesis.\n\nThere are many reasons this can occur:\n\nthe true effect might be small\n\nthe sample size might be too small to detect it\n\nthe data may be noisy\n\nthe observed effect may genuinely be zero\n\nStatistical tests can only evaluate evidence, not truth.\nThe correct interpretation is therefore:\n\n“We do not have sufficient evidence to conclude that a relationship exists,”\nnot\n“there is no relationship.”\n\nThis distinction is central to proper hypothesis testing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#one-sided-vs-two-sided-tests",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#one-sided-vs-two-sided-tests",
    "title": "5  Hypothesis Testing",
    "section": "5.4 One-Sided vs Two-Sided Tests",
    "text": "5.4 One-Sided vs Two-Sided Tests\nIn the example above, we focused on a two-sided test, where evidence in either direction (a positive or negative slope) could lead us to reject the null hypothesis. However, not all research questions are symmetrical like this. Sometimes we are only interested in detecting an effect in one direction—for example, testing whether increased phone usage reduces KOLOR performance, but not whether it improves it. In such situations we use a one-sided test, where the entire significance level α is placed in a single tail of the distribution. One-sided tests are more powerful in the predicted direction, but they must be justified in advance and used carefully, since they will not detect effects in the opposite direction.\n\nTwo-sided TestsOne-sided Tests\n\n\nIn our current scenario, we are interested in whether phone usage predicts KOLOR performance. At this stage, we are not assuming in advance that:\n\nmore phone usage reduces KOLOR score, or\nmore phone usage improves KOLOR score.\n\nWe simply want to know whether any relationship exists.\nA two-sided test evaluates both possibilities. The hypotheses are:\n\n\\(H_0:\\beta_1=0\\) (no relationship between phone usage and KOLOR score)\n\\(H_1:\\beta_1\\ne0\\) (a relationship between phone usage and KOLOR score exists)\n\nBecause both directions (positive or negative slope) count as evidence against the null, the critical region is split between the two tails of the t-distribution.\n\n\nA one-sided test is used when the research question predicts a specific direction of effect before looking at the data.\nIn our context, this would require believing—in advance—that:\n\nMore phone usage will definitely reduce KOLOR performance (but not improve it).\n\nIn that case, we might set up a lower-tailed test:\n\n\\(H_0:\\beta_1≥0\\)\n\\(H_1:\\beta_1&lt;0\\)\n\nHere, only negative slopes count as evidence against the null. The entire 5% significance level is placed in the left tail of the t-distribution.\n\n\n\nUsing the critical values for this scenario (with \\(\\alpha=0.05\\) and \\(df=98\\)), the t-distributions can be visualised as:\n\n\n\n\n\n\n\n\n\n\n5.4.1 Choosing between two-sided or one-sided test\nA simple guide:\n\nUse a two-sided test when:\n\nyou do not have a justified directional hypothesis\nthe consequences of missing an effect in the opposite direction are important\n\nUse a one-sided test when:\n\ntheory or prior evidence strongly predicts the direction\nthe opposite direction is either impossible or irrelevant\nyou specify the direction before collecting or analysing data\n\n\nIn practice, most introductory statistical analyses rely on two-sided tests unless there is a clear and defensible reason to choose otherwise.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#the-p-value",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#the-p-value",
    "title": "5  Hypothesis Testing",
    "section": "5.5 The p-value",
    "text": "5.5 The p-value\nSo far we have introduced the idea of critical regions, where we compare the observed test statistic against fixed cut-offs such as ±1.984. This approach works well, but it leads to an obvious question:\n\nHow “unlikely” is our observed test statistic if the null hypothesis were really true?\n\nThe p-value answers exactly this question.\nFormally, the p-value is: The probability of obtaining a test statistic as extreme as (or more extreme than) the one we observed, assuming the null hypothesis is true.\nIn other words:\n\nPretend \\(H_0\\) is true (the slope is really 0).\nLook at the t-distribution that describes what values of the test statistic we would expect just from random sampling.\nAsk: How far out in the tails does our observed t-value sit?\n\nThe more extreme the observed t-value, the smaller the p-value.\nGiven this definition of a p-value, let’s interpret it in the context of our study examining whether phone usage predicts KOLOR scores.\n\nPretend \\(H_0\\) is true (that phone usage has no effect on KOLOR score).\nLook at the t-distribution (above) for this scenario\nA t-value of –1.56 sits comfortably inside the main body of the distribution. It is not close to the extreme tail areas beyond ±1.984 that would count as “surprising” if the true slope were zero.\n\nIn practical terms:\n\nA t-value of this size is quite plausible even if phone usage and KOLOR score are unrelated.\nIt does not represent strong evidence that the slope differs from zero.\n\nBecause the observed t-value is not extreme, the p-value will be relatively large — greater than 0.05. A large p-value means the data are compatible with there being no real relationship between phone usage and KOLOR test performance in the population. We can inspect our Coefficients table from earlier to see what the p-value is.\n\n\n.CoefficientsSEt_statP_valueIntercept73.35122.862925.620.00001phone usage-0.02140.0137-1.560.12200\n\n\nIf we read across the phone usage row, we can see that it’s corresponding p-value is 0.1220. We can interpret this as:\n\nIf phone usage truly had no effect on KOLOR performance (i.e., if the real slope were 0), then a t-statistic as extreme as –1.56 would occur about 12.2% of the time purely by chance.\n\nThe shaded red area in the figure below reflects this p-value of 0.1220 (~ 0.061 within each tail). Twelve percent is not particularly rare. It is well above our usual threshold of 5% for deciding whether a result is surprising enough to reject the null hypothesis.\nThis tells us that the negative slope we observed in our sample, suggesting that students who used their phones more tended to perform slightly worse on the KOLOR test, is not strong enough evidence to conclude that such a relationship exists in the population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNothing Special About 0.05\n\n\n\nIt is easy to assume that α = 0.05 is some universal rule of science. In reality, there is nothing magical about the number 0.05. The value 0.05 became popular largely through historical convention, not because it represents a scientifically optimal threshold. Different fields, and even different studies within the same field, may sensibly choose different significance levels.\nExamples:\n\nMedical trials often use α = 0.01 to reduce the risk of false positives.\n\nEarly exploratory research may use α = 0.10 to avoid missing potential effects.\n\nLarge datasets may justify even more stringent thresholds (e.g., α = 0.001).\n\nThe choice of α should reflect the context, the consequences of errors, and the goals of the analysis, not a rigid rule.\nThe key idea is this:\n\nStatistical significance depends on the chosen α.\nIt is a decision threshold, not a law of nature.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#significant-results",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#significant-results",
    "title": "5  Hypothesis Testing",
    "section": "5.6 Significant results",
    "text": "5.6 Significant results\nIn the previous section we saw what it means to explore data where we cannot reject \\(H_0\\) (i.e. our results are not statistically significant). Let us now change the data slightly. Suppose we did the KOLOR test for a different set of 100 students from ETC1000 (e.g. from a different tutorial). The panel on the left is from the original class, and the panel on the right is from the new class:\n\n\n\n\n\n\n\n\n\n\n\nThe slopes from both plots look quite similar, albeit the new plot appears to have a slightly steeper slope. Let’s now have a look at the regression table to see if this changes our interpretation:\n\n\n.CoefficientsSEt_statP_valueIntercept69.9673.32921.0160.00001phone usage-0.0350.153-2.2740.02510\n\n\nNotice here that the t-statistic is -2.274. If we were to visualise this on the same t-distribution from earlier (with the same critical regions of \\(\\pm1.984\\)), then this would show:\n\n\n\n\n\n\n\n\n\nFrom the plot above, we can see that the t-statistic of \\(t=-2.274\\) lays within the critical region. Therefore, we reject the null hypothesis and conclude that there is statistically significant evidence of a relationship between phone usage and KOLOR score.\nLikewise, we could have also used the p-value from our regression output as well:\n\n\n.CoefficientsSEt_statP_valueIntercept69.9673.32921.0160.00001phone usage-0.0350.153-2.2740.02510\n\n\nHere, the p-value is 0.0251, which is less than the significance level of 0.05. This provides us with the same conclusion as above (i.e. reject the null hypothesis).\n\nWe can reject \\(H_0\\) because the p-value 0.025 is less than the significance level \\(\\alpha\\) 0.05\n\nVisually, if the p-value is 0.0251, then within each tail, the probability is \\(\\frac{0.0251}{2}=0.0126\\), which we can see is beyond the critical region as well:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#confidence-intervals",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#confidence-intervals",
    "title": "5  Hypothesis Testing",
    "section": "5.7 Confidence Intervals",
    "text": "5.7 Confidence Intervals\nUp to this point, we have focused on hypothesis tests and p-values as methods for deciding whether an estimated effect is statistically significant. However, there is another powerful and intuitive approach:\n\nUsing confidence intervals to assess whether an effect is compatible with zero.\n\nA confidence interval (CI) provides a range of plausible values for the true parameter—in our case, the true regression slope describing the relationship between phone usage and KOLOR performance. For a two-tailed hypothesis test at the 5% significance level, the decision rule using confidence intervals is simple:\n\nIf the 95% confidence interval for the slope does not include zero, reject \\(H_0\\)\nIf the interval does include zero, fail to reject \\(H_0\\)\n\nThis works because the critical values used to construct a 95% CI (±1.984 for df = 98) are the same critical values used in the α = 0.05 hypothesis test (see 5.3.3). Thus, CIs and hypothesis tests are mathematically equivalent — they will always lead to the same conclusion when the confidence level and significance level match.\nWe saw how to compute the CIs for the sample mean back in Chapter 3 (see 3.7). The formula to construct CIs for regression slopes is very similar:\n\\[\\beta \\pm t_\\text{crit} \\times \\frac{\\sigma}{\\sqrt{n}}\\]\nwhere \\(\\frac{\\sigma}{\\sqrt{n}}=SE\\)\nThis is useful because regression output typically comes with the \\(SE\\) reported. See the examples below.\n\nNot Statistically SignificantStatistically Significant\n\n\nWe will use the output below to compute the CIs:\n\n\n.CoefficientsSEt_statP_valueIntercept73.35122.862925.620.00001phone usage-0.02140.0137-1.560.12200\n\n\n\\[\n\\begin{aligned}\nCI &= -0.0214 \\pm 1.984 \\times 0.0137 \\\\\n   &= -0.0214 \\pm 0.02717 \\\\\n   &= [-0.0486,\\; 0.0058]\n\\end{aligned}\n\\]\nAnd, if we plot these on a number line we can see that this 95% CI does capture 0:\n\n\n\n\n\n\n\n\n\nThis means that we fail to reject \\(H_0\\) and conclude that there is no significant relationship between phone usage and KOLOR performance.\n\n\nWe will use the output below to compute the CIs:\n\n\n.CoefficientsSEt_statP_valueIntercept69.9673.32921.0160.00001phone usage-0.0350.153-2.2740.02510\n\n\n\\[\n\\begin{aligned}\nCI &= -0.035 \\pm 1.984 \\times 0.0153 \\\\\n   &= -0.035 \\pm 0.0304 \\\\\n   &= [-0.0654,\\; -.0046]\n\\end{aligned}\n\\]\nAnd, if we plot these on a number line we can see that this 95% CI does not capture 0:\n\n\n\n\n\n\n\n\n\nThis means that we reject \\(H_0\\) and conclude that there is a significant relationship between phone usage and KOLOR performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#summary",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#summary",
    "title": "5  Hypothesis Testing",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nIn this chapter, we moved from building regression models to evaluating them. While previous chapters focused on estimating relationships from data, the central question here was:\n\nIs an observed relationship real, or could it simply be due to random chance?\n\nTo answer this, we introduced the framework of hypothesis testing and applied it to regression models.\nWe began by defining null and alternative hypotheses, where the null hypothesis represents the idea of no effect (e.g. a regression slope of zero), and the alternative represents the presence of a relationship. Hypothesis testing works by assuming the null hypothesis is true and then assessing how compatible our observed data are with that assumption.\nWe then introduced the t-statistic, which measures how far an estimated regression coefficient lies from zero in standard-error units. This statistic is compared against a t-distribution, whose shape depends on the degrees of freedom, to account for uncertainty introduced by estimating model parameters from data.\nUsing the t-distribution, we explored three equivalent decision-making approaches:\n\nCritical regions, where we compare the test statistic to fixed cut-off values (such as ±1.984 at the 5% level), and\np-values, which quantify how likely it would be to observe a result as extreme as ours if the null hypothesis were true.\n\nWe emphasised that a small p-value indicates strong evidence against the null hypothesis, while a large p-value means the data are consistent with no effect. Importantly, failing to reject the null hypothesis does not prove it is true—it simply means the sample does not provide strong enough evidence.\nWe also distinguished between two-sided tests, which look for effects in either direction, and one-sided tests, which focus on a specific direction and must be justified before analysing the data.\nFinally, we introduced confidence intervals as an alternative and often more informative way to assess results. Confidence intervals provide a range of plausible values for the true regression coefficient and lead to the same conclusions as hypothesis tests when the confidence level and significance level match. Unlike p-values alone, confidence intervals also convey the size and uncertainty of an effect.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "05-Multiple-regression-and-hypothesis-testing.html#exercises",
    "href": "05-Multiple-regression-and-hypothesis-testing.html#exercises",
    "title": "5  Hypothesis Testing",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nExplain the difference between a null hypothesis and an alternative hypothesis. Why do we assume the null is true until proven otherwise?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nNull hypothesis \\(H_0\\): A statement of no effect, no difference, or status quo.\nAlternative hypothesis \\(H_1\\): A statement representing the presence of an effect or difference.\n\nWe assume the null is true because it provides a baseline comparison, it avoids claiming an effect without sufficient evidence, and hypothesis testing is framed similarly to the justice system: assume no effect until evidence suggests otherwise.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nResearchers want to understand whether daily phone usage affects students’ exam performance. The university claims that phone usage has no effect on exam performance.\nYou believe that phone usage does affect performance (it could increase or decrease scores).\nWrite the hypotheses, where the university’s claim is the null and your belief is the alternative.\nIs this a one- or two-sided test?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\\(H_0: \\beta_1 = 0\\)\n(Phone usage has no effect on exam performance)\n\\(H_1: \\beta_1 \\ne 0\\)\n(Phone usage does affect performance)\n\nThis is a two-sided test, because we are testing for any effect (positive or negative).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nA regression model gives the estimate for “hours on TikTok” on exam score:\n\n\\(\\beta = -1.8\\)\np-value = 0.030\n\nInterpret these estimates. Use \\(\\alpha=0.05\\).\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nEvery additional hour spent on TikTok is associated with a 1.8-point decrease in exam score, on average, and this is statistically significant (p = .030).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nSuppose we fit a simple linear regression model to predict \\(y\\) from a single predictor \\(x\\). When testing whether \\(x\\) is a significant predictor — that is, testing\n\n\\(H_0:\\beta = 0\\)\n\\(H_0:\\beta \\ne 0\\),\n\nthe output gives a t-statistic of 2.57. At the 5% significance level, the corresponding t-critical value is 1.65.\nBased on this information, what conclusion should we draw about the null hypothesis?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nSince \\(|t|=2.57&gt;t_{crit}=1.65\\), we can reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nEngineers and consumers are often interested in understanding what factors influence a car’s fuel efficiency. In this exercise, we will explore how three key characteristics relate to fuel consumption:\n\nwt: the weight of the car (in 1000 lbs)\nhp: engine horsepower\nqsec: time required to travel a quarter mile (in seconds)\n\nResearchers are interested in whether these variables help explain variation in fuel efficiency using the multiple regression model:\n\\[mpg=\\beta_0+\\beta_1(wt)+\\beta_2(hp)+\\beta_3(qsec)\\] Suppose the regression output looks like:\n\n\n.CoefficientsSEt_statP_valueIntercept27.6118.4193.2790.00300wt-4.3590.753-5.7910.00001hp-0.0180.015-1.1900.24400qsec0.5110.4391.1630.25500\n\n\n\nWrite the fitted regression equation for this scenario.\nWhich variables are statistically significant in this model? Let \\(\\alpha=0.05\\)\nWrite a short paragraph that summaries the findings.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\\(mpg=27.611-4.359(wt)-0.018(hp)+0.511(qsec)\\)\nOnly wt is statistically significant (p = .00001)\nIn this multiple regression model, car weight is the only strong and statistically significant predictor of fuel efficiency. Heavier cars get substantially lower mpg, even after accounting for horsepower and acceleration. Horsepower and quarter-mile time show weak relationships with mpg and are not statistically significant in this model. This suggests that weight is the dominant factor influencing fuel economy among the variables considered.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nA researcher investigates whether daily screen time (hours) predicts students’ exam scores.\nThe regression output for the slope coefficient \\(\\beta_1\\) is:\n\n\n\nTerm\nEstimate\nSE\n\n\n\n\nScreen time\n\\(-1.80\\)\n\\(0.72\\)\n\n\n\nUsing this output, compute the 95% confidence interval for \\(\\beta_1\\). (Use the normal approximation with critical value \\(1.96\\).)\nDoes your CI provide evidence that there is a statistically significant finding?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\\[\n\\begin{aligned}\nCI &= -1.80 \\pm 1.96 \\times 0.72 \\\\\n   &= -1.80 \\pm 1.41 \\\\\n   &= [-3.21,\\; -0.39]\n\\end{aligned}\n\\]\nBecause the 95% CI does not capture 0, we can reject \\(H_0\\) and conclude that there is a significant relationship between daily screen time and exam scores. (specifically, more screen time tends to reduce exam score)\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nDownload the file below and open it in Excel.\n\n\n Wage\n\n\nThis workbook contains data collected from 200 employees, and relates to their wage (per 100,000), age and years of education.\n\nCreate scatterplots to explore the relationship between (i) wage and age, and (ii) wage and years of education.\nRun a simple linear regression model to predict wage using only age as a predictor. Interpret the beta coefficient and p-value for this model.\nRun a simple linear regression model to predict wage using only education as a predictor. Interpret the beta coefficient and p-value for this model.\nRun a multiple regression model with both age and education as predictors. Interpret the beta coefficients and p-values for this model.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe estimated slope for age is 0.356. This means that, on average, each additional year of age is associated with an increase of approximately $0.36 in hourly wage. The p-value is less than the significance level (using \\(\\alpha=0.05\\)). We conclude that age is a statistically significant predictor of wage in this simple regression model.\nThe estimated slope for years of education is 3.40. This indicates that each additional year of education is associated with an increase of approximately $3.40 in hourly wage, on average. The p-value is less than the significance level (using \\(\\alpha=0.05\\)). We conclude that years of eduction is a statistically significant predictor of wage in this simple regression model.\n\nHolding age constant, each additional year of education is associated with an increase of approximately $3.54 in hourly wage, and this is statistically significant.\nHolding education constant, each additional year of age is associated with a decrease of 0.045, however this is not statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nThis question continues from Question 7 above. Look at the output you created for parts b to d (i.e. the 3 regression models) and answer the following questions.\n\nHow does the estimated effect of age change when years of education is included in the model?\nProvide a statistical explanation for why this change occurs.\nWhat does this tell us about the relationship between age, education, and wage?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nIn the simple regression model, age has a positive and statistically significant effect on wage. However, once years of education is included in the multiple regression model, the estimated effect of age becomes close to zero and statistically insignificant. This indicates that age no longer explains additional variation in wages after controlling for education.\nThis change occurs because age and years of education are correlated. In the simple regression, age captures not only its own effect but also part of the effect of education on wages. When education is omitted, this leads to omitted variable bias in the estimated age coefficient. Including education in the model isolates the direct effect of each variable, revealing that the apparent effect of age was driven by its association with education.\nThis suggests that education is the primary driver of wage differences, while age has little independent effect once education is taken into account. The observed relationship between age and wage in the simple model reflects the fact that older individuals tend to have higher levels of education, which in turn leads to higher wages. Thus, education mediates the relationship between age and wage.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html",
    "href": "06-More-regression.html",
    "title": "6  More Regression Concepts",
    "section": "",
    "text": "6.1 Introduction\nIn the previous chapters, we learnt how to build regression models and how to use hypothesis testing to decide whether the relationships we observe are likely to be real or simply due to chance. We now know how to estimate effects, interpret coefficients, and assess statistical significance.\nBut real data rarely fits neatly into a single, simple regression line.\nIn practice, we often want to answer deeper questions, such as:\nThis chapter extends regression beyond the basics and shows how it adapts to more realistic and complex situations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#introduction",
    "href": "06-More-regression.html#introduction",
    "title": "6  More Regression Concepts",
    "section": "",
    "text": "How well does my model explain what is happening?\nWhat happens when predictors are categories rather than numbers?\nHow do relationships change over time?\nDoes the effect of one variable depend on another?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#goodness-of-fit",
    "href": "06-More-regression.html#goodness-of-fit",
    "title": "6  More Regression Concepts",
    "section": "6.2 Goodness of fit",
    "text": "6.2 Goodness of fit\nSo far, we have focused on interpreting individual regression coefficients and using p-values to assess whether a relationship is statistically significant. However, a statistically significant relationship does not automatically mean that a model is useful or informative.* A model can detect a real association while still doing a poor job of explaining the outcome overall. This leads to an important next question:\n\nhow well does our regression model actually fit the data?\n\nMeasures of goodness of fit help us quantify how closely the model’s predictions align with the observed values, allowing us to assess how much of the variation in the outcome is explained by the model and to compare different regression models in a meaningful way.\n\n6.2.1 \\(R^2\\) as a measure of model fit\nOne of the most commonly reported measures of goodness of fit in regression is \\(R^2\\), known as the coefficient of determination. While regression coefficients tell us about the direction and size of relationships between variables, \\(R^2\\) focuses on the model as a whole. Conceptually, \\(R^2\\) answers the following question:\n\nHow much of the variation in the outcome variable is explained by the regression model?\n\nTo understand this, recall that real-world data vary. Individuals differ in their outcomes for many reasons, not all of which are captured by the predictors in a model. A regression model attempts to explain some of this variability using the predictors we include. The remaining variation reflects factors not in the model, randomness, or measurement error.\nThe value of \\(R^2\\) lies between 0 and 1:\n\nAn \\(R^2\\) close to 0 indicates that the model explains very little of the variation in the outcome.\nAn \\(R^2\\) close to 1 indicates that the model explains a large proportion of the variation.\n\nFor example, an \\(R^2=0.65\\) means that 65% of the variability in the outcome variable can be explained by the predictors in the regression model, while the remaining 35% is unexplained.\n\n\n6.2.2 Interpreting \\(R^2\\) in context\nIt is important to interpret \\(R^2\\) in context, rather than treating it as a universal benchmark of model quality.\nIn fields such as physics or engineering, very high \\(R^2\\) values are common because systems are tightly controlled. In contrast, in social, behavioural, and economic data — — human behaviour is influenced by many unobserved factors. As a result, moderate \\(R^2\\) values can still represent meaningful and useful models.\nA low \\(R^2\\) does not necessarily imply that a model is “wrong.” It may simply reflect the complexity of the phenomenon being studied. Likewise, a high \\(R^2\\) does not guarantee that a model is appropriate or that the relationship is causal.\n\n\n6.2.3 \\(R^2\\) and statistical significance\nA common misconception is that a high \\(R^2\\) is required for regression coefficients to be statistically significant. In reality, these concepts address different questions:\n\nStatistical significance (via p-values) assesses whether a relationship is likely to exist in the population.\n\\(R^2\\) assesses how much of the outcome’s variability the model explains.\n\nIt is entirely possible to have:\n\na statistically significant coefficient with a low \\(R^2\\), or\na high \\(R^2\\) with some non-significant predictors.\n\nThis distinction reinforces why both coefficient-level inference and model-level fit should be considered together when interpreting regression results.\n\n\n6.2.4 Computing \\(R^2\\)\nBack in Chapter 2 we learnt the formula for the variance as:\n\\[\n\\frac{1}{n-1}\\sum_{i=1}^{n} (X_i - \\bar{X})^2\n\\] This formula (more importantly: the sum of squares) will help us to determine \\(R^2\\). Let’s use a small sample to explore this. Consider the time spent on the phone in one week (mins) by 6 random university students. In the table below, we compute the squared errors for each student:\n\n\nStudentTime130823003306430453086310\n\n\nWe’ll begin by visualising this data on a scatterplot (and we can see that overall, there is variability bewteen the data points). If we plot the mean (\\(306\\)), we can see how far away each point is from this line (referred to as a deviation, or error):\n\n\n\n\n\n\n\n\n\n\n\nWhen we add up the squared errors, we get the sum of squares. Now at this point, we are only looking at the outcome variable (i.e. we haven’t included any predictors yet), so we can refer to this as the sum of squares total (\\(SS_T\\)):\n\\[\n\\begin{aligned}\nSS_T &= \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\\\\n   &= (308-306)^2+(300-306)^2+(306-306)^2+(304-306)^2+(308-306)^2+(310-306)^2 \\\\\n   &= 4+36+0+4+4+16 \\\\\n   &= 64\n\\end{aligned}\n\\]\nNote there are three sources of \\(SS\\) that we need to be aware of:\n\n\\(SS_T\\) (Sum of Squares Total)\n\\(SS_E\\) (Sum of Squares Error)\n\\(SS_R\\) (Sum of Squares Regression)\n\nIn this example, we have computed \\(SS_T=64\\). In other words:\n\nThe total variability in “Time” is 64 units\n\nNow suppose we build a regression model to predict Time based on number of social media friends (let’s call this variable Social). Suppose the data looks something like this:\n\n\nStudentTimeSocial130811023001153306120430412553081306310135\n\n\nAnd let’s suppose we used software to obtain the regression coefficients for this model to be:\n\\[Time=283.6+0.18(Social)\\]\nUsing this regression equation, we can see what the model would have predicted Time to be for each student. And we can determine the \\(SS\\) using a similar approach to \\(SS_T\\). However, we refer to this as \\(SS_E\\) (Sum of Squares Error):\n\n\nStudentTimeSocialModelErrorError21308110283.6 + (0.18 x 110) = 303.44.618.52300115283.6 + (0.18 x 115) = 304.3-4.321.23306120283.6 + (0.18 x 120) = 305.20.80.34304125283.6 + (0.18 x 125) = 306.1-2.16.35308130283.6 + (0.18 x 130) = 307.01.00.46310135283.6 + (0.18 x 135) = 307.92.10.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nSS_E &= \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\\\\n   &= (308-303.4)^2+(300-304.3)^2+(306-305.2)^2+(304-306.1)^2+(308-307.0)^2+(310-307.9)^2 \\\\\n   &= 18.5+21.2+0.3+6.3+0.4+2.9 \\\\\n   &= 49.4\n\\end{aligned}\n\\] With these two sources of error (\\(SS_T\\) and \\(SS_E\\)), we can determined \\(SS_R\\). This is because\n\\[SS_T=SS_E+SS_R\\]\nTherefore:\n\\[SS_R=SS_T-SS_E\\]\nAnd because we have calculated \\(SS_T\\) and \\(SS_E\\) to be 64 and 49.4, respectively, we can solve \\(SS_R\\) to be:\n\\[SS_R=64-49.4=14.6\\]\nMore importantly, if we take the ratio of \\(SS_R\\) and \\(SS_T\\), we can express how much of the variance in our outcome is explained by the regression model, rather than left unexplained by random variation. This is called the coefficient of determination, or \\(R^2\\).\n\\[\n\\begin{aligned}\nR^2 &= \\frac{SS_R}{SS_T} \\\\\n   &= \\frac{14.6}{64} \\\\\n   &= 0.229\n\\end{aligned}\n\\]\nIn this example, we can interpret this as:\n\n22.9% of the variation in time on the phone can be explained by this model.\n\nNote: \\(R^2\\) is more suitable when we are dealing with simple linear regression. For multiple linear regression, we should use \\(\\text{Adjusted } R^2\\). This is because \\(R^2\\) will always increase (or stay the same) when additional predictors are added to a model, even if those predictors contribute little or no meaningful explanatory power, whereas adjusted \\(R^2\\) penalises unnecessary model complexity and provides a fairer measure of model fit.\nWe will learn how to compute \\(\\text{Adjusted } R^2\\) in the Excel section of this chapter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#regression-with-a-binary-predictor",
    "href": "06-More-regression.html#regression-with-a-binary-predictor",
    "title": "6  More Regression Concepts",
    "section": "6.3 Regression with a binary predictor",
    "text": "6.3 Regression with a binary predictor\nUp to this point, all of our regression examples have used numeric predictors. In these settings, interpretation is relatively straightforward: a regression coefficient represents the expected change in the outcome associated with a one-unit increase in the predictor, holding all other variables constant.\nHowever, many important variables in real data are categorical rather than numeric. Examples include treatment group, employment status, or whether a customer is new or returning. Regression can still be used in these situations, but the interpretation of the coefficients changes.\nWe begin with the simplest categorical case: a binary predictor, which takes only two possible values. Suppose we are interested in understanding job satisfaction among employees. For each employee, we record:\n\nJob satisfaction (measured on a numeric scale)\nIncome (numeric)\nYears at company, coded as a binary variable:\n\n0 = less than 4 years\n1 = 4 years or more\n\n\n\\[\\text{Job Satisfaction} =\\beta_0+\\beta_1\\text{Income}+\\beta_2 \\text{Years}\\]\nSuppose we ran a regression model on some data and determined the coefficients to be:\n\\[\\text{Job Satisfaction} =7.2953+0.001(\\text{Income})+ 3.4714 (\\text{Years})\\]\nLet’s interpret what the coefficient for Years (here = 3.4717) actually means by comparing two people with different codes for this variable, but keeping everything else constant:\n\nPerson A\n\nIncome: \\(75,000\\)\nYears: “Less than 4” (0)\n\nPerson B\n\nIncome: \\(75,000\\)\nYears: “More than 4” (1)\n\n\nIf we substitute these values into the model for each person, we can determine their predicted job satisfaction scores:\n\\[\\text{Job Satisfaction}_A =7.2953+0.001(\\text{Income})+ 3.4714 (\\text{0})=14.7953\\] \\[\\text{Job Satisfaction}_B =7.2953+0.001(\\text{Income})+ 3.4714 (\\text{1})=18.2667\\]\nThe difference between the two predictions (18.2667 – 14.7953) is equal to the coefficient for years (3.4714). Therefore, we can interpret this coefficient as:\n\nJob satisfaction is 3.4714 points higher, on average, for employees who have been at the company for more than 4 years, compared to those who have been at the company less than 4 years, when income is controlled for.\n\nNow, what if we had used different codes? E.g. what if we switched the 0s and 1s the other way?\n\nYears at company, coded as a binary variable:\n\n1 = less than 4 years\n0 = 4 years or more\n\n\nIn this case, the coefficients for the other terms might change, but the coefficient for Years will remain the same. The only difference is that the direction (i.e. positive or negative) will switch. For example, running a regression analysis on the same data, but switching the codes for Years would yield us the following:\n\\[\\text{Job Satisfaction} =10.7668+0.001(\\text{Income})- 3.4714 (\\text{Years})\\]\nIf we take the same two people from before, but now with the codes reversed:\n\nPerson A\n\nIncome: \\(75,000\\)\nYears: “Less than 4” (1)\n\nPerson B\n\nIncome: \\(75,000\\)\nYears: “More than 4” (0)\n\n\nAnd substitute these values into the model for each person with the new model:\n\\[\\text{Job Satisfaction}_A =10.7668+0.001(\\text{Income})- 3.4714 (\\text{1})=14.7953\\]\n\\[\\text{Job Satisfaction}_B =10.7668+0.001(\\text{Income})- 3.4714 (\\text{0})=18.2667\\]\nNotice here that the predictions work out to be exactly the same as above. And that the difference between the two predictions (18.2667 – 14.7953) is equal to the coefficient for years (3.4714).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#regression-with-a-categorical-predictor-k-2-levels",
    "href": "06-More-regression.html#regression-with-a-categorical-predictor-k-2-levels",
    "title": "6  More Regression Concepts",
    "section": "6.4 Regression with a categorical predictor ((k > 2) levels)",
    "text": "6.4 Regression with a categorical predictor ((k &gt; 2) levels)\nThe binary predictor case works neatly because there are only two groups. But in many real-world problems, categorical variables have more than two categories. Common examples include job position, education level, or type of contract. Regression can still be used in these situations — but we need to be more careful about how the variable is represented in the model.\nSuppose we extend our job satisfaction example by including job position as a predictor, with three possible levels:\n\nClerical\n\nLabourer\n\nManager\n\nOur goal is to understand how job satisfaction differs across these roles, while still controlling for income.\n\n6.4.1 Choosing a reference level\nA categorical variable with three levels does not correspond to a single numeric coefficient. Instead, regression works by comparing groups to a reference level (also called the baseline).\nWe must choose one category to act as this reference group. Suppose we choose:\n\nClerical as the reference level.\n\nAll other categories will be interpreted relative to clerical employees.\n\n\n6.4.2 Dummy coding (indicator variables)\nTo include job position in the regression model, we create dummy variables (also called indicator variables). With three categories, we need two dummy variables:\n\nLabourer\n\n1 = Labourer\n\n0 = otherwise\n\nManager\n\n1 = Manager\n\n0 = otherwise\n\n\nClerical employees are implicitly represented when both dummy variables are equal to 0.\nThe regression model can be written as:\n\\[\n\\text{Job Satisfaction}\n= \\beta_0\n+ \\beta_1 \\text{Income}\n+ \\beta_2 \\text{Labourer}\n+ \\beta_3 \\text{Manager}\n\\]\n\n\n6.4.3 Interpreting the coefficients\nUnder this coding:\n\n\\((\\beta_1)\\) represents the predicted job satisfaction for every unit increase in income, holding position constant.\n\\((\\beta_2)\\) represents the expected difference in job satisfaction between labourers and clerical employees, holding income constant.\n\\((\\beta_3)\\) represents the expected difference in job satisfaction between managers and clerical employees, holding income constant.\n\nOnce we include a categorical predictor, the coefficients should be interpreted as group differences relative to the reference category, rather than as “per-unit increases”.\nAs an example, suppose we used software to fit the model, and we determined the coefficients to be:\n\\[\n\\text{Job Satisfaction}\n= 6.20\n+ 0.002\\,(\\text{Income})\n- 1.50\\,(\\text{Labourer})\n+ 2.80\\,(\\text{Manager})\n\\] When can interpret each term in this equation as:\n\nIncome \\((\\beta_1 = 0.002)\\)\nFor every $1 increase in income, predicted job satisfaction increases by 0.002 points, on average, holding job position constant.\nLabourer \\((\\beta_2 = -1.50)\\)\nLabourers are predicted to have job satisfaction scores that are 1.50 points lower than clerical employees, on average, holding income constant.\nManager \\((\\beta_3 = 2.80)\\)\nManagers are predicted to have job satisfaction scores that are 2.80 points higher than clerical employees, on average, holding income constant.\n\n\n\n\n6.4.4 Why we cannot include all three positions as predictors\nA natural question at this point is:\n\nWhy don’t we just include three dummy variables — one for clerical, one for labourer, and one for manager?\n\nFor example:\n\\[\n\\text{Job Satisfaction}\n= \\beta_0\n+ \\beta_1 \\text{Income}\n+ \\beta_2 \\text{Labourer}\n+ \\beta_3 \\text{Manager}\n+ \\beta_3 \\text{Clerical}\n\\]\nIf we attempted this, we would run into a problem known as perfect multicollinearity. This occurs because, for every observation:\n\\[\n\\text{Clerical} + \\text{Labourer} + \\text{Manager} = 1.\n\\]\nThis creates redundancy in the predictors, meaning the regression model cannot estimate all coefficients uniquely.\n\n\n\n\n\n\nMore advanced: why the model becomes rank deficient (matrix view)\n\n\n\nThe maths for this is beyond the scopes of this unit, but for those interested, consider that linear regression can be written in matrix form as:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}.\n\\]\nIf we include an intercept, income, and three dummy variables (clerical, labourer, manager), then for three example employees (A), (B), and (C), the design matrix would look like:\n\\[\n\\mathbf{X}\n=\n\\begin{bmatrix}\n1 & \\text{Income}_A & 1 & 0 & 0 \\\\\n1 & \\text{Income}_B & 0 & 1 & 0 \\\\\n1 & \\text{Income}_C & 0 & 0 & 1\n\\end{bmatrix}.\n\\]\nFor every row, exactly one of the three dummy variables equals 1 and the other two equal 0. Therefore:\n\\[\n\\text{Clerical} + \\text{Labourer} + \\text{Manager} = 1 = \\text{Intercept}.\n\\]\nThis means:\n\\[\n\\text{Column 3} + \\text{Column 4} + \\text{Column 5} = \\text{Column 1}.\n\\]\nIn other words, one column of the design matrix is an exact linear combination of the others. The matrix () is therefore rank deficient (it does not have full rank).\nOrdinary least squares (OLS) estimation relies on:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}.\n\\]\nWhen \\(\\mathbf{X}\\) is rank deficient, the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\) is singular (non-invertible), so OLS regression cannot compute unique coefficient estimates.\nIn practice, statistical software will typically return an error or warning such as:\n\n“Coefficients are not uniquely determined”\n“Model matrix is rank deficient”\n\nIn Excel, the model will still run, but one of the rows will have an error:\n\n\n\n\n\n\n\n6.4.5 The fix: use (k - 1) dummy variables\nTo avoid this issue, we must always include only (k - 1) dummy variables for a categorical predictor with (k) levels, treating the remaining category as the reference level.\nWith three job positions, we include two dummy variables (e.g. labourer and manager), and clerical becomes the baseline group.\n\n\n6.4.6 Does the reference level matter?\nThe choice of reference level affects how we interpret the coefficients, but it does not change the fitted values or predictions produced by the model. Changing the reference category simply changes which comparisons appear explicitly in the regression output.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#regression-with-time-series",
    "href": "06-More-regression.html#regression-with-time-series",
    "title": "6  More Regression Concepts",
    "section": "6.5 Regression with time-series",
    "text": "6.5 Regression with time-series\nUp to this point, our independent variables (IVs) have been either continuous (such as income or years at the company) or categorical (such as position coded with dummy variables).\nHowever, when we work with time series data, the situation is quite different. Time carries a natural sequential order, and the relationships between observations often depend on their position in that sequence.\nUnlike income or years of experience, which can vary independently across individuals, time introduces autocorrelation - today’s outcome is usually related to yesterday’s.\nAs an example, suppose I had some Google Trends data, where I wanted to see how popular was the search term AFL on Google. And, let’s suppose, I looked at this across a five year period. The table below shows the first and last five obervations in this data set.\n\n\nWeekTimeafl2020-08-161432020-08-232522020-08-303522020-09-064572020-09-13553⋮⋮⋮2025-07-20258472025-07-27259482025-08-03260532025-08-10261532025-08-1726242\n\n\nIf we were to plot the data it shows a very obvious seasonal pattern, where AFL interest is low in Q1 and Q4 of any given year (because it’s the off-season), and high during Q2 and Q3 (within season).\n\n\n\n\n\n\n\n\n\n\n\nNow, suppose we modelled afl as a regression model (using Time as our predictor). Visually, this might look like:\n\n\n\n\n\n\n\n\n\n\n\nNotice here that this model, which is a straight line, is only picking up on the trend of the data. It does a very poor job of predicting actual afl interest for most of the data. As an example, suppose the fitted model was:\n\\[afl=32.45+0.023(Time)\\]\nAnd suppose we wanted to predict afl interest at time = 62 (which was 17/10/2021). The predicted value would be:\n\\[afl=32.45+0.023(62)=33.876\\]\nThis is quite different to the actual value of 8:\n\n\n\n\n\n\n\n\n\n\n\nIn future units, you will learn about time-series analysis, which is a more appropriate technique for handling this type of data. In this unit, ETC1000, you just need to be aware of this limitation when constructing your regression models.\nAs an example, consider the image below. Both panels use the exact same data (afl over time). The panel on the left is fitted with a linear regression model, and has made a linear forecast for the next 52 weeks. The panel on the right is a time series model, and it’s forecast is much more reflective on the actual data:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#interaction-effects",
    "href": "06-More-regression.html#interaction-effects",
    "title": "6  More Regression Concepts",
    "section": "6.6 Interaction Effects",
    "text": "6.6 Interaction Effects\nThus far, our regression models have only considered additive effects, where each predictor is assumed to influence the outcome independently of the others. These are sometimes referred to as main effects. Under this assumption, the effect of a predictor — such as income, years at a company, or job position — is the same for all individuals, regardless of the values of other variables in the model.\nIn many real-world situations, however, this assumption is too simplistic. The relationship between a predictor and the outcome may depend on another variable. For example, an increase in income may have a different impact on job satisfaction for managers than it does for clerical staff, or the effect of time at a company may vary across roles. Interaction effects allow us to model these conditional relationships explicitly, extending regression beyond “average effects” and enabling more nuanced and realistic interpretations.\nAs an example, suppose a team of scientists recruited 40 high school students who enjoyed playing violent video games on a regular basis. They were asked to keep a record of how many hours of violent video games they played in one week. In addition, they had to complete a (pre-post) aggression questionnaire, where higher scores represented higher levels of aggression.\nIn this example, the researchers are investigating if time spent playing violent video games is associated with aggression. Graphically, this could be expressed as predictor \\(\\longrightarrow\\) Outcome (a standard one-one relationship):\n\n\n\n\n\n\n\n\n\nInteractions occur when we believe a third variable (usually called a moderator) can influence that one-to-one relationship:\n\n\n\n\n\n\n\n\n\nIn our current example, the main effect is:\n\n\n\n\n\n\n\n\n\nIn our current dataset, if we consider just the relationship between Time and Aggression, we can observe a positive linear trend line (i.e. the more time one plays violent video games \\(\\longrightarrow\\) the higher the aggression score), see the left panel below. However, suppose there was a third variable, History of Family Violence (yes / no). And we fitted separate models for each group. We can see that depending on whether or not the participant had a history of family violence, then the slope of the relationship is quite different (see the right panel).\n\n\n\n\n\n\n\n\n\nIf we look at the relationship between Time and Aggression for those with a family history of violence,then it appears that there is a strong positive relationship between Time and Aggression. However, if we only look at the children from the No history of violence group, then we can see that there is a very weak relationship between Time and Aggression. Given these differences, using the original scatter plot (left-panel) would not paint a clear picture of what is really happening. Depending on the child’s group (Yes / No), the relationship between Time and Aggression is quite different!\n\nTherefore, we would say that Family History is moderating the relationship between Time and Aggression\n\nMathematically, moderation (or interaction) is represented by including a product term in the regression model, formed by multiplying the two variables together (for example, Variable 1 × Variable 2). This interaction term allows the slope of one predictor to vary across levels of the other.\nSuppose the first five rows of our data looked like the table below, where\n\nGroup (0 = No family history; 1 = Yes family history)\n\n\n\nGroupTimeAggression05.422.7814.046.4204.786.8707.205.0914.306.58\n\n\nThe interaction term is computed by multiplying Group and Time together. In this example:\n\n\nGroupTimeGroup.TimeAggression05.420 x 5.42 = 02.7814.041 x 4.04 = 4.046.4204.780 x 4.78 = 06.8707.200 x 7.2 = 05.0914.301 x 4.3 = 4.36.58\n\n\nWe would then fit this as a term (effect) in our regression model. Suppose the coefficients table, for the full 40 children provided the following results:\n\n\n.CoefficientsSEt.StatP.valueIntercept3.8540.9024.2740.0001Group-1.8141.184-1.5230.1311Time0.2390.1731.3840.1718Group*Time0.6630.2193.0230.0038\n\n\nUsing this table, we can determine the model to be:\n\\[\nAggression=3.85-1.81(Group)+0.24(Time)+0.66(Group\\times Time)\n\\]\nAnd because we know how Group is coded (0 = No; 1 = Yes), we can construct seperate models for each group:\n\\[\n\\begin{aligned}\nAggression_{Group:0} &= 3.85 -1.81(0)+0.24(Time)+0.66(0 \\times Time) \\\\\n   &= 3.85+0.24(Time) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nAggression_{Group:1} &= 3.85 -1.81(1)+0.24(Time)+0.66(1 \\times Time) \\\\\n   &= 2.04+0.90(Time) \\\\\n\\end{aligned}\n\\]\nNotice here that the slope for Group 0 (No history of family violence) is relatively small (\\(\\beta=0.24\\)), indicating little increase in Aggression as Time increases. By comparison, the slope for Group 1 (Yes history of family violence) is much larger (\\(\\beta=0.90\\)), indicating a much larger increase in Aggression as Time increases. This is consistent with what we saw in our scatterplots earlier:\n\n\n\n\n\n\n\n\n\nFinally, let’s look at the p-values for this scenario.\n\n\n.CoefficientsSEt.StatP.valueIntercept3.8540.9024.2740.0001Group-1.8141.184-1.5230.1311Time0.2390.1731.3840.1718Group*Time0.6630.2193.0230.0038\n\n\nThe interaction term (Group × Time) is statistically significant (p = .0038), providing strong evidence that the effect of Time on Aggression differs between the two groups. In contrast, the main effect of Time alone (p = 0.1718) is not statistically significant, highlighting that the relationship between Time and Aggression cannot be adequately described by a single common slope. Together, these results confirm that Group moderates the relationship between Time and Aggression, which is precisely what we observed visually in the scatterplots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#excel",
    "href": "06-More-regression.html#excel",
    "title": "6  More Regression Concepts",
    "section": "6.7 Excel",
    "text": "6.7 Excel\nIn this section we will learn how to use Microsoft Excel to solve some of the concepts covered in this chapter. Begin by downloading the data file below and then follow the instructions by navigating through the screenshots.\n\n\n\n Job Satisfaction\n\n\n\n\n6.7.1 Recap of Excel’s Data Analysis Tool Pack for Regression\n\n1234\n\n\nBegin by loading the data into Excel.\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Data tab, select Data Analysis and choose Regression.\n\n\n\n\n\n\n\n\n\n\n\nFrom there, specify your input Y (here: Job Satisfaction, so cells D1 to D31) and input X (here: Income, cells A1 to A31) ranges. Make sure “Labels” is selected, and specify your output range (e.g. F2).\n\n\n\n\n\n\n\n\n\n\n\nClicking OK, generates the output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.2 Multiple Regression with a Binary predictor\nHave a look at the second column in the data set (Years at company), which is a numeric variable from 1 to 8. In this exercise we will transform this variable into a binary one with two categories:\n\n0: Less than 4 years\n1: 4 or more years\n\n\n1234\n\n\nInsert a blank column to the right of the Years at company column. Call this new column Years_Cat. The easiest way to do this is to right click the current Colunm C and select “Insert”.\n\n\n\n\n\n\n\n\n\n\n\nWe need to write a formula that dichotomises Years at company into either 0 (if less than 4) or 1 (4 or more). In cell C2 type:\n\n=IF(B2 &lt; 4, 0, 1)\n\nand apply this to each cell in this column.\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Data tab, select Data Analysis and choose Regression. From there, specify your input Y (here: Job Satisfaction, so cells E1 to E31) and input X (here: Years_Cat, cells C1 to C31) ranges. Make sure “Labels” is selected, and specify your output range (e.g. G2).\n\n\n\n\n\n\n\n\n\n\n\nClicking OK, generates the output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.3 Multiple Regression with a Categorical predictor\nHave a look at the variable Position. This is a categorical variable with three levels:\n\nClerical\nManager\nLabourer\n\nIn this excercise we will create three new columns using dummy coding for this variable and construct a multiple regression model.\n\n1234567\n\n\nInsert 3 blank columns to the right of the Position column. Call these columns, ‘Clerical’, ‘Manager’ and ‘Labourer’ respectively.\n\n\n\n\n\n\n\n\n\n\n\nIn cell D2 (Clerical) we can use the following formula to write a 1 if Position is Clerical, otherwise return a 0. Apply this formula to all cases in this column.\n\n=IF(C2 = \"Clerical\", 1, 0)\n\n\n\n\n\n\n\n\n\n\n\n\nWrite and apply a similar formula for Managers (in cell E2).\n\n=IF(C2 = \"Manager\", 1, 0)\n\n\n\n\n\n\n\n\n\n\n\n\nWrite and apply a similar formula for Labourers (in cell F2).\n\n=IF(C2 = \"Labourer\", 1, 0)\n\n\n\n\n\n\n\n\n\n\n\n\nSelect one level to be the reference category. This level is not included into the model. For example, the screenshot below will use ‘Clerical’ as the reference\n\n\n\n\n\n\n\n\n\n\n\nSwitch to the Data tab, select Data Analysis and choose Regression. From there, specify your input Y (here: Job Satisfaction, so cells G1 to G31) and input X (here: Manager and Labourer, cells E1 to F31) ranges. Make sure “Labels” is selected, and specify your output range (e.g. I2).\n\n\n\n\n\n\n\n\n\n\n\nClicking OK, generates the output.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#summary",
    "href": "06-More-regression.html#summary",
    "title": "6  More Regression Concepts",
    "section": "6.8 Summary",
    "text": "6.8 Summary\nIn this chapter, we extended regression beyond the “one predictor, one straight line” idea and explored several concepts that make regression more useful for real-world data.\nFirst, we introduced goodness of fit, showing that statistical significance alone does not tell us whether a model is useful. We learnt that \\((R^2)\\) describes the proportion of variation in the outcome explained by the model, and we linked this idea back to the sum of squares framework (\\(SS_T\\), \\(SS_E\\), and \\(SS_R\\)). We also highlighted why \\(R^2\\) is most appropriate for simple linear regression, and why adjusted \\(R^2\\) is preferred in multiple regression settings where adding extra predictors can artificially inflate \\(R^2\\).\nNext, we showed how regression can incorporate categorical predictors. With a binary predictor, coefficients are interpreted as differences between two groups (relative to whichever group is coded as 0). With a categorical predictor that has more than two levels, we learnt that regression requires dummy coding and a reference category. We also discussed why including all \\(k\\) dummy variables creates perfect multicollinearity, leading to a rank-deficient design matrix and preventing unique coefficient estimation.\nWe then explored regression with time series data, where a simple linear regression often captures only the overall trend and can miss important patterns such as seasonality.\nFinally, we introduced interaction effects, where the effect of one predictor depends on the value of another. Using the violent games example, we saw how interactions appear as different slopes for different groups, how to compute the interaction term, how to write separate regression equations for each group, and how to interpret the key p-value for the interaction term as evidence of moderation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "06-More-regression.html#exercises",
    "href": "06-More-regression.html#exercises",
    "title": "6  More Regression Concepts",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nA regression model predicting weekly phone time from number of social media friends produces:\n\\[\nR^2 = 0.23\n\\]\n(a) What does this value mean in context?\n(b) Does \\(R^2 = 0.23\\) imply the model is “bad”? Explain briefly.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n(a)\nAn \\(R^2\\) value of 0.23 means that 23% of the variation in weekly phone time is explained by the regression model. The remaining 77% of the variation is due to other factors, randomness, or measurement error.\n(b)\nNo. It depends on the discipline. Moderate or low \\(R^2\\) values are common because outcomes are influenced by many unobserved factors. A model can still capture a meaningful relationship even if \\(R^2\\) is not large.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nYou are told that for a regression model:\n\\[\nSS_T = 80\n\\]\n\\[\nSS_E = 50\n\\]\n(a) Compute \\(SS_R\\).\n(b) Compute \\(R^2\\).\n(c) Interpret the result.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n(a)\n\\[\nSS_R = SS_T - SS_E = 80 - 50 = 30\n\\]\n(b)\n\\[\nR^2 = \\frac{SS_R}{SS_T} = \\frac{30}{80} = 0.375\n\\]\n(c)\nThe regression model explains 37.5% of the variation in the outcome variable.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nA regression model predicting job satisfaction includes a binary variable:\n\\[\n\\text{Years} =\n\\begin{cases}\n0 & \\text{less than 4 years} \\\\\n1 & \\text{4 years or more}\n\\end{cases}\n\\]\nThe fitted model is:\n\\[\n\\text{Job Satisfaction}\n= 7.30 + 0.001(\\text{Income}) + 3.47(\\text{Years})\n\\]\n(a) Interpret the coefficient 3.47.\n(b) If the coding of Years is reversed, what happens to the sign of the coefficient?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n(a)\nHolding income constant, employees who have been at the company for 4 years or more are predicted to have job satisfaction scores that are 3.47 points higher, on average, than employees who have been at the company for less than 4 years.\n(b)\nIf the coding is reversed, the sign of the coefficient will change, but the predicted values and group differences remain the same. Only the reference group changes.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nJob position has three levels: Clerical, Labourer, and Manager. Clerical is the reference group. The fitted model is:\n\\[\n\\text{Job Satisfaction}\n= 6.20 + 0.002(\\text{Income})\n- 1.50(\\text{Labourer})\n+ 2.80(\\text{Manager})\n\\]\nAssume income is held constant.\n(a) Interpret the coefficient −1.50.\n(b) Interpret the coefficient 2.80.\n(c) What values do the dummy variables take for a clerical employee?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n(a)\nLabourers are predicted to have job satisfaction scores that are 1.50 points lower than clerical employees, on average, holding income constant.\n(b)\nManagers are predicted to have job satisfaction scores that are 2.80 points higher than clerical employees, on average, holding income constant.\n(c)\nFor clerical employees (the reference group):\n\\[\n\\text{Labourer} = 0\n\\]\n\\[\n\\text{Manager} = 0\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nA regression model examining aggression includes:\n\\[\n\\text{Group} =\n\\begin{cases}\n0 & \\text{no family violence history} \\\\\n1 & \\text{family violence history}\n\\end{cases}\n\\]\nThe fitted model is:\n\\[\n\\text{Aggression}\n= 3.85 - 1.81(\\text{Group})\n+ 0.24(\\text{Time})\n+ 0.66(\\text{Group} \\times \\text{Time})\n\\]\n(a) Write the regression equation for Group = 0.\n(b) Write the regression equation for Group = 1.\n(c) Interpret the interaction coefficient 0.66.\n(d) If the p-value for the interaction term is 0.0038, what is the conclusion?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n(a) For Group = 0:\n\\[\n\\text{Aggression}\n= 3.85 + 0.24(\\text{Time})\n\\]\n(b) For Group = 1:\n\\[\n\\begin{aligned}\n\\text{Aggression}\n&= 3.85 - 1.81 + (0.24 + 0.66)(\\text{Time}) \\\\\n&= 2.04 + 0.90(\\text{Time})\n\\end{aligned}\n\\]\n(c)\nThe interaction coefficient 0.66 means that the effect of Time on Aggression is 0.66 units larger for individuals with a family violence history compared to those without.\n(d)\nSince\n\\[\np = 0.0038\n\\]\nthere is strong evidence that the relationship between Time and Aggression differs by group, confirming that Group moderates the effect of Time on Aggression.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>More Regression Concepts</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html",
    "href": "07-Introduction-to-R.html",
    "title": "7  Introduction to R",
    "section": "",
    "text": "7.1 Some background information",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#some-background-information",
    "href": "07-Introduction-to-R.html#some-background-information",
    "title": "7  Introduction to R",
    "section": "",
    "text": "7.1.1 What is R?\nUp to this point, most of your data handling and analysis has probably been carried out in Microsoft Excel. Excel is an extremely accessible and versatile tool. It is excellent for tasks such as entering data, performing quick calculations, producing pivot tables, and creating basic charts. For many introductory tasks, particularly when datasets are relatively simple and the required analyses are straightforward, Excel is both sufficient and efficient. However, as data analysis requirements become more complex, the limitations of Excel start to appear. Large datasets can quickly slow Excel down or cause it to become unstable. Sophisticated statistical analyses can be difficult or impossible to implement in Excel without resorting to cumbersome add-ins.\nR addresses many of these limitations directly. R is an open-source (and free!) programming language and computing environment designed specifically for statistical analysis, data manipulation, and visualisation. It was developed in the mid-1990s by Ross Ihaka and Robert Gentleman at the University of Auckland, with the explicit goal of providing a free, flexible, and powerful tool for statistical computing.\nIt is important to emphasise that learning R does not mean abandoning Excel entirely. In many workplaces, Excel remains the default tool for data sharing and quick checks, while R is used for the more demanding analytical tasks. You might export results from R into Excel for colleagues to view, or import Excel files into R for more sophisticated analysis. Rather than replacing Excel, R complements it, adding depth, efficiency, and power to your analytical skill set.\n\n\n7.1.2 What is RStudio?\nWhile R is the programming language and computational engine that performs the analysis, RStudio is the interface that makes working with R more intuitive, organised, and efficient. At its simplest:\n\nR is like the operating system on your phone (e.g. iOS or Android) — it’s the engine that actually does the calculations and runs the code.\nRStudio is like the phone itself — the screen, keyboard, and interface that make using R easy and comfortable.\n\n\n\n\n\nRStudio was developed to make the process of coding in R more approachable, especially for those who may be new to programming. Without RStudio’s Integrated development environment (IDE), using R would involve writing code in a basic text editor and running it through the R console, an approach that is functional but lacks structure and ease of navigation. RStudio addresses this by combining a code editor, console, file manager, plotting tools, and help system into a single, unified workspace. This integration allows you to work more productively, keep track of your files and scripts, and view outputs such as plots or model results without switching between multiple programs.\nIn essence, RStudio provides a professional and efficient working environment for anyone using R, from beginners to experienced data scientists. It is designed to streamline your workflow, reduce the friction of managing code and outputs, and make R more accessible without sacrificing power or flexibility.\n\n\n7.1.3 Installation\nYou will need to install both R and RStudio. Depending on your operating system, the instructions will be different:\n\n7.1.3.1 Installing R (Do this first)\n\nWindowsmacOS\n\n\n\nOpen your web browser and go to the  R Projectwebsite.\nUnder the “Download and Install R” section, click on the “Download R for Windows” Link\nOn the next page, click “base” to download the base distribution of R.\nClick on the “Download R x.x.x for Windows” link (the version number will vary).\nOnce the file has been downloaded, run the .exe file (by default it will go to your ‘My Downloads’ folder and will look something like R-x.x.x-win.exe)\nClick through the installation wizard to finish installation.\n\n\n\n\nOpen your web browser and go to the  R Projectwebsite.\nUnder the “Download and Install R” section, click on the “Download R for macOS” Link\nOn the next page, click the “R-4.x.x.pkg” link (where “x.x” will be the version number) to download the R installer for macOS. This will download a .pkg file.\nAfter downloading the .pkg file, open it to start the installation process.\nFollow the on-screen instructions in the installation wizard\n\n\n\n\n\n\n7.1.3.2 Installing RStudio (Do this second)\n\nWindowsmacOS\n\n\n\nOpen your web browser and go to the  RStudio website.\nUnder the “RStudio Desktop” section, click on the Download RStudio button.\nYou will be directed to a page where you can select the version of RStudio for your operating system. Select RStudio for Windows.\nClick on the Download RStudio Desktop button to download the installer for Windows. It will download a .exe file.\nAfter downloading the .exe file, open the executable file, and follow the instructions in the installation wizard.\n\n\n\n\nOpen your web browser and go to the  RStudio website.\nScroll down to the “RStudio Desktop” section and click Download RStudio.\nOn the next page, under RStudio Desktop for macOS, click Download RStudio Desktop (this will download the .dmg file).\nAfter the .dmg file has downloaded, locate the file and double-click it to open the disk image.\nA new window will appear showing the RStudio application icon. Drag the RStudio icon into your Applications folder.\nOnce the application is copied to the Applications folder, you can close the disk image window.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#rstudio-interface-tour",
    "href": "07-Introduction-to-R.html#rstudio-interface-tour",
    "title": "7  Introduction to R",
    "section": "7.2 RStudio Interface Tour",
    "text": "7.2 RStudio Interface Tour\nWhen you first open RStudio, you will see that the window is divided into four main panes (if you only see 3 panes, go to File &gt; New File &gt; R Script). Each pane serves a distinct purpose and can be customised to suit your workflow. Understanding these panes is the first step to working efficiently in RStudio.\n\n\n7.2.1 The Console (bottom left)\nThe Console is the beating heart of RStudio—it’s where R actually executes commands. When you type something like:\n\n\n\n\n\n\nR Code\n\n\n\n\n2+2\n\n[1] 4\n\n\n\n\nand press Enter, the Console evaluates it and returns the answer (in this case “4”). Think of the Console as a direct conversation with R: you speak in R code, and R answers back.\n\nGood for quick calculations or testing out snippets of code.\nNot ideal for saving longer scripts—once you close RStudio, your Console history may be lost.\n\n\n\n7.2.2 The Source Editor (top left)\nWhile the Console is for “chatting” with R, the Source Editor is for writing full documents. This is where you type and save your scripts (.R files) or notebooks (.Rmd files).\nKey features:\n\nRun Code: You can highlight a line (or multiple lines) and press Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) to send it to the Console for execution.\nScripts are Saved: Unlike Console commands, everything in the Editor can be saved and reused. This makes it the best place for reproducible analysis.\n\n\n\n7.2.3 The Environment / History Pane (top right)\nThis pane helps you keep track of what’s happening “behind the scenes.”\n\nEnvironment tab: Shows all the objects (data frames, vectors, functions) currently stored in memory. You can think of this as your backpack, everything you’ve created or loaded in this session lives here.\nHistory tab: Lists all the commands you’ve run, even those typed directly into the Console.\n\nThis pane is invaluable for staying organized and avoiding the classic beginner question: “Wait, where did my dataset go?”\n\n\n7.2.4 The Output Pane (Bottom-Right)\nThis multipurpose pane is like a Swiss Army knife, switching roles depending on which tab you select:\n\nFiles: A file browser - lets you navigate folders and open files without leaving RStudio.\nPlots: Displays the visualisations you create with plot(), ggplot(), and other tools.\nPackages: Lists installed packages and allows you to check which are currently loaded.\nHelp: A searchable manual for R functions. Try typing ?mean in the Console - this tab will show the documentation.\nViewer: Displays rendered outputs such as interactive plots, Shiny apps, or R Markdown HTML documents.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#installing-and-loading-packages",
    "href": "07-Introduction-to-R.html#installing-and-loading-packages",
    "title": "7  Introduction to R",
    "section": "7.3 Installing and Loading Packages",
    "text": "7.3 Installing and Loading Packages\nWhen you first download and install R onto your computer, think of it like buying a brand-new smartphone. Out of the box, the phone comes with a handful of useful apps already installed: a calculator, messaging, and a web browser. Similarly, R comes with a base set of functions ready to use immediately: functions for arithmetic, working with vectors and data frames, or producing simple plots. These built-in tools are powerful, but sooner or later you’ll want to extend what your R phone can do.\nOn your smartphone, if you want to stream music, you open the App Store and download Spotify. If you want to edit photos, you might download Instagram. Each app extends the base functionality of the phone. In the world of R, these add-ons are called packages.\n\n\nImage Source: Statistical Inference via Data Science: A modern dive in R and the Tidyverse (2025)\n\n\nPackages are collections of R functions, data, and documentation bundled together to solve specific problems. For example:\n\nggplot2 adds advanced data visualization tools.\ndplyr makes data wrangling faster and easier.\nlme4 allows you to fit complex mixed-effects models.\n\nJust as there are millions of apps in the App Store, there are thousands of packages available for R, hosted on the Comprehensive R Archive Network (CRAN) and other repositories.\n\n7.3.1 Installing a Package\nInstalling a package is like downloading an app to your phone. You only need to do this once. On a smartphone, once you have downloaded Spotify, the app is stored on your device until you delete it. Likewise, when you run:\n\n\n\n\n\n\nR Code\n\n\n\n\ninstall.packages(\"ggplot2\")\n\n\n\nR connects to CRAN (the App Store equivalent) and downloads the files it needs. These files are then stored in your R library folder. You do not need to install ggplot2 again unless you remove it or want to update it.\nA useful point to emphasise to learners is that “install” is about making the tool available on your computer, not about actually starting to use it. Installing Spotify doesn’t mean music starts playing, it just means the app is on your phone.\n\n\n7.3.2 Loading a Package\nNow imagine you restart your phone. Even though Spotify is installed, it doesn’t automatically open itself every time the phone powers on. If you want to listen to music, you need to tap the Spotify icon. In R, this step is accomplished with the library() function:\n\n\n\n\n\n\nR Code\n\n\n\n\nlibrary(ggplot2)\n\n\n\nThis command tells R to load the package into the current session so its functions are available. Until you run library(ggplot2), R doesn’t “know” about the package: you can’t call ggplot() or other functions it contains. This is like your phone not be able to play music until you’ve loaded your Spotify app.\nThe key distinction:\n\ninstall.packages() = downloading the app once.\nlibrary() = opening the app each time you want to use it.\n\nThis means if you close R or restart your computer, the package is still installed but not yet loaded. You must run library(ggplot2) again in your new session, just like you would open the Spotify app each time you want to play music.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#functions-and-arguments",
    "href": "07-Introduction-to-R.html#functions-and-arguments",
    "title": "7  Introduction to R",
    "section": "7.4 Functions and Arguments",
    "text": "7.4 Functions and Arguments\nOne of the most important ideas to understand early in R is that almost everything you do involves functions. Functions are the verbs of the R language: they perform actions. You supply arguments to a function, which are the nouns - the data or instructions the function uses to do its job.\n\n7.4.1 Functions: Action Words\nIn everyday life, think of functions as commands like bake, drive, or add. Each one requires some input to make sense. You can’t just say “bake” without specifying what to bake. Similarly, in R, functions require input.\nFor example, the function sum() calculates the total of a set of numbers:\n\n\n\n\n\n\nR Code\n\n\n\n\nsum(2, 4, 6)\n\n[1] 12\n\n\n\n\nHere, sum() is the function, and the numbers 2, 4, 6 are the arguments.\n\n\n7.4.2 Arguments: The Details\nArguments are the details you give to the function so it knows exactly what to do. Some arguments are required, while others are optional.\nFor example:\n\n\n\n\n\n\nR Code\n\n\n\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\n\n\n\nThe first argument (3.14159) is the number to round.\nThe second argument (digits = 2) tells R how many decimal places to keep.\n\nIf you pass an argument that doesn’t make sense into a function, R will return an error. For example:\n\n\n\n\n\n\nR Code\n\n\n\n\nround(3.14159, digits = \"apple\")\n\nError in round(3.14159, digits = \"apple\"): non-numeric argument to mathematical function\n\n\n\n\n\n\n7.4.3 Named vs Unnamed Arguments\nArguments can be provided in two ways named or unnamed. In the example below we will use the seq function, which generates a sequence of numbers by entering in 3 arguments:\n\nwhat number we are starting from\nwhat number are we going to\nhow much are we increasing by\n\nThe first way is a named argument (by keyword). You specify the keywords within the function (i.e. from, to and by). Beginners are encouraged to use named arguments until they are confident with the order of positional arguments.\n\n\n\n\n\n\nR Code\n\n\n\n\nseq(from = 1, to = 5, by = 1)\n\n[1] 1 2 3 4 5\n\n\n\n\nThe second way is with unnamed (by position). Here, R interprets 1 as the starting number, 5 as the ending number, and 1 as the step size. You need to know the position of the arguments to use this method. In the code below, the three numbers (1,5,1) represent the arguments (from,to,by).\n\n\n\n\n\n\nR Code\n\n\n\n\nseq(1, 5, 1)\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n7.4.4 Functions inside Functions\nFunctions can also be nested, where the output of one becomes the input of another:\n\n\n\n\n\n\nR Code\n\n\n\n\nsqrt(sum(4, 5, 6))\n\n[1] 3.872983\n\n\n\n\nAnd you can keep nested arguments inside more and more functions. For example:\n\n\n\n\n\n\nR Code\n\n\n\n\nround(sqrt(sum(4, 5, 6)), digits = 2)\n\n[1] 3.87\n\n\n\n\n\n\n7.4.5 The Native Pipe Operator\nIn the previous section, we saw how functions can be nested, the output of one function becomes the input of another. R worked through those functions to receive an output:\n\nR first calculated the sum of 4, 5 and 6\nR then took the square root\nFinally, R rounded the answer to 2 decimal places.\n\nIn newer versions of R, there is something called a native pipe operator. The pipe lets you express the same logic in a step-by-step sequence, reading from left to right. Instead of wrapping one function inside another, you “pipe” the result forward.\nNote: you might need to enable this in your settings:\n\nclick on Tools &gt; Global Options &gt; Code &gt; make sure ‘Use native pipe operator’ is ticked\n\n\n\n\n\n\n\nR Code\n\n\n\n\nsum(4, 5, 6) |&gt; \n  sqrt() |&gt; \n  round(digits = 2)\n\n[1] 3.87",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#entering-data-manually",
    "href": "07-Introduction-to-R.html#entering-data-manually",
    "title": "7  Introduction to R",
    "section": "7.5 Entering Data Manually",
    "text": "7.5 Entering Data Manually\nWhen working with R, most of the time you’ll import datasets from a file (for example, a CSV or Excel sheet). However, in the early stages of learning, it’s useful to practice by entering data manually. This helps you understand how R stores information and how to create your own data frames.\nLet’s consider the dataset below, which records annual income (in thousands of dollars) and years of education for five individuals:\n\n\nIncome (per $1000)Years of Education20.01432.41440.0161.11326.013\n\n\n\n7.5.1 Creating vectors\nThe simplest way to enter this data is by creating vectors. A vector is an ordered list of values of the same type (all numbers, all characters, etc.). Here, we will create a vector for income. We can do this with the c() function, which stands for ‘combine.’\n\n\n\n\n\n\nR Code\n\n\n\n\nc(20, 32.4, 40, 1.1, 26)\n\n[1] 20.0 32.4 40.0  1.1 26.0\n\n\n\n\nThis is useful for a quick calculation, but if we wanted to save this vector (so that we can use it later) we will need to store it as an object. To do this, we need to assign it to a name using the assignment operator &lt;-. In the code below I am assigning these five numbers to an object that I will call income.\n\n\n\n\n\n\nR Code\n\n\n\n\nincome &lt;- c(20, 32.4, 40, 1.1, 26)\n\n\n\nNote that when you assign something a name and run the code, no output appears. This is because the object is now stored in your global environment (have a look at the global environment pane in the top-right). It will be stored there until we ‘call’ upon it. For example:\n\n\n\n\n\n\nR Code\n\n\n\n\nincome\n\n[1] 20.0 32.4 40.0  1.1 26.0\n\n\n\n\nLet’s do the same for the second variable as well:\n\n\n\n\n\n\nR Code\n\n\n\n\neducation &lt;- c(14, 14, 16, 13, 13)\n\n\n\nCheck to make sure you now have two vectors (income and education) stored in your environment.\n\n\n7.5.2 Creating a data frame\nA data frame is R’s way of storing tabular data, much like a spreadsheet. Each column is a vector (all of the same type: numeric, character, logical, etc.), and each row is an observation. In our example:\n\nThe income vector is one column.\nThe education vector is another column.\nTogether, they make a two-column table.\n\nWe can combine our two vectors into a data frame using the data.frame() function (assumming you have created two objects and assigned them to your global environment as income and education.)\n\n\n\n\n\n\nR Code\n\n\n\n\ndata.frame(\n  income,\n  education\n)\n\n  income education\n1   20.0        14\n2   32.4        14\n3   40.0        16\n4    1.1        13\n5   26.0        13\n\n\n\n\nNote: in the example above, we did not assign our data frame with a name, so the output is printed directly into the console. Let’s now assign it a name so that we can use it in a meaningful way.\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data &lt;- data.frame(\n  income,\n  education\n)\n\n\n\nIf we look at the Environment pane now, we should see:\n\n\n\nThis tells us the data frame has:\n\n5 observations (rows).\n2 variables (columns).\n\nClicking on the object name (income_data) in the Environment pane will open the Data Viewer in RStudio, which displays the data frame in a spreadsheet-like format.\n\n\n\n\n\n\n7.5.3 Inspecting the data frame\nNow that we’ve created our first data frame, it’s important to learn how to look inside it. When working with larger datasets, it’s easy to lose track of what variables you have, how many rows are included, and what types of data each column contains.\nR provides several functions for inspecting data, but one of the most versatile is str(), short for structure, which tells us:\n\nThe number of observations (rows).\nThe number of variables (columns).\nThe names of the variables.\nThe data type of each variable (numeric, character, factor, etc.).\nA preview of the first few values.\n\n\n\n\n\n\n\nR Code\n\n\n\n\nstr(income_data)\n\n'data.frame':   5 obs. of  2 variables:\n $ income   : num  20 32.4 40 1.1 26\n $ education: num  14 14 16 13 13",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#applying-functions-to-data-frames",
    "href": "07-Introduction-to-R.html#applying-functions-to-data-frames",
    "title": "7  Introduction to R",
    "section": "7.6 Applying functions to data frames",
    "text": "7.6 Applying functions to data frames\n\n7.6.1 Base R functions (traditional methods)\nIn base R, we directly call functions and pass columns as arguments. To access a variable inside a data frame, we use the $ operator. For example, in the section above we used str() to see that the income_data has two variables: income and education. By calling income_data$income, we’re basically saying take the income variablle from the income_data data frame. We can then pass this into different functions, for example:\n\n\n\n\n\n\nR Code\n\n\n\n\nmean(income_data$income)\n\n[1] 23.9\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nmedian(income_data$income)\n\n[1] 26\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nsd(income_data$income)\n\n[1] 14.75568\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nmin(income_data$income)\n\n[1] 1.1\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nmax(income_data$income)\n\n[1] 40\n\n\n\n\n\n\n7.6.2 Using the Native Pipe\nThe pipe operator allows us to read code from left to right, almost like a recipe: &gt; “take this data, then apply this function.”\nHere’s the same analysis from above with pipes:\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data$income |&gt; mean()\n\n[1] 23.9\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data$income |&gt; median()\n\n[1] 26\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data$income |&gt; sd()\n\n[1] 14.75568\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data$income |&gt; min()\n\n[1] 1.1\n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data$income |&gt; max()\n\n[1] 40\n\n\n\n\n\n\n7.6.3 Using packages\nSimilar to Excel’s ‘Data Analysis Tool Kit,’ R has a lot of packages that can make our lives much easier. One of the most widely used packages for data manipulation is dplyr. This package is part of the tidyverse collection and is designed to make working with data frames simple, consistent, and highly readable.\nTo use dplyr, you first need to install it (just once per computer) and then load it:\n\n\n\n\n\n\nR Code\n\n\n\n\ninstall.packages(\"dplyr\")\nlibrary(dplyr)\n\n\n\nWhile base R requires you to manually extract vectors with $ or use with(), dplyr is designed to work directly on data frames. Its functions pair naturally with the pipe |&gt;, allowing you to write code that reads almost like English.\nFor example, we could use the summarise() function to calculate the mean of income in our income_data dataset:\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  summarise(mean_income = mean(income))\n\n  mean_income\n1        23.9\n\n\n\n\nOne of the biggest advantages of dplyr is how easy it is to calculate several summaries in a single step:\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  summarise(\n    mean   = mean(income),\n    median = median(income),\n    sd     = sd(income),\n    min    = min(income),\n    max    = max(income)\n  )\n\n  mean median       sd min max\n1 23.9     26 14.75568 1.1  40\n\n\n\n\nIn the previous section we used summarise() to collapse the dataset down to single values such as the mean or median. Another approach is to use mutate(), which adds new columns to the data frame while keeping the original rows intact.\nThis is less common for summaries (since we usually don’t need the row detail), but it’s useful to see the difference.\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  mutate(\n    mean   = mean(income),\n    median = median(income)\n  )\n\n  income education mean median\n1   20.0        14 23.9     26\n2   32.4        14 23.9     26\n3   40.0        16 23.9     26\n4    1.1        13 23.9     26\n5   26.0        13 23.9     26\n\n\n\n\n\n\n7.6.4 Creating New Variables with mutate()\nWhile mutate() can be used to attach summary statistics, its real power is in creating new variables derived from existing ones. This keeps the original data intact while extending it with additional information.\nSuppose our dataset stores income in thousands of dollars, but we want a new column showing income in actual dollars.\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  mutate(income_dollars = income * 1000)\n\n  income education income_dollars\n1   20.0        14          20000\n2   32.4        14          32400\n3   40.0        16          40000\n4    1.1        13           1100\n5   26.0        13          26000\n\n\n\n\nWe can also create multiple new variables in the same mutate() call. For example:\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  mutate(\n    income_dollars = income * 1000,\n    income_per_years = income / education\n  )\n\n  income education income_dollars income_per_years\n1   20.0        14          20000       1.42857143\n2   32.4        14          32400       2.31428571\n3   40.0        16          40000       2.50000000\n4    1.1        13           1100       0.08461538\n5   26.0        13          26000       2.00000000\n\n\n\n\n\n\n7.6.5 Combining mutate() and summarise()\nOne of the strengths of the pipe workflow is that you can apply several transformations in sequence, each building on the previous step. Let’s create two new variables with mutate(), then compute summary statistics with summarise().\n\n\n\n\n\n\nR Code\n\n\n\n\nincome_data |&gt; \n  mutate(income_dollars = income * 1000) |&gt; \n  mutate(income_per_years = income_dollars / education) |&gt; \n  summarise(\n    mean = mean(income_per_years),\n    sd   = sd(income_per_years),\n  )\n\n      mean      sd\n1 1665.495 972.694\n\n\n\n\nWe can read the above pipeline in plain English as:\n\nStart with the income_data data frame, then\nCreate a new variable called income_dollars by taking income variable and multiply it by 1000, then\nCreate a new variable called income_per_years by taking the income_dollars variable and dividing by the education variable, then\nCompute the mean and standard deviation for income_per_years",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#creating-factor-variables",
    "href": "07-Introduction-to-R.html#creating-factor-variables",
    "title": "7  Introduction to R",
    "section": "7.7 Creating Factor Variables",
    "text": "7.7 Creating Factor Variables\nUp to this point, we have worked with numeric data such as income and years of education. However, in many situations, data is categorical—representing groups, categories, or yes/no outcomes rather than continuous numbers. In R, categorical variables are stored as factors. Factors are useful because they carry not just the raw labels (e.g., “Yes”, “No”), but also information about the possible categories, known as levels.\nLet’s imagine that, in addition to income and education, we collected two categorical variables for each of the same 5 participants:\n\nStatus: “Employed” or “Unemployed”\nSex: “Male” or “Female”\n\nWe can create these as factors and then attach them to our existing data frame, or to a new data frame. In the code example below, I create a new data frame called income_data2.\n\n\n\n\n\n\nR Code\n\n\n\n\n# This creates two new variables\nStatus &lt;- c(\"Employed\", \"Employed\", \"Unemployed\", \"Unemployed\", \"Employed\")\nSex &lt;- c(\"Male\", \"Female\", \"Male\", \"Female\", \"Male\")\n\n# This adds those two new variables to the current data and creates a new data frame\nincome_data2 &lt;- income_data |&gt; \n  mutate(\n    Status = Status,\n    Sex = Sex\n  )\n\n# This allows us to view the new data\nstr(income_data2)\n\n'data.frame':   5 obs. of  4 variables:\n $ income   : num  20 32.4 40 1.1 26\n $ education: num  14 14 16 13 13\n $ Status   : chr  \"Employed\" \"Employed\" \"Unemployed\" \"Unemployed\" ...\n $ Sex      : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n\n\n\n\n\n7.7.1 Exploring Factors with table()\nNow that our income_data dataset includes categorical variables (Status and Sex), we can explore how they relate to one another. One of the simplest and most useful tools for this is the table() function.\nIf we call table() on a single factor, R will show us the counts of each category. This tells us there are 2 females and 3 males in the dataset.\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(income_data2$Sex)\n\n\nFemale   Male \n     2      3 \n\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(income_data2$Status)\n\n\n  Employed Unemployed \n         3          2 \n\n\n\n\nIf we supply two factors, table() produces a contingency table: a cross-tabulation showing how the categories overlap.\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(income_data2$Status, income_data2$Sex)\n\n            \n             Female Male\n  Employed        1    2\n  Unemployed      1    1\n\n\n\n\n\n\n7.7.2 From Counts to Probabilities\nThe table() function gives raw counts, but we can easily convert them into relative frequencies (probabilities) using prop.table().\n\n\n\n\n\n\nR Code\n\n\n\n\nprop.table(table(income_data2$Status, income_data2$Sex))\n\n            \n             Female Male\n  Employed      0.2  0.4\n  Unemployed    0.2  0.2\n\n\n\n\nHere:\n\nThe probability of randomly selecting an employed male is 0.4 (40%).\nThe probability of selecting an unemployed female is 0.2 (20%).\nAll cells sum to 1, as probabilities should.\n\n\n\n7.7.3 Conditional Probabilities\nWe can also calculate conditional probabilities, for example, the probability of being employed given someone is male, by including the margin argument:\n\nmargin = 1 → proportions within rows.\nmargin = 2 → proportions within columns.\n\n\n\n\n\n\n\nR Code\n\n\n\n\nprop.table(table(income_data2$Status, income_data2$Sex), margin = 1)\n\n            \n                Female      Male\n  Employed   0.3333333 0.6666667\n  Unemployed 0.5000000 0.5000000\n\n\n\nAmong employed, 33% are female and 67% male.\nAmong unemployed, 50% are female and 50% male.\n\n\n\n\n\n\n\n\n\nR Code\n\n\n\n\nprop.table(table(income_data2$Status, income_data2$Sex), margin = 2)\n\n            \n                Female      Male\n  Employed   0.5000000 0.6666667\n  Unemployed 0.5000000 0.3333333\n\n\n\nAmong females, 50% are employed and 50% unemployed.\nAmong males, 67% are employed and 33% unemployed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#visualising-data-with-ggplot2",
    "href": "07-Introduction-to-R.html#visualising-data-with-ggplot2",
    "title": "7  Introduction to R",
    "section": "7.8 Visualising Data with ggplot2",
    "text": "7.8 Visualising Data with ggplot2\nOne of the most effective ways to understand your data is through visualisation. While functions such as summary() or table() provide numerical insights, a well-chosen plot can make relationships and patterns instantly clear. In R, the most widely used package for data visualisation is ggplot2, which is part of the tidyverse. It implements the “grammar of graphics,” where plots are constructed by layering components: you start with a dataset, map variables to visual aesthetics such as axes or colours, and then add geometric layers (geoms) such as points, boxes, or bars.\n\n\n\n\nThe general structure of a ggplot2 command is:\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;AESTHETICS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;other layers&gt;\n\nHere,  is your data frame,  define how variables are represented visually (e.g., x-axis, y-axis, colour, fill), and  specifies the type of plot. Additional layers such as labels, scales, or themes can then be added with the + operator.\n\n\nLet’s use our income_data dataset, which now contains numeric variables (income and education) as well as categorical variables (Status and Sex). We will explore three different plots: a scatterplot, a boxplot, and a stacked bar chart.\n\n7.8.1 Scatterplots\nScatterplots are ideal for showing the relationship between two numeric variables. Here, we plot education on the x-axis and income on the y-axis.\n\n\n\n\n\n\nR Code\n\n\n\n\nlibrary(ggplot2)\n\nggplot(data = income_data2,\n       aes(x = education, y = income)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nWe can add a layer to our current code, using the + operator, to add new elements. For example:\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = income_data2,\n       aes(x = education, y = income)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\n\n\n\n\n7.8.2 Boxplot\nTo compare a numeric variable across categories, a boxplot is more effective. Here, we compare income between males and females. Note here that I added in a few extra layers.\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = income_data2,\n       aes(x = Sex, y = income)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nWithin each geom function, we can add additional arguments. For example, suppose we wanted to fill the box plots with colors. In this case we can:\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = income_data2,\n       aes(x = Sex, y = income)) +\n  geom_boxplot(aes(fill = Sex))\n\n\n\n\n\n\n\n\n\n\n\n\n7.8.3 Bar charts\nHere’s a few examples of bar charts. Can you try to break the code into their different layers and explain them in plain English?\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = income_data2,\n       aes(x = Sex, fill = Status)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nLets create another version of this plot that rescales the bars to 100%, making it easy to compare proportions across groups. For example, you can directly interpret 𝑃(Employed ∣ Male) versus 𝑃(Employed ∣ Female).\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = income_data2,\n       aes(x = Sex, fill = Status)) +\n  geom_bar(position = 'fill')",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#importing-data-into-r",
    "href": "07-Introduction-to-R.html#importing-data-into-r",
    "title": "7  Introduction to R",
    "section": "7.9 Importing Data into R",
    "text": "7.9 Importing Data into R\nSo far, we have created small datasets by typing values directly into R. This is excellent for learning, but in practice most datasets come from external files - for example, CSV spreadsheets, Excel workbooks, or databases. Being able to import data into R is the essential first step in any real analysis. The most common format for data exchange is the CSV file (comma-separated values). These are plain text files where each row is an observation and each column is separated by commas. They are lightweight, portable, and can be opened in Excel, Google Sheets, or even a text editor.\n\n7.9.1 Working Directory\nBefore we learn how to import data into R, it’s important to understand the idea of the working directory. The working directory is the “default folder” where R looks for files to read and where it saves files you write.\nYou can see your current working directory with the getwd() function. For example, in creating this ETC1000 eBook, I set my working directory to be:\n\n\n\n\n\n\nR Code\n\n\n\n\ngetwd()\n\n[1] \"C:/Users/mhuy0019/OneDrive - Monash University/Teaching/ETC1000/ETC1000 Quarto Book 2026\"\n\n\nThis tells me that my working directory for this project is. By default R usually sets your working directory as your Documents folder.\n\n\n\n\n7.9.2 Using RStudio Projects\nA more reliable and professional way to manage working directories is to use RStudio Projects. When you create a new Project in RStudio, the project folder automatically becomes your working directory whenever the project is opened. This means you don’t have to keep changing directories manually or worrying about paths breaking when you share your code with others.\nTo create a project:\n\nGo to File → New Project.\nChoose New Directory\nChoose New Project\nGive it a directory name and choose where this folder will be placed.\n\nLet’s create a Folder (click on the + Folder button in this pane) and call it “Data.” This creates a folder directly in the working directory. We can see it in the RStudio, but also if you navigated to this location on your device you would see it as well.\n\n\n7.9.3 9.3 Moving files to your Project Folders\nDownload the “Lecture7_data.csv” file using the link below.\n\n\n\n Lecture7_data\n\n\n\n\nAfterwards, locate this file (probably in your Downloads folder) and copy it into the newly created “Data” folder in your R project.\nWe can use the read.csv() function to import the data into our environment.\nMinh will demonstrate (during the lecture) how to use this data to replicate some of the tasks we have already completed thus far (and a few new ones). If you would like to try it yourself before the lecture, the tasks are:\n\nInspect the data (using the str function)\nObtaining descriptive statistics for Job.Satisfaction (mean, median, sd, min, max)\nCreating a new variable, called Z, that divides Job.Satisfaction by Years.at.company\nFinding the correlation (using the cor function) between Income and Years.at.company\nBuilding a linear model (using the lm function) for Z based on Income\nCreating plots\n\nScatter plot (Z and Income)\nBoxplot (Z and Position)\n\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# Load the Data\ndata &lt;- read.csv(\"Lecture7_data.csv\")\n\n\n# Descriptive Statistics\ndata |&gt; \n  summarise(\n    mean = mean(Job.Satisfaction),\n    median = median(Job.Satisfaction),\n    sd = sd(Job.Satisfaction),\n    min = min(Job.Satisfaction),\n    max = max(Job.Satisfaction)\n  )\n\n      mean median       sd  min  max\n1 15.85667   15.8 3.064894 10.1 20.4\n\n\n\n# Create new variable\ndata2 &lt;- \n  data |&gt; \n  mutate(Z = Job.Satisfaction / Years.at.company)\n\n\n# Correlation\ndata2 |&gt; \n  summarise(correlation = cor(Income, Years.at.company))\n\n  correlation\n1    0.485877\n\n\n\n# Linear Regression\nsatisfaction_model &lt;- \n  lm(\n  formula = Z ~ Income,\n  data = data2\n)\n\ncoef(satisfaction_model) |&gt; round(5) # Z = 7.64370 - 0.00005(Income)\n\n(Intercept)      Income \n    7.64370    -0.00005 \n\n\n\n# Create scatterplot\ndata2 |&gt; \n  ggplot(aes(x = Income, y = Z)) + \n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  scale_x_continuous(\n    limits = c(40000,120000),\n    breaks = seq(40000, 120000, by = 20000))\n\n\n\n\n\n\n\n\n\n# Optional (and not recommend without a valid reason): \n# See what happens if we filter out the outlier\ndata2 |&gt; \n  filter(Z &lt; 10) |&gt; \n  ggplot(aes(x = Income, y = Z)) + \n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  scale_x_continuous(\n    limits = c(40000,120000),\n    breaks = seq(40000, 120000, by = 20000))\n\n\n\n\n\n\n\n\n\n# Create boxplot\ndata2 |&gt; \n  ggplot(aes(x = Position, y = Z)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#summary",
    "href": "07-Introduction-to-R.html#summary",
    "title": "7  Introduction to R",
    "section": "7.10 Summary",
    "text": "7.10 Summary\nThis chapter introduced R and RStudio as powerful, modern tools for data analysis that complement, rather than replace, Excel. You learned what R is, why it is widely used for statistical computing, and how RStudio provides a structured and user-friendly environment for writing, running, and managing R code. The chapter guided you through installing both R and RStudio, navigating the RStudio interface, and understanding the roles of its key panes.\nYou were introduced to core programming ideas in R, including functions, arguments, errors, and the use of the native pipe operator to write clear, step-by-step code. You learned how to enter data manually, create vectors and data frames, inspect data structures, and apply functions to variables using both base R and tidyverse tools such as dplyr. The chapter also covered creating new variables, summarising data, and working with categorical variables (factors), including contingency tables and probabilities.\nFinally, you explored data visualisation with ggplot2, learning how to build scatterplots, boxplots, and bar charts using the grammar of graphics. The chapter concluded by introducing good data-management practices, including working directories, RStudio Projects, and importing data from external files such as CSVs. Together, these skills form the foundation for reproducible, efficient, and transparent data analysis in the chapters that follow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "07-Introduction-to-R.html#exercises",
    "href": "07-Introduction-to-R.html#exercises",
    "title": "7  Introduction to R",
    "section": "7.11 Exercises",
    "text": "7.11 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nCreate a new Project called “Wage”. Download the data file below and place it in your newly created project.\n\n\n\n Wage\n\n\n\n\nNow, write some code to complete the following tasks:\n\nLoad the data into your global environment and save it as Wage.\nLoad the dplyr package.\nUsing a function determine how many columns there are in this data set.\nDetermine the mean Wage in this data\nCreate a new variable called Wage_Adj which divides wage by age, and fave this as a new data frame called Wage2.\nDetermine the mean, median, sd, min and max for your Wage_Adj variable\nCreate a new variable called Age_Cat the categorises people into either younger than 40 or 40 and older, and save this as a new data frame called Wage3.\nGroup your Wage3 data by Age_Cat and determine which group has the larger mean wage.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nWage &lt;- read.csv(\"Wage.csv\")\n\n\n# b.\nlibrary(dplyr)\n\n\n# c.\nstr(Wage)\n\n'data.frame':   200 obs. of  3 variables:\n $ wage     : num  109 125 114 121 121 ...\n $ age      : int  33 55 38 60 62 22 44 60 45 41 ...\n $ education: num  6.5 11.1 8.1 10.5 9.5 6.5 7.7 7.9 8.6 10.8 ...\n\n\n\n# d.\nWage |&gt; summarise(mean = mean(wage))\n\n      mean\n1 122.1763\n\n\n\n# e.\nWage2 &lt;- Wage |&gt; mutate(Wage_Adj = wage/age)\n\n\n# f.\nWage2 |&gt; \n  summarise(\n    mean = mean(Wage_Adj),\n    median = median(Wage_Adj),\n    sd = sd(Wage_Adj),\n    Min = min(Wage_Adj),\n    Max = max(Wage_Adj)\n  )\n\n      mean   median        sd      Min      Max\n1 3.099205 2.912675 0.9189401 1.905079 6.578845\n\n\n\n# g.\nWage3 &lt;- Wage2 |&gt; \n  mutate(Age_Cat = ifelse(age &lt; 40, \"younger than 40\", \"40 and over\"))\n\n\n\n# A tibble: 2 × 2\n  Age_Cat          mean\n  &lt;chr&gt;           &lt;dbl&gt;\n1 40 and over      125.\n2 younger than 40  118.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCreate a new Project called “Titanic”. Download the data file below and place it in your newly created project.\n\n\n\n Titanic\n\n\n\n\nNow, write some code to complete the following tasks:\n\nLoad the data into your global environment and save it as Titanic.\nThe variable Survived is a binary variable where 0 = not survived and 1 = survived. Get the counts for this variable.\nCreate a crosstab of Survived by Sex.\nObtain the probabilities of survival given sex.\nThe third variable Pclass is a categorical variable where 1 = 1st class ticket, 2 = 2nd class ticket and 3 = 3rd class ticket. Filter your data to only include 1st class passengers and save this as Titanic_1st.\nObtain the probabilities of survival given sex, for 1st class passengers only.\nRepeat the steps above for 3rd class passengers.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nTitanic &lt;- read.csv(\"Titanic.csv\")\n\n\n# b.\ntable(Titanic$Survived)\n\n\n  0   1 \n549 342 \n\n\n\n# c.\ntable(Titanic$Survived, Titanic$Sex)\n\n   \n    female male\n  0     81  468\n  1    233  109\n\n\n\n# d.\nprop.table(table(Titanic$Survived, Titanic$Sex), margin = 1)\n\n   \n       female      male\n  0 0.1475410 0.8524590\n  1 0.6812865 0.3187135\n\n\n\n# e.\nTitanic_1st &lt;- \n  Titanic |&gt; \n  filter(Pclass == 1)\n\n\n# f.\nprop.table(table(Titanic_1st$Survived, Titanic_1st$Sex), margin = 1)\n\n   \n       female      male\n  0 0.0375000 0.9625000\n  1 0.6691176 0.3308824\n\n\n\n# g.\nTitanic_3rd &lt;- \n  Titanic |&gt; \n  filter(Pclass == 3)\n\nprop.table(table(Titanic_3rd$Survived, Titanic_3rd$Sex), margin = 1)\n\n   \n       female      male\n  0 0.1935484 0.8064516\n  1 0.6050420 0.3949580\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nCreate a new Project called ‘Tips’. Download the file below and place it in your newly created project.\n\n\n\n Tips\n\n\n\n\n\nWhich Day of the week is Tip.Percentage the highest?\nFor which Meal type is Tip.Percentage the highest?\nWhat is the correlation between Bill and Tip?\nFilter the data to only show orders when Alcohol was purchased. What is the correlation between Bill and Tip now?\nChange the filter to only show orders for when Alcohol was not purchased. For which Day is the correlation between Bill and Tip the highest?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# Load and save data\nTips &lt;- read.csv(\"Tips\")\n\n\n# a.\nTips |&gt; \n  group_by(Day) |&gt; \n  summarise(mean = mean(Tip.Percentage)) |&gt; \n  arrange(mean)\n\n# A tibble: 7 × 2\n  Day        mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Wednesday 0.149\n2 Monday    0.152\n3 Sunday    0.162\n4 Thursday  0.171\n5 Friday    0.174\n6 Saturday  0.180\n7 Tuesday   0.181\n\n\n\n# b.\nTips |&gt; \n  group_by(Meal) |&gt; \n  summarise(mean = mean(Tip.Percentage)) |&gt; \n  arrange(mean)\n\n# A tibble: 3 × 2\n  Meal        mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 Dinner     0.165\n2 Lunch      0.168\n3 Late Night 0.211\n\n\n\n# c.\ncor(Tips$Bill, Tips$Tip)\n\n[1] 0.86795\n\n\n\n# d.\nTips2 &lt;- \n  Tips |&gt; \n  filter(Alcohol == 'Yes')\n\ncor(Tips2$Bill, Tips2$Tip)\n\n[1] 0.8598178\n\n\n\n# e.\nTips |&gt; \n  filter(Alcohol == 'No') |&gt; \n  group_by(Day) |&gt; \n  summarise(Correlation = cor(Bill, Tip)) |&gt; \n  arrange(Correlation)\n\n# A tibble: 7 × 2\n  Day       Correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Friday          0.806\n2 Saturday        0.856\n3 Sunday          0.856\n4 Thursday        0.863\n5 Tuesday         0.878\n6 Monday          0.947\n7 Wednesday       0.973\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nDuring the 1970s, Harris Trust faced legal action over alleged gender-based pay discrimination. To investigate this issue, the organisation analysed the starting salaries of skilled, entry-level clerical employees hired between 1965 and 1977. For each employee, the following information (available in banksalary.csv below) recorded:\n\nbsal : Beginning salary (annual salary at time of hire)\nsal77 : Annual salary in 1977\nsex : MALE or FEMALE\nsenior : Months since hired\nage : Age in months\neduc : Years of education\nexper : Months of prior work experience\n\n\n\n\n banksalary\n\n\n\n\nCreate a new Project called ‘Bank_salary’ and place this data file into the project folder.\n\nCreate a boxplot showing bsal (salary in 1965) between males and females. Which sex has a higher salary on average?\nCreate a boxplot showing sal77 (salary in 1977) between males and females. Which sex has a higher salary on average?\nCreate a new variable called difference that subtracts bsal from sal77, and create a boxplot to show this new variable between males and females. Has the sex difference changed over time?\nWhich numeric variable (age, educ or exper) has the strongest correlation with bsal?\nExplore and comment on the correlation between bsal and educ across sex.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# Load and save data\nbanksalary &lt;- read.csv(\"banksalary.csv\")\n\n\n# a.\nbanksalary |&gt; \n  ggplot(aes(sex, bsal)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# b. \nbanksalary |&gt; \n  ggplot(aes(sex, sal77)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# c.\nbanksalary |&gt; \n  mutate(difference = sal77 - bsal) |&gt; \n  ggplot(aes(sex, difference)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# d.\nbanksalary |&gt; \n  summarise(\n    age_cor = cor(bsal, age),\n    educ_cor = cor(bsal, educ),\n    exper_cor = cor(bsal, exper)\n  )\n\n     age_cor  educ_cor exper_cor\n1 0.03389932 0.4119852 0.1667405\n\n\n\n# e.\nbanksalary |&gt; \n  group_by(sex) |&gt; \n  summarise(educ_cor = cor(bsal, educ))\n\n# A tibble: 2 × 2\n  sex    educ_cor\n  &lt;chr&gt;     &lt;dbl&gt;\n1 FEMALE    0.266\n2 MALE      0.367",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html",
    "href": "08-The-tidyverse.html",
    "title": "8  The tidyverse",
    "section": "",
    "text": "8.1 Introduction\nLast time, we explored the foundations of R: objects, vectors, data frames, base R functions, and some simple visualisations. These skills are essential, but the way we wrote code was often verbose. For example, we used $ to extract columns from data frames and had to write the following just to get an average:\nmean(income_data$income)\nAs analyses become more complex, this style can quickly become hard to read and maintain.\nTo address this, the R community developed the tidyverse: a collection of packages that work together to make data analysis more consistent, readable, and powerful. The tidyverse is like a toolkit where each tool is designed to fit with the others. We’ve already seen one of the packages from the tidyverse (dplyr).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#what-is-the-tidyverse",
    "href": "08-The-tidyverse.html#what-is-the-tidyverse",
    "title": "8  The tidyverse",
    "section": "8.2 What is the Tidyverse?",
    "text": "8.2 What is the Tidyverse?\nThe tidyverse is a family of R packages that share a common philosophy:\n\nData is stored in tibbles (modern data frames).\nFunctions follow predictable naming conventions.\nThe pipe (|&gt;) allows code to be written step by step.\nPackages interoperate seamlessly: what you do in dplyr can be visualised in ggplot2 or reshaped with tidyr.\n\nThe core tidyverse includes:\n\nggplot2 – data visualisation.\ndplyr – data manipulation (selecting, filtering, summarising).\ntidyr – reshaping data.\nreadr – importing CSV files.\npurrr – functional programming (working with lists).\nstringr – string manipulation. forcats – working with factors.\n\nWe install the tidyverse once, then load it whenever we want to use it:\n\ninstall.packages(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#the-grammar-of-data-manipulation",
    "href": "08-The-tidyverse.html#the-grammar-of-data-manipulation",
    "title": "8  The tidyverse",
    "section": "8.3 The Grammar of Data Manipulation",
    "text": "8.3 The Grammar of Data Manipulation\nThe tidyverse, especially through the dplyr package, gives us a set of verbs that form the grammar of data manipulation. These include (but not limited to):\n\nselect() – choose columns.\nfilter() – choose rows.\narrange() – reorder rows.\nmutate() – create new variables.\nsummarise() (with group_by()) – collapse data into summaries.\n\nThese verbs are simple but powerful: by chaining them together with the pipe, we can express very complex data operations clearly. Let’s have a look at an example using the data set diamonds (which comes with tidyverse).\nTo begin, we’ll create a new project where we can store our script files, data and output.\n\nI’ll create my project folder in “My Documents”, however feel free to place it wherever you like.\nI’ll call this project “Week 8 – diamonds analysis”.\n\nLet’s now load the diamonds data set into our global environment. Because it’s a package that comes with the ggplot2 package, we’ll begin by loading the ggplot2 package. Or better yet, we’ll load the tidyverse package, which will load ggplot2 (among other packages) as well! I can then use the ? operator to learn more about this data set.\n\nlibrary(tidyverse)\n?diamonds\n\n\n\n\nFrom the information above, we can see that the data set contains the price of over 50,000 round cut diamonds! For each diamond, the most important variable might be the price, but we also have useful information on other variables that we can use to try and explain price.\nLet’s begin with a simple task. Suppose I wanted to only look at the diamonds with the best clarity. With these, I would like to determine what cut and colorhas the highest costing diamonds, on average. This can easily be achieved by piping together several dplyr functions:\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the diamonds data set\ndiamonds |&gt; \n  \n  # Look in the Column ‘clarity’ and only include those rows where the value is\n  # ‘IF’ (which stands for the best clarity apparently)\n  filter(clarity == \"IF\") |&gt;\n  \n  # Group the remaining rows by the combination of cut and color\n  group_by(cut, color) |&gt;\n  \n  # For each group, calculate: n: the number of diamonds, and M_price: the mean\n  # price of the diamonds based upon their groupings from the step above\n  summarise(\n    n = n(),\n    M_price = mean(price)) |&gt;\n  \n  # Sort the groups so that those with the highest prices appear first.\n  arrange(desc(M_price))\n\n# A tibble: 31 × 4\n# Groups:   cut [5]\n   cut       color     n M_price\n   &lt;ord&gt;     &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Very Good D        23  10298.\n 2 Good      D         9  10030.\n 3 Premium   D        10   9056.\n 4 Premium   J        12   7026 \n 5 Ideal     D        28   6567.\n 6 Good      H         4   5949.\n 7 Very Good F        67   4677.\n 8 Premium   E        27   4525.\n 9 Very Good E        43   4333.\n10 Very Good I        19   4094.\n# ℹ 21 more rows\n\n\n\n\n\nFrom this output, we can see that the most expensive diamonds typically have “very good cuts” and are “D color.”\nAlthough it seems like there’s not too much difference in price between the other two cuts when the color is “D”.\nHowever, is we start to work our way down the list, we can see more variability.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#ggplot2",
    "href": "08-The-tidyverse.html#ggplot2",
    "title": "8  The tidyverse",
    "section": "8.4 ggplot2",
    "text": "8.4 ggplot2\nThe table in the previous section was a good start, but let’s start making some visualisations. After all, a picture is worth 1000 words! Last week, we already learnt about the basics behind ggplot2. As a quick refresher, think of ggplot2 as a function that adds layers upon layers to create plots.\n\n\n\nThe general structure of a ggplot2 command is:\n\n\n\n\n\n\nR Code\n\n\n\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;AESTHETICS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;other layers&gt;\n\nHere,  is your data frame,  define how variables are represented visually (e.g., x-axis, y-axis, colour, fill), and  specifies the type of plot. Additional layers such as labels, scales, or themes can then be added with the + operator.\n\n\n\n8.4.1 One variable example (numeric)\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the initial layer (the data)\ndiamonds |&gt; \n  \n  # Add an aesthetics layer (the variable)\n  ggplot(aes(x = price)) +\n  \n  # Add a layer for the geometry function\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.2 One variable example (categorical)\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the initial layer (the data)\ndiamonds |&gt; \n  \n  # Add an aesthetics layer (the variable)\n  ggplot(aes(x = cut)) +\n  \n  # Add a layer for the geometry function\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.3 Two variable examples\n\n8.4.3.1 Boxplot\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the initial layer (the data)\ndiamonds |&gt; \n  \n  # Add an aesthetics layer (the variable)\n  ggplot(aes(x = cut, y = price)) +\n  \n  # Add a layer for the geometry function\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.3.2 Scatterplot\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the initial layer (the data)\ndiamonds |&gt; \n  \n  # Add an aesthetics layer (the variable)\n  ggplot(aes(x = carat, y = price)) +\n  \n  # Add a layer for the geometry function\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.4 Three variable example\n\n\n\n\n\n\nR Code\n\n\n\n\n# Start with the initial layer (the data)\ndiamonds |&gt; \n  \n  # Add an aesthetics layer (the variable)\n  ggplot(aes(x = carat, y = price)) +\n  \n  # Add a layer for the geometry function\n  geom_point(aes(color = cut)) # cut as a new variable\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.5 A more complicated example\n\n\n\n\n\n\nNote\n\n\n\nNote: You will not be expected to know how to code up something this complicated in this unit. The figure below is just an example of the types of visualisations you can generate when you become proficient with R.\n\n8.4.6 R Code\n\nlibrary(ggExtra)\ntheme_set(theme_minimal())\n\np &lt;- diamonds %&gt;% \n  ggplot(aes(x = carat, y = price, color = color, fill = color)) + \n  geom_point(alpha = .5, shape = \".\") +\n  \n  # color scale\n  scale_fill_viridis_d(option = \"A\", direction = -1) +\n  scale_color_viridis_d(option = \"A\", direction = -1) +\n  theme(legend.position = c(.85, .4)) +\n  guides(fill = guide_legend(\n    override.aes = list(\n      size = 5, alpha = 1, color = \"black\", shape = 21))) +\n  \n  # log 2 transformation\n  scale_x_continuous(trans = \"log2\") +\n  \n  # breaks are original value before log transformation\n  scale_y_continuous(trans = \"log2\", breaks = 2^c(9:14)) +  \n  annotation_logticks(base = 2) +\n  \n  # regression calculated upon data after transformation\n  geom_smooth(\n    method = \"lm\", se = F, linewidth = .5, show.legend = F) + \n  \n  # zoom in\n  # the limit are original values before the log transformation\n  coord_cartesian(xlim = c(.25, 3), ylim = c(400, 2^14))\n\nggMarginal(p, margins = \"y\", groupColour = T)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#more-useful-functions",
    "href": "08-The-tidyverse.html#more-useful-functions",
    "title": "8  The tidyverse",
    "section": "8.5 More useful functions",
    "text": "8.5 More useful functions\n\n8.5.1 subset() and filter()\nWhen working with data frames, we often want to keep only the rows that satisfy certain conditions. For example, maybe we only want to look at diamonds that are “Ideal” cut, or perhaps we only want diamonds that cost less than $1,000.\nThere are two main ways to achieve this in R:\n\nthe older base R function subset()\nthe modern tidyverse function filter()\n\n\nsubset()filter()\n\n\nPlain English: Start with the diamonds data set, and return only the rows where the cut is “Ideal” and the price is less than 1000.\n\nsubset(diamonds, \n       cut == \"Ideal\" & price &lt; 1000)\n\n# A tibble: 6,838 × 10\n   carat cut   color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.23 Ideal J     VS1      62.8    56   340  3.93  3.9   2.46\n 3  0.31 Ideal J     SI2      62.2    54   344  4.35  4.37  2.71\n 4  0.3  Ideal I     SI2      62      54   348  4.31  4.34  2.68\n 5  0.33 Ideal I     SI2      61.8    55   403  4.49  4.51  2.78\n 6  0.33 Ideal I     SI2      61.2    56   403  4.49  4.5   2.75\n 7  0.33 Ideal J     SI1      61.1    56   403  4.49  4.55  2.76\n 8  0.23 Ideal G     VS1      61.9    54   404  3.93  3.95  2.44\n 9  0.32 Ideal I     SI1      60.9    55   404  4.45  4.48  2.72\n10  0.3  Ideal I     SI2      61      59   405  4.3   4.33  2.63\n# ℹ 6,828 more rows\n\n\n\n\nPlain English: Start with the diamonds data set, then filter so that only rows remain where cut is “Ideal” and price is less than 1000.\n\ndiamonds |&gt;\n  filter(cut == \"Ideal\", price &lt; 1000)\n\n# A tibble: 6,838 × 10\n   carat cut   color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.23 Ideal J     VS1      62.8    56   340  3.93  3.9   2.46\n 3  0.31 Ideal J     SI2      62.2    54   344  4.35  4.37  2.71\n 4  0.3  Ideal I     SI2      62      54   348  4.31  4.34  2.68\n 5  0.33 Ideal I     SI2      61.8    55   403  4.49  4.51  2.78\n 6  0.33 Ideal I     SI2      61.2    56   403  4.49  4.5   2.75\n 7  0.33 Ideal J     SI1      61.1    56   403  4.49  4.55  2.76\n 8  0.23 Ideal G     VS1      61.9    54   404  3.93  3.95  2.44\n 9  0.32 Ideal I     SI1      60.9    55   404  4.45  4.48  2.72\n10  0.3  Ideal I     SI2      61      59   405  4.3   4.33  2.63\n# ℹ 6,828 more rows\n\n\n\n\n\nFrom the example above, we can see that both methods will provide us with the same output. However, in modern coding, we prefer to use filter() over subset. This is because filter():\n\nIt integrates seamlessly with other tidyverse verbs (mutate(), summarise(), etc.).\nIt reads more clearly when chained with pipes.\nIt avoids some of the quirks of subset() (e.g., issues with variable scoping).\n\n\n\n8.5.2 taaply() and group_by() -&gt; summarise()\nSometimes we don’t just want to filter rows — we want to calculate summary statistics within groups. For example, what is the average price of diamonds for each cut category? There are two common ways to do this:\n\nthe older base R function tapply()\nthe modern tidyverse approach using group_by() and summarise()\n\n\ntapply()group_by() and summarise()\n\n\nPlain English: Take the price column from the diamonds dataset, split it by the categories of cut, and compute the mean for each category. The result is a named vector showing the average price for each type of cut.\n\ntapply(diamonds$price, \n       diamonds$cut, \n       mean)\n\n     Fair      Good Very Good   Premium     Ideal \n 4358.758  3928.864  3981.760  4584.258  3457.542 \n\n\n\n\nPlain English: Start with the diamonds dataset, then:\n\nGroup the rows by the variable cut.\nWithin each group, summarise by calculating the mean of price.\n\n\ndiamonds |&gt;\n  group_by(cut) |&gt;\n  summarise(mprice = mean(price))\n\n# A tibble: 5 × 2\n  cut       mprice\n  &lt;ord&gt;      &lt;dbl&gt;\n1 Fair       4359.\n2 Good       3929.\n3 Very Good  3982.\n4 Premium    4584.\n5 Ideal      3458.\n\n\n\n\n\nWhy prefer group_by() and summarise()?\n\nIt produces a data frame rather than a named vector, which makes it easier to continue working with the result.\nIt allows you to calculate multiple summaries at once (e.g., mean, median, standard deviation).\nIt integrates seamlessly with other dplyr verbs (filter(), mutate(), arrange()).\n\n\n\n8.5.3 ifelse() and case_when()\nOften, we want to create a new variable based on conditions. For example, suppose we want to classify diamonds as either “Expensive” if their price is above $10,000, or “Affordable” otherwise.\nThere are two common ways to do this:\n\nthe older base R function ifelse()\nthe modern tidyverse function case_when()\n\n\nifelse()case_when()()\n\n\nPlain English: Create a new column called category. If price &gt; 10000, assign “Expensive”. Otherwise, assign “Affordable”.\n\ndiamonds |&gt; \n  mutate(category = ifelse(price &gt; 10000,\n                           \"Expensive\",\n                           \"Affordable\"))\n\n\n\nPlain English: Start with the diamonds data set, then create a new column called category:\n\nIf price &gt; 10000, label it “Expensive”.\nElse if price &gt; 5000, label it “Moderate”.\nOtherwise, label it “Affordable”.\n\n\ndiamonds |&gt;\n  mutate(category = case_when(\n    price &gt; 10000 ~ \"Expensive\",\n    price &gt; 5000  ~ \"Moderate\",\n    TRUE          ~ \"Affordable\"\n  ))\n\n\n\n\nWhy prefer case_when()?\n\nHandles multiple conditions clearly (no messy nested ifelse() calls).\nReads like a set of rules or a decision tree.\nWorks seamlessly inside tidyverse pipelines.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#linear-regression-in-r",
    "href": "08-The-tidyverse.html#linear-regression-in-r",
    "title": "8  The tidyverse",
    "section": "8.6 Linear Regression in R",
    "text": "8.6 Linear Regression in R\nLet’s now build several linear regression models with the diamonds data set\n\n8.6.1 Simple Linear Regression\nLet’s start with the simplest form: predicting diamond price using just one variable. A natural candidate is carat, since we expect larger diamonds to be more expensive. In a simple linear regression, the model is written as:\n\\[\\text{price}=\\beta_0+\\beta_1\\text{carat}+\\epsilon\\]\nHere, β0 is the intercept, β1 is the slope (the expected change in price for each one-unit increase in carat), and ϵ captures the error (the variation in price not explained by carat). By fitting this model, we will see whether carat size alone is a good predictor of price, and how well a straight line captures the relationship.\nIn R, we can use the lm() function to build a linear model. This function has two required arguments:\n\n‘formula’ (how you building the model)\n‘data’ (the data set)\n\nUsing our diamonds data set, let’s fit a model for price using only carat as a variable\n\n\n\n\n\n\nR Code\n\n\n\n\nlm(\n  formula = price ~ carat,\n  data = diamonds\n)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7756  \n\n\n\n\nWhen you run the lm() function (as we have above), it only outputs the beta coefficients. Here, we can use these to construct the model as\n\\[\\text{price}=-2256+(7756\\times\\text{carat})+\\epsilon\\]\nIf we wanted more information on the model, we should begin by saving the model as an object. Then we can use other functions, such as summary() to learn more about our results.\n\n\n\n\n\n\nR Code\n\n\n\n\nmodel1 &lt;- \n  lm(\n  formula = price ~ carat,\n  data = diamonds\n)\n\nsummary(model1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2256.36      13.06  -172.8   &lt;2e-16 ***\ncarat        7756.43      14.07   551.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nThe summary() function transforms our regression results into a table that’s quite similar to the one we saw when we ran these in Excel (with Standard Errors, t values and p values.) Below the table we also have fit metrics such as R squared.\n\n\n8.6.2 Multiple linear regression with numeric predictors\nDiamonds are more than just their weight. Features such as depth (the height of the diamond relative to its width) and table (the size of the flat top facet) can also influence price. In this section, we extend the model to include several numeric predictors:\n\\[\\text{price}=\\beta_0+\\beta_1\\text{carat}+\\beta_2\\text{depth}+\\beta_3\\text{table}+\\epsilon\\]\nWith multiple predictors, the model can capture more of the variation in diamond prices. However, we must be careful as adding variables can increase complexity without necessarily improving predictive power. This is where interpretation and statistical judgment come in.\n\n\n\n\n\n\nR Code\n\n\n\n\nmodel2 &lt;- \n  lm(\n  formula = price ~ carat + depth + table,\n  data = diamonds\n)\n\nsummary(model2)\n\n\nCall:\nlm(formula = price ~ carat + depth + table, data = diamonds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18288.0   -785.9    -33.2    527.2  12486.7 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13003.441    390.918   33.26   &lt;2e-16 ***\ncarat        7858.771     14.151  555.36   &lt;2e-16 ***\ndepth        -151.236      4.820  -31.38   &lt;2e-16 ***\ntable        -104.473      3.141  -33.26   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1526 on 53936 degrees of freedom\nMultiple R-squared:  0.8537,    Adjusted R-squared:  0.8537 \nF-statistic: 1.049e+05 on 3 and 53936 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n8.6.3 Multiple linear regression with both numeric and categorical predictors\nSo far, we have only used numeric predictors. But the diamonds dataset also contains categorical features such as cut, color, and clarity. These are qualitative measures that buyers care deeply about, and they can significantly affect the price.\nTo incorporate categorical predictors into regression, we use dummy variables (also called indicator variables). For example, “cut” has five categories: Fair, Good, Very Good, Premium, and Ideal. The model will estimate how much each cut category increases or decreases the expected price, relative to a reference category.\n\\[\\text{price}=\\beta_0+\\beta_1\\text{carat}+\\beta_2\\text{depth}+\\beta_3\\text{table}+\\beta4\\text{cut}+\\epsilon\\]\nTo make our life a bit easier, and to also showcase some more dplyr functions, we’re going to reduce the 5 categories in the cut variable down to 3, before passing it into the model. Have a look at the code below and see if you can understand its logic (don’t worry if you can’t get it now, as Minh will go through it during the lecture):\n\n\n\n\n\n\nR Code\n\n\n\n\n# Note here I am creating a new data set called diamonds2\ndiamonds2 &lt;- \n  diamonds |&gt; \n  mutate(cut2 = case_when(\n    cut %in% c(\"Fair\",\"Good\") ~ \"Level 1\",\n    cut %in% c(\"Very Good\") ~ \"Level 2\",\n    cut %in% c(\"Premium\", \"Ideal\") ~ \"Level 3\"\n  ))\n\n\n# Build a model with the new variable and new data set\nmodel3 &lt;- lm(price ~ carat + depth + table + cut2, data = diamonds2)\n\n# view results\nsummary(model3)\n\n\nCall:\nlm(formula = price ~ carat + depth + table + cut2, data = diamonds2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18042.0   -779.9    -32.8    517.9  12518.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8083.652    444.034   18.20   &lt;2e-16 ***\ncarat       7861.277     14.076  558.47   &lt;2e-16 ***\ndepth       -104.820      5.195  -20.18   &lt;2e-16 ***\ntable        -76.998      3.376  -22.81   &lt;2e-16 ***\ncut2Level 2  501.226     24.240   20.68   &lt;2e-16 ***\ncut2Level 3  550.542     22.876   24.07   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1518 on 53934 degrees of freedom\nMultiple R-squared:  0.8553,    Adjusted R-squared:  0.8553 \nF-statistic: 6.374e+04 on 5 and 53934 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n8.6.4 Comparing models\nWith several models at hand, a natural question arises: Which model is best?\nThere are many ways to compare models:\n\nGoodness of fit: How much variation in price does the model explain? (measured by R2)\nParsimony: Does the model use predictors efficiently, without overcomplicating things?\nPrediction error: How well does the model perform on unseen data?\n\nSometimes a simpler model with fewer predictors is preferable if it explains nearly as much as a complex one. Other times, adding categorical variables or interactions is worth it because it greatly improves predictive accuracy.\nLooking at the output from the 3 models above, which model do you think is best?\n\n\n\n\n\n\n\n\n\n\n\n \nModel 1\nModel 2\nModel 3\n\n\nPredictors\nEstimates\nEstimates\nEstimates\n\n\n(Intercept)\n-2256.36 ***\n(-2281.95 – -2230.77)\n13003.44 ***\n(12237.24 – 13769.64)\n8083.65 ***\n(7213.34 – 8953.96)\n\n\ncarat\n7756.43 ***\n(7728.86 – 7784.00)\n7858.77 ***\n(7831.03 – 7886.51)\n7861.28 ***\n(7833.69 – 7888.87)\n\n\ndepth\n\n-151.24 ***\n(-160.68 – -141.79)\n-104.82 ***\n(-115.00 – -94.64)\n\n\ntable\n\n-104.47 ***\n(-110.63 – -98.32)\n-77.00 ***\n(-83.62 – -70.38)\n\n\ncut2 [Level 2]\n\n\n501.23 ***\n(453.71 – 548.74)\n\n\ncut2 [Level 3]\n\n\n550.54 ***\n(505.70 – 595.38)\n\n\nObservations\n53940\n53940\n53940\n\n\nR2 / R2 adjusted\n0.849 / 0.849\n0.854 / 0.854\n0.855 / 0.855\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#summary",
    "href": "08-The-tidyverse.html#summary",
    "title": "8  The tidyverse",
    "section": "8.7 Summary",
    "text": "8.7 Summary\nIn this chapter, we introduced the tidyverse as a modern, coherent approach to data analysis in R. Rather than relying on verbose base R syntax, the tidyverse encourages clear, readable workflows built around pipes and a small set of consistent verbs. By working with tibbles and chaining operations step by step, complex analyses become easier to write, understand, and modify.\nWe focused on the core ideas of data manipulation using dplyr, including selecting variables, filtering observations, creating new variables, grouping data, and producing summaries. Along the way, we compared older base R functions (such as subset(), tapply(), and ifelse()) with their tidyverse counterparts, highlighting why the tidyverse approach is generally preferred in modern analysis.\nWe also revisited ggplot2, reinforcing the idea of plots as layered objects that map data to visual aesthetics. Through a series of examples, we showed how the tidyverse workflow naturally extends from data manipulation into data visualisation.\nFinally, we demonstrated how tidyverse tools integrate seamlessly with linear regression, from simple models to more complex models that include multiple numeric predictors and categorical variables. By combining tidy data manipulation, clear visualisation, and modelling, the tidyverse provides a powerful and flexible foundation for the analyses you will encounter throughout the rest of this unit.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "08-The-tidyverse.html#exercises",
    "href": "08-The-tidyverse.html#exercises",
    "title": "8  The tidyverse",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nWe will use the built-in mtcars dataset, which contains fuel consumption and design characteristics for 32 cars. Write some code in R to complete the following tasks:\n\nLoad the tidyverse package\nLoad the mtcars dataset and inspect it.\n\nHow many rows and columns does the dataset have?\nWhat does each row represent?\nWhich variables appear to be numeric?\n\nConvert mtcars into a tibble and store it as a new object called cars.\nConvert cyl (number of cylinders) to a factor.\nCreate a new data set (call it cars2) that:\n\nIncludes only cars with 4 or 6 cylinders.\nIncludes only cars with fuel efficiency (mpg) greater than 20\nKeeps only the variables model, mpg, cyl, hp and wt.\n\nUsing your new data set, create a new categorical variable called efficiency:\n\nHigh if mpg is greater than 25,\nModerate if mpg is between 20 and 25,\nLow otherwise\n\nGroup the data by cyl and calculate:\n\nthe average number of cars (n),\nthe average fuel efficienty (mean_mpg),\nthe average horsepower (mean_hp)\n\nArrange your results from part g so that the group with the highest average fuel efficiency appears first\nCreate a scatterplot showing the relationship between:\n\ncar weight (wt) on the x-axis,\nfuel efficency (mpg) on the y-axis\ncolor the points by the number of cylinders (cyl)\n\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nlibrary(tidyverse)\n\n\n# b.\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n# c. \ncars &lt;- as_tibble(mtcars, rownames = \"model\")\n\n\n# d.\ncars &lt;- \n  cars |&gt;\n  mutate(cyl = factor(cyl))\n\n\n# e.\ncars2 &lt;- \n  cars |&gt;\n  filter(cyl %in% c(\"4\", \"6\"), \n         mpg &gt; 20) |&gt;\n  select(model, mpg, cyl, hp, wt)\n\n\n# f.\ncars2 &lt;- \n  cars2 |&gt; \n  mutate(\n    efficiency = case_when(\n      mpg &gt; 25 ~ \"High\",\n      mpg &gt;= 20 & mpg &lt;= 25 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    )\n  )\n\n\n# g./h.\ncars2 |&gt;\n  group_by(cyl) |&gt;\n  summarise(\n    n = n(),\n    mean_mpg = mean(mpg),\n    mean_hp = mean(hp)\n  ) |&gt;\n  arrange(desc(mean_mpg))\n\n# A tibble: 2 × 4\n  cyl       n mean_mpg mean_hp\n  &lt;fct&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 4        11     26.7    82.6\n2 6         3     21.1   110  \n\n\n\n# i.\ncars2 |&gt;\n  ggplot(aes(x = wt, y = mpg, color = cyl)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nIn this exercise, you will use the palmerpenguins dataset to practise tidyverse data handling and fit regression models. The dataset contains measurements on penguins from three species.\nTo begin you’ll need to have the palmerpenguins package installed.\n\ninstall.packages(\"palmerpenguins\")\n\nOnce you have installed this package, write some code to complete the following tasks:\n\nLoad the tidyverse and palmerpenguins package\nLoad the penguins dataset. Inspect the data and determine:\n\nHow many rows and columns are there?\nWhich variables are numeric?\nWhaich variables are categorical?\n\nCreate a new dataset called penguins2 that:\n\nKeeps only these variables: species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g\nRemoves rows with missing values in any of these variables\n\nFor each species, calculate:\n\nnumber of penguins (n)\nmean body mass (mean_mass)\nstandard deviation of body mass (sd_mass)\n\nCreate a scatterplot of flipper_length_mm (x-axis) vs body_mass_g (y-axis), coloured by species. Add a regression line for each species.\nFit a simple linear regression model predicting body mass from flipper length and store it as m1\nUse the summary() function to inspect m1. Comment on each coefficient.\nFit a multiple regression model predicting body mass from flipper length and bill length, and call it m2. Inspect the model and comment on each coefficient.\nFit a model that includes species as a categorical predictor, plus flipper length, and call it m3. Inspect the model and comment on each coefficient.\nFit an interaction model that includes flipper length, species, and the interaction between flipper_length and species, and store it as m4. Inspect the model and comment on each coefficient.\nCompare all four models using \\(R^2\\) and \\(\\text{Adjusted } R^2\\). Which model seems best, and why?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n\n# b.\nstr(penguins)\n\ntibble [344 × 7] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\n\n# c.\npenguins2 &lt;- \n  penguins |&gt; \n  select(species, \n         island, \n         bill_length_mm, \n         bill_depth_mm, \n         flipper_length_mm, \n         body_mass_g) |&gt; \n  na.omit()\n\n\n# d.\npenguins2 |&gt; \n  group_by(species) |&gt; \n  summarise(\n    n = n(),\n    mean_mass = mean(body_mass_g),\n    sd_mass = sd(body_mass_g)\n  )\n\n# A tibble: 3 × 4\n  species       n mean_mass sd_mass\n  &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Adelie      151     3701.    459.\n2 Chinstrap    68     3733.    384.\n3 Gentoo      123     5076.    504.\n\n\n\n# e.\npenguins2 |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F)\n\n\n\n\n\n\n\n\n\n# f.\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm, penguins2)\n\n\n# g.\nsummary(m1)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm, data = penguins2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1058.80  -259.27   -26.88   247.33  1288.69 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5780.831    305.815  -18.90   &lt;2e-16 ***\nflipper_length_mm    49.686      1.518   32.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.3 on 340 degrees of freedom\nMultiple R-squared:  0.759, Adjusted R-squared:  0.7583 \nF-statistic:  1071 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\n\n\n# h.\nm2 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm, penguins2)\nsummary(m2)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm, \n    data = penguins2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1090.5  -285.7   -32.1   244.2  1287.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -5736.897    307.959 -18.629   &lt;2e-16 ***\nflipper_length_mm    48.145      2.011  23.939   &lt;2e-16 ***\nbill_length_mm        6.047      5.180   1.168    0.244    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 394.1 on 339 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7585 \nF-statistic: 536.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\n\n# i.\nm3 &lt;- lm(body_mass_g ~ species + flipper_length_mm, penguins2)\nsummary(m3)\n\n\nCall:\nlm(formula = body_mass_g ~ species + flipper_length_mm, data = penguins2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-927.70 -254.82  -23.92  241.16 1191.68 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -4031.477    584.151  -6.901 2.55e-11 ***\nspeciesChinstrap   -206.510     57.731  -3.577 0.000398 ***\nspeciesGentoo       266.810     95.264   2.801 0.005392 ** \nflipper_length_mm    40.705      3.071  13.255  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 375.5 on 338 degrees of freedom\nMultiple R-squared:  0.7826,    Adjusted R-squared:  0.7807 \nF-statistic: 405.7 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\n\n# j.\nm4 &lt;- lm(body_mass_g ~ species + flipper_length_mm + species*flipper_length_mm, penguins2)\nsummary(m4)\n\n\nCall:\nlm(formula = body_mass_g ~ species + flipper_length_mm + species * \n    flipper_length_mm, data = penguins2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-911.18 -251.93  -31.77  197.82 1144.81 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        -2535.837    879.468  -2.883  0.00419 ** \nspeciesChinstrap                    -501.359   1523.459  -0.329  0.74229    \nspeciesGentoo                      -4251.444   1427.332  -2.979  0.00311 ** \nflipper_length_mm                     32.832      4.627   7.095 7.69e-12 ***\nspeciesChinstrap:flipper_length_mm     1.742      7.856   0.222  0.82467    \nspeciesGentoo:flipper_length_mm       21.791      6.941   3.139  0.00184 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 370.6 on 336 degrees of freedom\nMultiple R-squared:  0.7896,    Adjusted R-squared:  0.7864 \nF-statistic: 252.2 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\n\n# k.\ndata.frame(\n  model = 1:4,\n  R2 = c(\n    summary(m1)$r.squared,\n    summary(m2)$r.squared,\n    summary(m3)$r.squared,\n    summary(m4)$r.squared),\n  Adj.R2 = c(\n    summary(m1)$adj.r.squared,\n    summary(m2)$adj.r.squared,\n    summary(m3)$adj.r.squared,\n    summary(m4)$adj.r.squared\n  )\n) |&gt; \n  mutate_all(round, 3)\n\n  model    R2 Adj.R2\n1     1 0.759  0.758\n2     2 0.760  0.759\n3     3 0.783  0.781\n4     4 0.790  0.786\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nIn this exercise, you will use the ames dataset to practise tidyverse data handling and fit regression models. The dataset contains sales data for 2,930 properties in Ames IA.\nTo begin you’ll need to have the modeldata package installed.\n\ninstall.packages(\"modeldata\")\n\nOnce you have installed this package, write some code to complete the following tasks:\n\nLoad the tidyverse and modeldata packages\nLoad the ames dataset. Inspect the data and determine how many rows and columns are there.\nWhat does the variable Sale_Price represent?\nCreate a new dataset called ames2 that:\n\nKeeps only the variables: Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built and Neighborhood\nRemoves rows with missing values\n\nFor each Neighborhood, calculate:\n\nthe number of houses (n)\nthe average sale price (mean_price)\n\nCreate a new dataset that keeps neighborhoods with at least 100 houses, and call this ames3.\nCreate a scatterplot such that:\n\nliving area (Gr_Liv_Area) is on the x-axis\nsale price (Sale_Price) is on the y-axis\nAdd a regression line\nlog transform the y-axis\nOnly show houses where Gr_Liv_Area is less than 4000\n\nFit a simple linear regression model predicting sale price from living area and store the model as m1. Inspect m1 and interpret the coefficients.\nFit a multiple regression model predicting sale price from living area, overall condition and year built, and store the model as m2. Inspect m2 and interpret the coefficients.\nCreate a new variable (call it Year_Cat) that groups houses into:\n\nOld (built before 1970)\nModern (built from 1970 onward)\n\nRefit m2 except replace year built with the newly created categorical variable from the previous question. Call this model m2b. Inspect m2b and interpret the coefficients.\nCompare m1, m2 and m2b using \\(\\text{adjusted } R^2\\). Which model provides the best balance between fit and complexity?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nlibrary(tidyverse)\nlibrary(modeldata)\n\n\n# b. / c.\nstr(ames)\n\ntibble [2,930 × 74] (S3: tbl_df/tbl/data.frame)\n $ MS_SubClass       : Factor w/ 16 levels \"One_Story_1946_and_Newer_All_Styles\",..: 1 1 1 1 6 6 12 12 12 6 ...\n $ MS_Zoning         : Factor w/ 7 levels \"Floating_Village_Residential\",..: 3 2 3 3 3 3 3 3 3 3 ...\n $ Lot_Frontage      : num [1:2930] 141 80 81 93 74 78 41 43 39 60 ...\n $ Lot_Area          : int [1:2930] 31770 11622 14267 11160 13830 9978 4920 5005 5389 7500 ...\n $ Street            : Factor w/ 2 levels \"Grvl\",\"Pave\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Alley             : Factor w/ 3 levels \"Gravel\",\"No_Alley_Access\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Lot_Shape         : Factor w/ 4 levels \"Regular\",\"Slightly_Irregular\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ Land_Contour      : Factor w/ 4 levels \"Bnk\",\"HLS\",\"Low\",..: 4 4 4 4 4 4 4 2 4 4 ...\n $ Utilities         : Factor w/ 3 levels \"AllPub\",\"NoSeWa\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Lot_Config        : Factor w/ 5 levels \"Corner\",\"CulDSac\",..: 1 5 1 1 5 5 5 5 5 5 ...\n $ Land_Slope        : Factor w/ 3 levels \"Gtl\",\"Mod\",\"Sev\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Neighborhood      : Factor w/ 29 levels \"North_Ames\",\"College_Creek\",..: 1 1 1 1 7 7 17 17 17 7 ...\n $ Condition_1       : Factor w/ 9 levels \"Artery\",\"Feedr\",..: 3 2 3 3 3 3 3 3 3 3 ...\n $ Condition_2       : Factor w/ 8 levels \"Artery\",\"Feedr\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ Bldg_Type         : Factor w/ 5 levels \"OneFam\",\"TwoFmCon\",..: 1 1 1 1 1 1 5 5 5 1 ...\n $ House_Style       : Factor w/ 8 levels \"One_and_Half_Fin\",..: 3 3 3 3 8 8 3 3 3 8 ...\n $ Overall_Cond      : Factor w/ 10 levels \"Very_Poor\",\"Poor\",..: 5 6 6 5 5 6 5 5 5 5 ...\n $ Year_Built        : int [1:2930] 1960 1961 1958 1968 1997 1998 2001 1992 1995 1999 ...\n $ Year_Remod_Add    : int [1:2930] 1960 1961 1958 1968 1998 1998 2001 1992 1996 1999 ...\n $ Roof_Style        : Factor w/ 6 levels \"Flat\",\"Gable\",..: 4 2 4 4 2 2 2 2 2 2 ...\n $ Roof_Matl         : Factor w/ 8 levels \"ClyTile\",\"CompShg\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Exterior_1st      : Factor w/ 16 levels \"AsbShng\",\"AsphShn\",..: 4 14 15 4 14 14 6 7 6 14 ...\n $ Exterior_2nd      : Factor w/ 17 levels \"AsbShng\",\"AsphShn\",..: 11 15 16 4 15 15 6 7 6 15 ...\n $ Mas_Vnr_Type      : Factor w/ 5 levels \"BrkCmn\",\"BrkFace\",..: 5 4 2 4 4 2 4 4 4 4 ...\n $ Mas_Vnr_Area      : num [1:2930] 112 0 108 0 0 20 0 0 0 0 ...\n $ Exter_Cond        : Factor w/ 5 levels \"Excellent\",\"Fair\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ Foundation        : Factor w/ 6 levels \"BrkTil\",\"CBlock\",..: 2 2 2 2 3 3 3 3 3 3 ...\n $ Bsmt_Cond         : Factor w/ 6 levels \"Excellent\",\"Fair\",..: 3 6 6 6 6 6 6 6 6 6 ...\n $ Bsmt_Exposure     : Factor w/ 5 levels \"Av\",\"Gd\",\"Mn\",..: 2 4 4 4 4 4 3 4 4 4 ...\n $ BsmtFin_Type_1    : Factor w/ 7 levels \"ALQ\",\"BLQ\",\"GLQ\",..: 2 6 1 1 3 3 3 1 3 7 ...\n $ BsmtFin_SF_1      : num [1:2930] 2 6 1 1 3 3 3 1 3 7 ...\n $ BsmtFin_Type_2    : Factor w/ 7 levels \"ALQ\",\"BLQ\",\"GLQ\",..: 7 4 7 7 7 7 7 7 7 7 ...\n $ BsmtFin_SF_2      : num [1:2930] 0 144 0 0 0 0 0 0 0 0 ...\n $ Bsmt_Unf_SF       : num [1:2930] 441 270 406 1045 137 ...\n $ Total_Bsmt_SF     : num [1:2930] 1080 882 1329 2110 928 ...\n $ Heating           : Factor w/ 6 levels \"Floor\",\"GasA\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ Heating_QC        : Factor w/ 5 levels \"Excellent\",\"Fair\",..: 2 5 5 1 3 1 1 1 1 3 ...\n $ Central_Air       : Factor w/ 2 levels \"N\",\"Y\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Electrical        : Factor w/ 6 levels \"FuseA\",\"FuseF\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ First_Flr_SF      : int [1:2930] 1656 896 1329 2110 928 926 1338 1280 1616 1028 ...\n $ Second_Flr_SF     : int [1:2930] 0 0 0 0 701 678 0 0 0 776 ...\n $ Gr_Liv_Area       : int [1:2930] 1656 896 1329 2110 1629 1604 1338 1280 1616 1804 ...\n $ Bsmt_Full_Bath    : num [1:2930] 1 0 0 1 0 0 1 0 1 0 ...\n $ Bsmt_Half_Bath    : num [1:2930] 0 0 0 0 0 0 0 0 0 0 ...\n $ Full_Bath         : int [1:2930] 1 1 1 2 2 2 2 2 2 2 ...\n $ Half_Bath         : int [1:2930] 0 0 1 1 1 1 0 0 0 1 ...\n $ Bedroom_AbvGr     : int [1:2930] 3 2 3 3 3 3 2 2 2 3 ...\n $ Kitchen_AbvGr     : int [1:2930] 1 1 1 1 1 1 1 1 1 1 ...\n $ TotRms_AbvGrd     : int [1:2930] 7 5 6 8 6 7 6 5 5 7 ...\n $ Functional        : Factor w/ 8 levels \"Maj1\",\"Maj2\",..: 8 8 8 8 8 8 8 8 8 8 ...\n $ Fireplaces        : int [1:2930] 2 0 0 2 1 1 0 0 1 1 ...\n $ Garage_Type       : Factor w/ 7 levels \"Attchd\",\"Basment\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Garage_Finish     : Factor w/ 4 levels \"Fin\",\"No_Garage\",..: 1 4 4 1 1 1 1 3 3 1 ...\n $ Garage_Cars       : num [1:2930] 2 1 1 2 2 2 2 2 2 2 ...\n $ Garage_Area       : num [1:2930] 528 730 312 522 482 470 582 506 608 442 ...\n $ Garage_Cond       : Factor w/ 6 levels \"Excellent\",\"Fair\",..: 6 6 6 6 6 6 6 6 6 6 ...\n $ Paved_Drive       : Factor w/ 3 levels \"Dirt_Gravel\",..: 2 3 3 3 3 3 3 3 3 3 ...\n $ Wood_Deck_SF      : int [1:2930] 210 140 393 0 212 360 0 0 237 140 ...\n $ Open_Porch_SF     : int [1:2930] 62 0 36 0 34 36 0 82 152 60 ...\n $ Enclosed_Porch    : int [1:2930] 0 0 0 0 0 0 170 0 0 0 ...\n $ Three_season_porch: int [1:2930] 0 0 0 0 0 0 0 0 0 0 ...\n $ Screen_Porch      : int [1:2930] 0 120 0 0 0 0 0 144 0 0 ...\n $ Pool_Area         : int [1:2930] 0 0 0 0 0 0 0 0 0 0 ...\n $ Pool_QC           : Factor w/ 5 levels \"Excellent\",\"Fair\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ Fence             : Factor w/ 5 levels \"Good_Privacy\",..: 5 3 5 5 3 5 5 5 5 5 ...\n $ Misc_Feature      : Factor w/ 6 levels \"Elev\",\"Gar2\",..: 3 3 2 3 3 3 3 3 3 3 ...\n $ Misc_Val          : int [1:2930] 0 0 12500 0 0 0 0 0 0 0 ...\n $ Mo_Sold           : int [1:2930] 5 6 6 4 3 6 4 1 3 6 ...\n $ Year_Sold         : int [1:2930] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...\n $ Sale_Type         : Factor w/ 10 levels \"COD\",\"Con\",\"ConLD\",..: 10 10 10 10 10 10 10 10 10 10 ...\n $ Sale_Condition    : Factor w/ 6 levels \"Abnorml\",\"AdjLand\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ Sale_Price        : int [1:2930] 215000 105000 172000 244000 189900 195500 213500 191500 236500 189000 ...\n $ Longitude         : num [1:2930] -93.6 -93.6 -93.6 -93.6 -93.6 ...\n $ Latitude          : num [1:2930] 42.1 42.1 42.1 42.1 42.1 ...\n\n\n\n# d.\names2 &lt;- \n  ames |&gt; \n  select(Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood) |&gt; \n  na.omit()\n\n\n# e.\names2 |&gt; \n  group_by(Neighborhood) |&gt; \n  summarise(n = n(),\n            mean_price = mean(Sale_Price))\n\n# A tibble: 28 × 3\n   Neighborhood           n mean_price\n   &lt;fct&gt;              &lt;int&gt;      &lt;dbl&gt;\n 1 North_Ames           443    145097.\n 2 College_Creek        267    201803.\n 3 Old_Town             239    123992.\n 4 Edwards              194    130843.\n 5 Somerset             182    229707.\n 6 Northridge_Heights   166    322018.\n 7 Gilbert              165    190647.\n 8 Sawyer               151    136751.\n 9 Northwest_Ames       131    188407.\n10 Sawyer_West          125    184070.\n# ℹ 18 more rows\n\n\n\n# f.\names3 &lt;- \n  ames2 |&gt; \n  group_by(Neighborhood) |&gt; \n  mutate(n = n()) |&gt; \n  filter(n &gt; 100)\n\n\n# g.\names3 |&gt; \n  filter(Gr_Liv_Area &lt; 4000) |&gt; \n  ggplot(aes(x = Gr_Liv_Area, y = log(Sale_Price))) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F)\n\n\n\n\n\n\n\n\n\n# h.\nm1 &lt;- lm(Sale_Price ~ Gr_Liv_Area, ames3)\nsummary(m1)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-436045  -25949   -2069   19287  345482 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28283.051   3535.397    8.00 1.92e-15 ***\nGr_Liv_Area   100.631      2.284   44.06  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53040 on 2386 degrees of freedom\nMultiple R-squared:  0.4487,    Adjusted R-squared:  0.4484 \nF-statistic:  1942 on 1 and 2386 DF,  p-value: &lt; 2.2e-16\n\n\n\n# i.\nm2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built, ames3)\nsummary(m2)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built, \n    data = ames3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-413391  -23037   -3052   14120  320545 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -2.207e+06  7.457e+04 -29.600  &lt; 2e-16 ***\nGr_Liv_Area                8.618e+01  1.895e+00  45.477  &lt; 2e-16 ***\nOverall_CondPoor           1.162e+04  2.737e+04   0.424 0.671321    \nOverall_CondFair           3.143e+04  2.233e+04   1.408 0.159350    \nOverall_CondBelow_Average  4.047e+04  2.176e+04   1.860 0.063021 .  \nOverall_CondAverage        6.048e+04  2.134e+04   2.834 0.004636 ** \nOverall_CondAbove_Average  6.250e+04  2.133e+04   2.930 0.003417 ** \nOverall_CondGood           7.577e+04  2.134e+04   3.551 0.000392 ***\nOverall_CondVery_Good      8.791e+04  2.155e+04   4.078 4.68e-05 ***\nOverall_CondExcellent      1.098e+05  2.238e+04   4.907 9.89e-07 ***\nYear_Built                 1.113e+03  3.724e+01  29.878  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42400 on 2377 degrees of freedom\nMultiple R-squared:  0.6491,    Adjusted R-squared:  0.6476 \nF-statistic: 439.6 on 10 and 2377 DF,  p-value: &lt; 2.2e-16\n\n\n\n# j.\names4 &lt;- ames3 |&gt; mutate(Year_Cat = ifelse(Year_Built &lt; 1970, \"Old\", \"Modern\"))\n\n\n# k.\nm2b &lt;- lm(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Cat, ames4)\nsummary(m2b)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Cat, \n    data = ames4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-398177  -25828   -4416   18589  332757 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -14318.725  23300.808  -0.615 0.538933    \nGr_Liv_Area                   85.197      2.075  41.062  &lt; 2e-16 ***\nOverall_CondPoor           18249.980  29619.524   0.616 0.537857    \nOverall_CondFair           46126.401  24152.803   1.910 0.056283 .  \nOverall_CondBelow_Average  54315.142  23538.713   2.307 0.021114 *  \nOverall_CondAverage        91812.629  23036.179   3.986 6.93e-05 ***\nOverall_CondAbove_Average  81409.270  23058.539   3.531 0.000423 ***\nOverall_CondGood           89818.457  23083.664   3.891 0.000103 ***\nOverall_CondVery_Good      94604.689  23323.096   4.056 5.15e-05 ***\nOverall_CondExcellent     106223.215  24225.492   4.385 1.21e-05 ***\nYear_CatOld               -46340.097   2275.212 -20.367  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45880 on 2377 degrees of freedom\nMultiple R-squared:  0.589, Adjusted R-squared:  0.5873 \nF-statistic: 340.6 on 10 and 2377 DF,  p-value: &lt; 2.2e-16\n\n\n\n# l.\ndata.frame(\n  model = c(\"1\",\"2\",\"2b\"),\n  Adj.R2 = c(\n    summary(m1)$adj.r.squared,\n    summary(m2)$adj.r.squared,\n    summary(m2b)$adj.r.squared\n  )\n)\n\n  model    Adj.R2\n1     1 0.4484222\n2     2 0.6475844\n3    2b 0.5872627\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nIn this exercise, you will analyse customer credit data to understand factors associated with credit card debt. This dataset combines demographic information, financial capacity, and credit history to help explain credit outcomes.\nTo begin you’ll need to have the modeldata package installed.\nOnce you have installed this package, write some code to complete the following tasks:\n\nLoad the tidyverse and modeldata packages\nLoad the credit_data dataset. Inspect the data and determine how many rows and columns are there.\nCreate a new dataset called credit2 that:\n\nKeeps only the variables Debt, Income, Age, Home, Marital and Assets\nRemoves any rows with missing values\n\nFor each Home category, calculate:\n\nthe numer of individuals (n),\nthe average debt (mean_debt),\nthe median income (median_income)\n\nCreate a boxplot that:\n\nhas Home on the x-axis\nlog(Debt) on the y-axis\nonly shows ‘owner’, ‘rent’, ‘parents’ and ‘priv’ as the Home categories\n\nFit a simple linear regression model predicting debt from income, and store it as m1. Inspect m1 and interpret the coefficients.\nFit a multiple linear regression model predicting debt from income, age and assets, and store it as m2. Inspect m2 and interpret the coefficients.\nCreate a new variable called Debt_level:\n\nLow if Debt &lt; 1000\nModerate if Debt is between 1000 and 5000\nHigh if Debt &gt; 5000\n\nRecreate m2 for each level of debt (so you should have 3 new models). Call these models m2_low, m2_mod and m2_high. Inspect each model and compare the coefficients.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a.\nlibrary(tidyverse)\nlibrary(modeldata)\n\n\n# b.\nstr(credit_data)\n\n'data.frame':   4454 obs. of  14 variables:\n $ Status   : Factor w/ 2 levels \"bad\",\"good\": 2 2 1 2 2 2 2 2 2 1 ...\n $ Seniority: int  9 17 10 0 0 1 29 9 0 0 ...\n $ Home     : Factor w/ 6 levels \"ignore\",\"other\",..: 6 6 3 6 6 3 3 4 3 4 ...\n $ Time     : int  60 60 36 60 36 60 60 12 60 48 ...\n $ Age      : int  30 58 46 24 26 36 44 27 32 41 ...\n $ Marital  : Factor w/ 5 levels \"divorced\",\"married\",..: 2 5 2 4 4 2 2 4 2 2 ...\n $ Records  : Factor w/ 2 levels \"no\",\"yes\": 1 1 2 1 1 1 1 1 1 1 ...\n $ Job      : Factor w/ 4 levels \"fixed\",\"freelance\",..: 2 1 2 1 1 1 1 1 2 4 ...\n $ Expenses : int  73 48 90 63 46 75 75 35 90 90 ...\n $ Income   : int  129 131 200 182 107 214 125 80 107 80 ...\n $ Assets   : int  0 0 3000 2500 0 3500 10000 0 15000 0 ...\n $ Debt     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Amount   : int  800 1000 2000 900 310 650 1600 200 1200 1200 ...\n $ Price    : int  846 1658 2985 1325 910 1645 1800 1093 1957 1468 ...\n\n\n\n# c.\ncredit2 &lt;- \n  credit_data |&gt; \n  select(Debt, Income, Age, Home, Marital, Assets) |&gt; \n  na.omit()\n\n\n# d.\ncredit2 |&gt; \n  group_by(Home) |&gt; \n  summarise(n = n(),\n            mean_debt = mean(Debt),\n            median_income = median(Income))\n\n# A tibble: 6 × 4\n  Home        n mean_debt median_income\n  &lt;fct&gt;   &lt;int&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 ignore     19      57.9           102\n2 other     259      38.0           119\n3 owner    1930     562.            133\n4 parents   721      55.6           101\n5 priv      227     623.            129\n6 rent      883      86.7           125\n\n\n\n# e.\ncredit_data |&gt; \n  filter(Home %in% c(\"owner\",\"rent\",\"parents\",\"priv\")) |&gt; \n  ggplot(aes(x = Home, y = log(Debt))) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# f.\nm1 &lt;- lm(Debt ~ Income, credit2)\nsummary(m1)\n\n\nCall:\nlm(formula = Debt ~ Income, data = credit2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1772.7  -359.9  -265.4  -183.0 20344.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.3956    33.6771   1.496    0.135    \nIncome        2.0097     0.2065   9.731   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1058 on 4037 degrees of freedom\nMultiple R-squared:  0.02292,   Adjusted R-squared:  0.02268 \nF-statistic:  94.7 on 1 and 4037 DF,  p-value: &lt; 2.2e-16\n\n\n\n# g.\nm2 &lt;- lm(Debt ~ Income + Age + Assets, credit2)\nsummary(m2)\n\n\nCall:\nlm(formula = Debt ~ Income + Age + Assets, data = credit2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4435.9  -363.5  -248.4  -113.4 20104.5 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 367.788731  61.713652   5.960 2.74e-09 ***\nIncome        1.720185   0.210754   8.162 4.36e-16 ***\nAge          -9.913925   1.536467  -6.452 1.23e-10 ***\nAssets        0.017775   0.001846   9.627  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1044 on 4035 degrees of freedom\nMultiple R-squared:  0.04987,   Adjusted R-squared:  0.04917 \nF-statistic:  70.6 on 3 and 4035 DF,  p-value: &lt; 2.2e-16\n\n\n\n# h.\ncredit2 &lt;- credit2 |&gt;\n  mutate(\n    Debt_level = case_when(\n      Debt &lt; 1000              ~ \"Low\",\n      Debt &gt;= 1000 & Debt &lt;= 5000 ~ \"Moderate\",\n      Debt &gt; 5000              ~ \"High\"\n    )\n  )\n\n\n# i.\nm2_low &lt;- lm(Debt ~ Income + Age + Assets, credit2 |&gt; filter(Debt_level == 'Low'))\nm2_mod &lt;- lm(Debt ~ Income + Age + Assets, credit2 |&gt; filter(Debt_level == 'Moderate'))\nm2_high &lt;- lm(Debt ~ Income + Age + Assets, credit2 |&gt; filter(Debt_level == 'High'))\n\nsjPlot::tab_model(\n  m2, m2_low, m2_mod, m2_high,\n  dv.labels = c(\"Combined\",\"Low Debt\",\"Moderate Debt\",\"High Debt\"),\n  collapse.ci = T,\n  p.style = 'stars'\n)\n\n\n\n\n\n\n\n\n\n\n\n \nCombined\nLow Debt\nModerate Debt\nHigh Debt\n\n\nPredictors\nEstimates\nEstimates\nEstimates\nEstimates\n\n\n(Intercept)\n367.79 ***\n(246.80 – 488.78)\n28.62 ***\n(12.83 – 44.41)\n2302.46 ***\n(1902.79 – 2702.13)\n5843.80 **\n(1698.61 – 9988.99)\n\n\nIncome\n1.72 ***\n(1.31 – 2.13)\n0.04 \n(-0.01 – 0.10)\n0.68 \n(-0.56 – 1.91)\n12.69 ***\n(6.13 – 19.26)\n\n\nAge\n-9.91 ***\n(-12.93 – -6.90)\n-0.16 \n(-0.54 – 0.23)\n-11.80 *\n(-23.11 – -0.49)\n-35.63 \n(-148.26 – 76.99)\n\n\nAssets\n0.02 ***\n(0.01 – 0.02)\n0.00 \n(-0.00 – 0.00)\n0.04 ***\n(0.03 – 0.06)\n0.07 \n(-0.00 – 0.14)\n\n\nObservations\n4039\n3569\n446\n24\n\n\nR2 / R2 adjusted\n0.050 / 0.049\n0.001 / 0.000\n0.066 / 0.060\n0.540 / 0.471\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The tidyverse</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html",
    "href": "09-Classification-and-prediction.html",
    "title": "9  Classification and Prediction",
    "section": "",
    "text": "9.1 Introduction\nOver the past few chapters, we have focused on linear models, where the outcome of interest is numeric. Using tools such as simple and multiple linear regression, we learned how to quantify relationships between variables, interpret model coefficients, and use data to make predictions about continuous outcomes — such as income ($), age (years), reaction time (secs), or performance scores (points).\nHowever, not all real-world questions involve predicting numbers. In many situations, our goal is to predict a category, group, or decision rather than a numerical value. For example:\nThese problems fall under the umbrella of classification, where the outcome variable is categorical rather than numeric. In this chapter, we extend the ideas of modelling and prediction to these types of problems. While the goal changes—from predicting a number to predicting a class—the underlying principles remain familiar. We still:\nIn this chapter, we take a first step into classification by using a familiar tool in an unfamiliar way: linear regression. Specifically, we introduce the linear probability model (LPM), where a binary outcome is modelled using a linear regression framework. The LPM allows us to interpret coefficients as changes in probability, making it an intuitive and accessible entry point into classification modelling—especially given our existing understanding of linear models.\nHowever, while the linear probability model is simple and easy to interpret, it comes with important limitations, such as:\nUnderstanding why the linear probability model breaks down is just as important as knowing how to fit it. These limitations motivate the need for more appropriate models for binary outcomes. In the next chapter, we will build on this foundation by introducing logistic regression, a model specifically designed for classification problems. Logistic regression resolves many of the issues encountered with the linear probability model while retaining a clear interpretation in terms of probabilities, making it a central tool in statistics, econometrics, and data science.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#introduction",
    "href": "09-Classification-and-prediction.html#introduction",
    "title": "9  Classification and Prediction",
    "section": "",
    "text": "Will a customer default or not default on a loan? (yes or no)\nWill a student pass or fail a unit? (pass or fail)\nIs an athlete selected or not selected for a team? (selected or not selected)\n\n\n\nuse predictors (explanatory variables),\nbuild models based on observed data,\nevaluate how well those models perform, and\nuse them to make predictions for new observations.\n\n\n\n\nthe possibility of predicted probabilities falling outside the 0–1 range,\nviolations of key regression assumptions, and\nchallenges in interpreting uncertainty and model fit.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#a-quick-recap-linear-regression",
    "href": "09-Classification-and-prediction.html#a-quick-recap-linear-regression",
    "title": "9  Classification and Prediction",
    "section": "9.2 A quick recap: Linear regression",
    "text": "9.2 A quick recap: Linear regression\nBefore moving into classification models, we briefly revisit linear regression as this will be crucial for understanding classification.\nSuppose we are interested in predicting goal difference (also called Margin) in a football match. A positive margin means the home team wins by that many goals, while a negative margin indicates a loss. As a simple starting point, we assume that the only predictor of match outcome is the difference in team ratings (Ratings Difference) prior to the game.\nWe observe the following data:\n\n\nMarginRatings Difference-20-17-17-304061840314946\n\n\nIn this particular case, we can express the relationship between these two variables as:\n\\[Margin=\\beta_0+\\beta_1(\\text{Ratings Difference})\\]\nTheoretically this model makes sense, as larger rating differences should have greater margins (imagine an elite adult level team vs a group of 5-year-old children - the difference in ratings would be huge, and therefore we would predict a much larger margin).\nIf we were to plot this data it might look like the image below (where the dashed blue line represents the trend line).\n\n\n\n\n\n\n\n\n\nAnd using the lm() function in R, we can easily see what the values of the \\(\\beta\\) coefficients would be in this model:\n\n\n\n\n\n\nR Code\n\n\n\n\nlm(Margin ~ Diff, df1)\n\n\nCall:\nlm(formula = Margin ~ Diff, data = df1)\n\nCoefficients:\n(Intercept)         Diff  \n     2.8088       0.9406  \n\n\n\n\nFrom the output above, our fitted model would be:\n\\[\\text{Margin}=2.81+0.94(\\text{Ratings Difference})\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#explain-versus-predict",
    "href": "09-Classification-and-prediction.html#explain-versus-predict",
    "title": "9  Classification and Prediction",
    "section": "9.3 Explain versus Predict",
    "text": "9.3 Explain versus Predict\nIn Statistics, it’s important to be clear about what we are trying to achieve. Broadly, statistical models are used for two related but distinct purposes: explanation and prediction.\n\n9.3.1 Explaination\nExplanation is about understanding why things happen. An explanatory model focuses on the relationship between variables, aiming to isolate and interpret the effect of one variable on another. For example, we might ask: How much does a one-unit increase in team rating difference change the expected goal margin, on average? From our model above, we can see that the slope for Ratings Difference (\\(\\beta_1\\)) is 0.94. This helps us to explain the relationship between ratings difference and margin, i.e. for every one-unit increase in ratings difference, the expected margin increases by 0.94 points.\n\n\n9.3.2 Prediction\nPrediction, by comparison, is about using a model to generate predictions for future or unseen observations. The primary goal is not to interpret individual coefficients, but to produce accurate forecasts of an outcome based on available information. For instance, we might use team rating differences to predict the goal margin in an upcoming match\nFor example, suppose two teams that were to play a match next week. The ratings for both teams are as follows:\n\n\\(Rating_\\text{Team A}=1530\\)\n\\(Rating_\\text{Team B}=1510\\)\n\nIn this case, the Ratings Difference is \\(1530-1510=20\\). Therefore, using our model, we would predict the Margin to be:\n\\[\n\\begin{aligned}\n\\text{Margin} &=2.81 + 0.94(\\text{Ratings Difference}) \\\\\n&=2.81 + 0.94(20) \\\\\n&=21.61\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#training-and-testing-sets",
    "href": "09-Classification-and-prediction.html#training-and-testing-sets",
    "title": "9  Classification and Prediction",
    "section": "9.4 Training and Testing Sets",
    "text": "9.4 Training and Testing Sets\nSo far, when fitting a regression model, we have used all available data to estimate the relationship between predictors and the outcome. While this is useful for understanding relationships, it does not tell us how well the model will perform on new, unseen data.\nIn prediction and classification problems, this distinction is crucial. Our goal is not just to explain the past, but to make accurate predictions about the future.\n\n9.4.1 Training data\nThe training dataset is the portion of data used to fit the model. This is where the model learns the relationship between variables—for example, how differences in team ratings relate to goal margin.\nUsing training data, we:\n\nestimate model parameters (such as the intercept and slope),\nchoose model structure, and\nunderstand how predictors influence the outcome.\n\nHowever, a model that fits the training data very well may still perform poorly on new data. This phenomenon is known as overfitting, where the model captures noise or idiosyncrasies specific to the training sample rather than the underlying pattern. For example, let’s use our current data to fit two models: (1) a linear model and (2) a complicated non-linear model that connects all of the data.\n\n\n\n\n\n9.4.2 Testing data\nThe testing dataset is held back during model fitting and is used only to evaluate model performance. Because the model has never “seen” this data before, performance on the test set provides a more realistic measure of how well the model is likely to perform in practice. For example, suppose we have 4 games (highlighted in green below) that were held back during training the model. We can visualise their deviations from the models as the solid black lines:\n\n\n\nIf we sum all of the errors (and look at the lengths of the errors - see image below), we would find that the linear model has a smaller error term which ultimately leads to better predictions.\n\n\n\n\nSo even though the complicated model fitted the data better, the linear model was better at making predictions. So if we had to choose between these two models for making predictions, we would go with the linear one. Note: fitting the training data well but making poor predictions is the bias-variance trade-off problem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#linear-probability-model-lpm",
    "href": "09-Classification-and-prediction.html#linear-probability-model-lpm",
    "title": "9  Classification and Prediction",
    "section": "9.5 Linear Probability Model (LPM)",
    "text": "9.5 Linear Probability Model (LPM)\nIn the previous section we looked at an example where the outcome (Margin) was numeric, see table below which includes a few extra data points.\n\n\nRatings DifferenceMargin-17-20-30-17041863140464910-15-1620-9\n\n\nSuppose that instead of modelling Margin, we were interested in a binary outcome such as Win / Loss\n\n\nRatings DifferenceMarginWin-17-20Loss-30-17Loss04Win186Win3140Win4649Win10-1Loss5-16Loss20-9Loss\n\n\nUsually these are denoted with 0s and 1s to make computation a bit easier\n\n\nRatings DifferenceMarginWin-17-200-30-1700411861314014649110-10516020-90\n\n\nNow, if we tried to plot Ratings Difference and Win, we would obtain the Figure below. Notice here that because our outcome is binary, we can only have y values of 0 (Loss) or 1 (Win):\n\n\n\n\n\n\n\n\n\n\n\nAnd similar to our previous examples, we can fit a linear regression model for this data\n\n\n\n\n\n\nR Code\n\n\n\n\nlm(Win ~ Diff, df4)\n\n\nCall:\nlm(formula = Win ~ Diff, data = df4)\n\nCoefficients:\n(Intercept)         Diff  \n    0.32123      0.01336  \n\n\n\n\n\\[\\text{Win}=0.321+0.013(\\text{Ratings Difference})\\]\nIf Win was a numeric outcome, we would interpret the coefficient in the same way as in standard linear regression. For example:\n\nFor each additional one-point increase in Ratings Difference, Win increases by 0.013.\n\nHowever, this interpretation does not make much sense in this context. The reason is that Win is a binary variable — it can only take the values 0 (loss) or 1 (win). There is no meaningful way for Win to increase by 0.013 in an observed match; a team either wins or it does not.\nInstead, when using a linear probability model (LPM), we reinterpret the outcome variable. Although Win is coded as 0 or 1, the expected value of Win represents a probability:\n\\[E(\\text{Win})=P(\\text{Win}=1)\\]\nThis means the model is not predicting individual wins or losses directly, but rather the probability of winning. Under this interpretation, the coefficient is understood as a change in probability:\n\nFor each additional point in Ratings Difference, the chance of winning (Win = 1) increases by 0.013.\n\nThis interpretation is intuitive and one of the main reasons the linear probability model is often used as a first step in classification problems. It allows us to:\n\ninterpret coefficients in a familiar linear regression framework, and\ndirectly link predictors to changes in probability.\n\n\n\n\n\n\n\nCaution\n\n\n\nHowever, this convenience comes at a cost. Because the model is linear, it can predict probabilities less than 0 or greater than 1, which are not meaningful. For example, suppose we had the following scenario:\n\n\\(Rating_\\text{Team A}=1500\\)\n\\(Rating_\\text{Team B}=1425\\)\n\nIn this case, the Ratings Difference is \\(1500-1425=75\\). Therefore, using our model, we would predict the chance of winning to be:\n\\[\n\\begin{aligned}\n\\text{Win} &=0.321 + 0.013(\\text{Ratings Difference}) \\\\\n&=0.321 + 0.013(75) \\\\\n&=1.323\n\\end{aligned}\n\\]\nClearly, this would not be possible (likewise, we could also have negative probability values as well). In the next chapter, we will learn about a statistical test that can deal with this limitation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#thresholds",
    "href": "09-Classification-and-prediction.html#thresholds",
    "title": "9  Classification and Prediction",
    "section": "9.6 Thresholds",
    "text": "9.6 Thresholds\nWhen using models for classification, the output is often a probability, not a direct class label. For example, a linear probability model might estimate that a team has a 0.63 probability of winning a match. On its own, this probability is informative — but in many practical situations, we must go one step further and make a decision: win or loss, approve or reject, intervene or not. To turn predicted probabilities into class labels, we must choose a classification threshold. A threshold specifies the probability value above which we predict one class (typically coded as 1) and below which we predict the other (coded as 0). A common default is 0.5, but this choice is not universal and is often not optimal.\nLet’s revisit the example from earlier. Using the derived model we can determine the predicted probability of Win for each observation.\n\n\n\nLet’s also plot these predictions (see Figure below). A threshold is then chosen (default = 0.50) to help us classify the predictions. Predictions below the threshold would have a predicted classification of 0. Whereas, predictions above the threshold would have a predicted classification of 1\n\n\n\n\n\n\n\n\n\n\n\nUsing this threshold, we can see which observation would be classified as a 0 and which ones would be classified a 1:\n\n\nWinR.DiffPredictionClass (Thresold = 0.50)0-170.09000-30-0.0800100.32101180.56211310.73511460.93610100.4550050.38800200.5881\n\n\nTo see how well the model did at making predictions with this threshold, we can compare the actual outcome (Win) to the predicted class. We can see in most cases, the Actual and the Predicted outcomes are the same. But in some other cases, they are not the same (which means that the prediction is incorrect). Specifically, we cab see two predictions at this threshold which are incorrect:\n\n\nWinR.DiffPredictionClass (Thresold = 0.50)Correct Prediction?0-170.0900Yes0-30-0.0800Yes100.3210No1180.5621Yes1310.7351Yes1460.9361Yes0100.4550Yes050.3880Yes0200.5881No\n\n\nWhen evaluating a binary classification model, results are often summarised in a 2×2 table, sometimes called a confusion matrix. This table compares what the model predicted with what actually occurred. Each cell in the table corresponds to a specific type of outcome, and understanding these outcomes is essential for interpreting model performance.\n\n\n\nPredicted\nActual\nTotal\n\n\n\n\nWin\nLoss\n\n\n\nWin\n3\n1\n\n\nLoss\n1\n4\n\n\nTotal\n4\n5\n\n\n\nFrom this table, we can easily see that:\n\nOf the 4 predicted ‘Win’ outcomes, 3 was correct and 1 was incorrect\nOf the 5 predicted ‘Loss’ outcomes, 1 was incorrect and 4 were correct",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#type-1-and-type-2-errors",
    "href": "09-Classification-and-prediction.html#type-1-and-type-2-errors",
    "title": "9  Classification and Prediction",
    "section": "9.7 Type 1 and Type 2 Errors",
    "text": "9.7 Type 1 and Type 2 Errors\nWhenever we use a model to make a classification decision, there is the possibility of making a mistake. In binary classification problems, these mistakes fall into two distinct types, known as Type I and Type II errors. Understanding the difference between them is essential, because not all errors have the same consequences.\nA Type I error occurs when the model predicts a positive outcome when the true outcome is negative. In other words, we classify an observation as belonging to class 1 when it actually belongs to class 0. In the example above, this would mean predicting a win when the team in fact loses. This type of mistake is commonly called a false positive.\nA Type II error occurs in the opposite situation: when the model predicts a negative outcome even though the true outcome is positive. That is, we classify an observation as class 0 when it truly belongs to class 1. In our football example, this corresponds to predicting a loss when the team actually wins. This type of mistake is commonly called a false negative.\nIn our current example:\n\n\n\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n3\n1\n\n\nPred Loss\n1\n4\n\n\n\n\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\nTrue Positive\nFalse Positive\n\n\nPred Loss\nFalse Negative\nTrue Negative\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTrue Positives (TP)\nA true positive occurs when the model predicts a positive outcome and the actual outcome is also positive. In other words, the model correctly identifies a case that truly belongs to class 1. For example, in a football context, a true positive would be predicting a win when the team actually wins. True positives represent correct positive classifications.\nFalse Positives (FP)\nA false positive occurs when the model predicts a positive outcome, but the actual outcome is negative. Here, the model signals a win when the team in fact loses. False positives are often associated with Type I errors. While the model appears confident, it has incorrectly classified a negative case as positive.\nTrue Negatives (TN)\nA true negative occurs when the model predicts a negative outcome and the actual outcome is also negative. This corresponds to correctly predicting a loss when the team does indeed lose. True negatives represent correct negative classifications, and together with true positives, they contribute to overall model accuracy.\nFalse Negatives (FN)\nA false negative occurs when the model predicts a negative outcome even though the actual outcome is positive. In this case, the model predicts a loss when the team actually wins. False negatives are associated with Type II errors and reflect situations where the model fails to detect a true positive outcome.\n\n\nConfusion matrixes can be useful in comparing the classification accuracy of different models. For example, let’s compare the two models below, which generated the following confusion matrixes:\n\n\n\nModel1\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n242\n47\n\n\nPred Loss\n51\n210\n\n\n\n\nModel2\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n51\n248\n\n\nPred Loss\n263\n62\n\n\n\n\nIn this example, it is clear that Model 1 (more True Positives and True Negatives) is better than Model 2 (which has more False Positives and False Negatives).\nBut sometimes it isn’t so obvious. For example, consider the following models and their respective confusion matrixes:\n\n\n\nModel1\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n242\n47\n\n\nPred Loss\n51\n210\n\n\n\n\nModel3\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n239\n40\n\n\nPred Loss\n32\n141\n\n\n\n\nThis is where the following formulas can help us in assessing the errors in our predictions.\n\n9.7.1 False Positive Rate (Type I Error)\nThe false positive rate (also called Type I Error) tells us the proportion of negative cases that were incorrectly predicted as positive (in this example, losses incorrectly predicted as wins), and can be calculated with the following formula:\n\\[\\text{False Positive Rate (Type I Error)}=\\frac{\\text{False Positive}}{\\text{False Positive + True Negative}}\\]\nIf we consider Model1 and Model3 above, the False Positive rate for these two would be:\n\\[\n\\begin{aligned}\n\\text{False Positive Rate}_\\text{Model 1} &= \\frac{47}{47+210} \\\\\n&= \\frac{47}{257} \\\\\n&= 0.183\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{False Positive Rate}_\\text{Model 3} &= \\frac{40}{40+141} \\\\\n&= \\frac{40}{181} \\\\\n&= 0.221\n\\end{aligned}\n\\]\nHere we can see that Model 1 has a lower False Positive Rate (which means it has made fewer errors in predicting wins)\n\n\n9.7.2 False Negative Rate (Type II Error)\nWe also have the false negative rate (also called Type II Error), which tells us the proportion of positive cases that were incorrectly predicted as negative (i.e. wins incorrectly predicted as losses). The False Negative Rate is given by:\n\\[\\text{False Negative Rate (Type II Error)}=\\frac{\\text{False Negative}}{\\text{False Negative + True Positive}}\\]\nFor our two models, the False Negative Rate are:\n\\[\n\\begin{aligned}\n\\text{False Negative Rate}_\\text{Model 1} &= \\frac{51}{51+242} \\\\\n&= \\frac{51}{293} \\\\\n&= 0.174\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{False Negative Rate}_\\text{Model 3} &= \\frac{32}{32+239} \\\\\n&= \\frac{32}{271} \\\\\n&= 0.118\n\\end{aligned}\n\\]\nHere we can see that Model 3 has a lower False Negative Rate (which means it has made fewer errors in predicting losses).\n\n\n9.7.3 Choosing Models based on Type I/II Errors\nIf we compare the two models above, we can see that:\n\nThe False Positive Rate is better for Model 1 than Model 3.\nBy comparison, the False Negative Rate is better for Model 3 than Model 1.\n\nWhich model we choose depends on what types of mistakes matter most in the context of the decision being made. In many real-world situations, decisions are binary, and the outcome of a decision can be either correct or incorrect when compared to what actually happens. These incorrect decisions map directly onto the ideas of false positives and false negatives that we have just introduced.\nConsider the two situations below. In each case, a decision must be made, and an incorrect decision can occur in one of two ways. However, the consequences of these errors are not the same. As you read through each scenario, think carefully about which type of error would be more acceptable—or less harmful—to make, and why. This exercise highlights an important idea in classification: there is rarely a single “best” model in all situations. Instead, the most appropriate model is the one that balances errors in a way that aligns with the goals and risks of the problem at hand.\n\nA Courtcase DecisionA Medical Test\n\n\n\n\n\n\nConsider a court case situation where a defendant is on trial. The defendant is either Guilty or Not Guilty of the charges laid against them, and the judge has to make a decision (guilty or not guilty) based on the evidence presented:\n\nA correct guilty verdict would be a TRUE POSITIVE\nA correct not guilty verdict would be a TRUE NEGATIVE\nHowever, if they judge concludes Guilty, but the defendant was actually Innocent, then this would be a FALSE POSITIVE\nConversely, if the judge concludes Not guilty, but the defendant actually was Guilty, then this would be a FALSE NEGATIVE.\n\nIn this situation, the consequences of incorrect decisions would be:\n\nFalse Positive: Innocent person goes to jail\nFalse Negative: Guilty person walks free\n\nIn your opinion, which error is worse to make?\n\n\n\n\n\n\nLet us now consider a different example that involves cancer patients. Suppose a patient has been advised to take a test to screen for lung cancer, and the test will tell the patient one of tweo outcomes (cancer or no cancer). Here:\n\nA correct cancer diagnosis would be a TRUE POSITIVE\nA correct non-cancer diagnosis would be a TRUE NEGATIVE\nIf the test says ‘Cancer’, but the patient doesn’t have Cancer, then this would be a FALSE POSITIVE\nIf the test says ‘No Cancer’, but the patient does have Cancer, then this would be a FALSE NEGATIVE\n\nIn this situation, the consequences of incorrect test diagnosis would be:\n\nFalse Positive: Stress, Worrying\nFalse Negative: Condition becomes worse (because not treated early)\n\nIn your opinion, which error is worse to make?\n\n\n\n\nIn the two examples above, we can see that different incorrect decisions can have largely different impacts. If correctly identifying positives is more important (e.g. detecting Cancer), then we would choose models that emphasise False Positive Rates. On the other hand if it’s preferable to have a guilty person walk through (as opposed to an innocent person going to jail), then we might choose a model that emphasises False Negative Rates.\n\n\n9.7.4 Overall Accuracy\nApart from the false positive and false negative rates, we can also calculate overall accuracy, which measures the proportion of all predictions that are correct.\n\\[\\text{Overall Accuracy}=\\frac{\\text{True Positive + True Negative}}{\\text{True Positive + False Positive + True Negative + False Negative}}\\]\nWhile accuracy is simple and intuitive, it is often less informative on its own. In practice, it is more common to keep these measures separate because accuracy hides the type of mistakes being made. Two models can have the same overall accuracy but very different balances of false positives and false negatives. In many real-world applications, these errors do not carry the same consequences, and treating them as equally important can lead to poor decision-making.\nBy examining false positive and false negative rates separately, we gain a clearer understanding of how a model is making errors, not just how often it is wrong. This allows us to choose models and classification thresholds that align with the priorities and risks of the problem, rather than relying on a single summary number.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#choosing-different-thresholds",
    "href": "09-Classification-and-prediction.html#choosing-different-thresholds",
    "title": "9  Classification and Prediction",
    "section": "9.8 Choosing different thresholds",
    "text": "9.8 Choosing different thresholds\nIn the examples so far, we have used a default threshold of 0.5 to convert predicted probabilities into class labels. That is, if the predicted probability of a win is greater than or equal to 0.5, we predict Win; otherwise, we predict Loss. While this rule is common, it is not mandatory and is often not the best choice.\nThe classification threshold controls the trade-off between false positives and false negatives. Lowering the threshold makes it easier for observations to be classified as positive, which typically increases the number of true positives but also increases false positives. Raising the threshold has the opposite effect: fewer positives are predicted, reducing false positives but increasing false negatives.\nWhich threshold is most appropriate depends on the context of the decision. In situations where false positives are costly—such as incorrectly convicting an innocent person or falsely flagging a legitimate transaction as fraud—a higher threshold may be preferred. In contrast, when missing a true positive is more serious—such as failing to detect a medical condition or overlooking a high-risk case—a lower threshold may be more appropriate.\nImportantly, changing the threshold does not change the underlying model or the predicted probabilities. It only changes how those probabilities are translated into decisions. This means that even with the same model, different stakeholders might reasonably choose different thresholds depending on their priorities and tolerance for risk.\nAs an example, let us consider the data from earlier. In the figure below we have the same data plotted with three different threshold (as the dashed line) values: 0.25, 0.50 and 0.75 respectively.\n\n\n\n\n\n\n\n\n\nNow depending on which threshold we decide on, how we classify each observation can vary:\n\n\nActualClassification (0.25)Classification (0.50)Classification (0.75)000000001100111011101111010001000110\n\n\nIf we compare each classification to the actual value, then we can see which predictions were correct under each of these three thresholds:\n\n\nActualClassification (0.25)Classification (0.50)Classification (0.75)0CorrectCorrectCorrect0CorrectCorrectCorrect1CorrectIncorrectIncorrect1CorrectCorrectIncorrect1CorrectCorrectIncorrect1CorrectCorrectCorrect0IncorrectCorrectCorrect0IncorrectCorrectCorrect0IncorrectIncorrectCorrect\n\n\nThis allows us to create confusion matrixes for each threshold:\n\n\n\nThreshold = 0.25\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n4\n3\n\n\nPred Loss\n0\n2\n\n\n\n\nThreshold = 0.50\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n3\n1\n\n\nPred Loss\n1\n4\n\n\n\n\nThreshold = 0.75\n\n\n\nActual Win\nActual Loss\n\n\n\n\nPred Win\n1\n0\n\n\nPred Loss\n4\n5\n\n\n\n\nFinally, we can compute the False Positive and False Negative rates to make a decision on which threshold we would like to choose\nFor the False Positive Rates:\n\\[\n\\begin{aligned}\n\\text{False Positive Rate}_\\text{(0.25)} &= \\frac{3}{3+2}=0.60 \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\text{False Positive Rate}_\\text{(0.50)} &= \\frac{1}{1+4}=0.20 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{False Positive Rate}_\\text{(0.75)} &= \\frac{0}{0+5}=0.00 \\\\\n\\end{aligned}\n\\]\nAnd for the False Negatives:\n\\[\n\\begin{aligned}\n\\text{False Negative Rate}_\\text{(0.25)} &= \\frac{0}{0+4}=0.00 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{False Negative Rate}_\\text{(0.50)} &= \\frac{1}{1+3}=0.25 \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{False Negative Rate}_\\text{(0.75)} &= \\frac{4}{4+1}=0.80 \\\\\n\\end{aligned}\n\\]\nNote here that because we are dealing with small samples, some of the error values are 0 - which is highly unlikely if we were to use a real data set!\nSo how do we decide which classification threshold to use? Rather than relying on a single default value, one approach is to examine how model performance changes across all possible thresholds. This idea underpins tools such as the Receiver Operating Characteristic (ROC) curve, which plots the trade-off between the true positive rate and the false positive rate as the threshold varies. The Area Under the Curve (AUC) then provides a single summary of how well the model separates the two classes, independent of any specific threshold choice.\nWhile ROC curves and AUC are widely used in practice, particularly in machine learning and diagnostic testing, they go beyond the scope of this unit. Instead, our focus in this unit is on understanding the conceptual trade-off between false positives and false negatives and choosing thresholds based on the context and consequences of decision-making. This perspective equips us to make informed, transparent classification choices, even without advanced optimisation tools.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#practice-with-r",
    "href": "09-Classification-and-prediction.html#practice-with-r",
    "title": "9  Classification and Prediction",
    "section": "9.9 Practice with R",
    "text": "9.9 Practice with R\nLet’s now work through a scenario that includes a proper data set.\n\n9.9.1 Context\nIn poor, developing countries often the government wants to target support to poor households, to help them with basic needs like food, health care, and education. To make the best use of government money, the government needs to find a quick way of classifying a household as poor or not poor. Based on this classification, they may target support to the households who are poor.\nConsider this example from Timor-Leste, a small country of 1.4 million people just north of Australia. The poverty rate in Timor-Leste is quite high (42% at last measurement), and the Government needs to target support to poor households. The question is:\n\nHow do they go about selecting the poor households?\n\n\n\n9.9.2 The Data\nHere we will use the “Timor-Leste Survey of Living Standards (TLSLS) undertaken in 2014, by the Government Statistics office, supported by the World Bank. This survey covered almost 6,000 households from across the country, carefully chosen to represent all households in the country, by using random selection and other more sophisticated methods.\nThis survey is very thorough: households are interviewed intensely, over almost 2 days, and asked to record all their food consumption and other spending for the past week. Based on this comprehensive data, data analysts can come up with a definite conclusion about whether the household is Poor or Not Poor. The concept of poverty that is used is known as Consumption Poverty, where if a household’s consumption is below a certain value (the Poverty Line), they classify as poor.\nIn total, we have a big dataset of almost 6,000 households. For each household, we know whether they are poor or not, and we also know a large range of characteristics of these households – their location, the family composition, quality of housing, everybody’s level of education, what assets they own, what job the adults have, and even things about people’s health.\nIn this example, we will only import some of the hundreds of variables in the dataset, the variables we have decided to use in the predictive model.\n\n\n9.9.3 Download the data\nThe data set is stored in a R data format file named timor.RDS. Note: this is not an excel file, so you won’t be able to open it in Excel. Begin by downloading the file and moving it your working directory.\n\n\n\n timor.RDS\n\n\n\n\n\n9.9.4 Load the data in RStudio\nNext, let us load the data into our working environment in RStudio. Because the data file is a .RDS file, we will use the readRDS() function. In the code below, I am saving the file as ‘timor’.\n\n\n\n\n\n\nR Code\n\n\n\n\ntimor &lt;- readRDS(\"timor.RDS\")\n\n\n\n\n\n9.9.5 Load the tidyverse\nUse the library() function to load the tidyverse package. This will provide us with all of the tools we need to complete this exercise.\n\n\n\n\n\n\nR Code\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\n\n9.9.6 Inspect the data\nRun the str() function to display the structure of the data frame:\n\n\n\n\n\n\nR Code\n\n\n\n\nstr(timor)\n\n'data.frame':   5916 obs. of  5 variables:\n $ poor     : num  0 1 0 0 0 1 1 0 0 0 ...\n $ hhsize   : int  7 7 8 2 4 10 5 1 6 4 ...\n $ district : chr  \"Aileu\" \"Baucau\" \"Dili\" \"Viqueque\" ...\n $ urban    : num  1 0 1 0 0 1 0 0 1 0 ...\n $ dirtfloor: num  0 0 0 1 0 0 1 1 1 1 ...\n\n\n\n\nThere are 5916 observations and five variables:\n\npoor: whether a household officially classifies as poor; 1 indicates poor, and 0 otherwise\nhhsize: household size\ndistrict: the district where a household locates\nurban: whether a household is from an urban area; 1indicates yes, and 0 otherwise (rural)\ndirtfloor: whether a household has a dirt floor; 1 indicates yes and 0 otherwise\n\n\n\n9.9.7 Training and Testing sets\nLet’s now create the model that can be used to predict whether a household is poor, based on these four characteristics.\nFirst, we will split the data into two subsets: a training set and test set. There are 5916 households in the data set. It’s common practice to randomly split your data into the training and testing sets, however in this exercise we will specify the first 70% (4141) households as training data, and the remaining 30% for testing.\nWe can use the slice() function to slice our data into parts. Remember that our data set has 5916 households. therefore:\n\nHouse 1 to House 4141 will be allocated to the training set\nHouse 4142 to House 5916 will be allocated to the testing set\n\n\n\n\n\n\n\nR Code\n\n\n\n\ntrain &lt;- timor |&gt; slice(1:4141)\ntest &lt;- timor |&gt; slice(4142:5916)\n\n\n\nTo make sure you have done this correctly, check that you have two objects (train and test) stored in your global environment.\n\n\n9.9.8 Building the model\nLet’s now build a model on the training data. Remember our goal here is to build a model to predict whether or not a household might be poor (the poor variable) based on the 4 variables we have available.\n\n\n\n\n\n\nR Code\n\n\n\n\ntimor_model &lt;- lm(poor ~ hhsize + district + urban + dirtfloor, train)\n\n\n\nWe can now use the summary() function to inspect this model\n\n\n\n\n\n\nR Code\n\n\n\n\n\n\nCall:\nlm(formula = poor ~ hhsize + district + urban + dirtfloor, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.53529 -0.29741 -0.09162  0.34359  1.08261 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.299783   0.033717  -8.891  &lt; 2e-16 ***\nhhsize            0.084698   0.002439  34.726  &lt; 2e-16 ***\ndistrictAinaro    0.057872   0.037928   1.526 0.127126    \ndistrictBaucau   -0.001921   0.035062  -0.055 0.956303    \ndistrictBobonaro  0.209155   0.035955   5.817 6.44e-09 ***\ndistrictCovalima  0.263496   0.037139   7.095 1.52e-12 ***\ndistrictDili      0.056406   0.033856   1.666 0.095777 .  \ndistrictErmera    0.110362   0.036710   3.006 0.002660 ** \ndistrictLautem   -0.005024   0.037320  -0.135 0.892911    \ndistrictLiqui?a   0.143246   0.037530   3.817 0.000137 ***\ndistrictManatuto  0.061082   0.037881   1.612 0.106935    \ndistrictManufahi  0.146757   0.037924   3.870 0.000111 ***\ndistrictOecussi   0.323493   0.036437   8.878  &lt; 2e-16 ***\ndistrictViqueque -0.019185   0.036352  -0.528 0.597693    \nurban            -0.116596   0.014212  -8.204 3.07e-16 ***\ndirtfloor         0.196339   0.013642  14.393  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3958 on 4125 degrees of freedom\nMultiple R-squared:  0.2867,    Adjusted R-squared:  0.2841 \nF-statistic: 110.5 on 15 and 4125 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n9.9.9 Making predictions\nWith our model, we can now generate predictions (probabilities between 0 and 1) for each household, representing how likely it is that the model believes the household to be poor (poor = 1) or not poor (poor = 0). To do this, we can use the predict() function.\nYou will need to pass in two arguments to run the predict function:\n\nThe object (in this case: our model from the previous step)\nThe data we are predicting on (in this case: the test data)\n\nIn the code below, we are storing each prediction (1 for each observation in the test set) as an object called ‘predictions’.\n\n\n\n\n\n\nR Code\n\n\n\n\npredictions &lt;- predict(timor_model, test)\n\n\n\n\n\n9.9.10 Combining Actual and Predictions\nWe can now create a data set that combines our actual values (the poor variable) with the predictions we generated in the previous step. There are a number of ways to do this, and in this example we will use the mutate() function. In the code below, we can read this in plain English as:\n\nStart with the test data\nCreate a new variable called preds based on the predictions object\nCreate a new variable called class that classifies the predictions into either ‘poor’ or ‘not poor’ using a threshold of 0.50.\nCreate a new variable called actual that provides text labels (instead of 0s and 1s) for the poor variable.\n\nI’ll also save this as ‘res’.\n\n\n\n\n\n\nR Code\n\n\n\n\nres &lt;- \n  test |&gt; \n  mutate(preds = predictions) |&gt; \n  mutate(class = ifelse(preds &gt; 0.5, \"poor\", \"not poor\")) |&gt; \n  mutate(actual = ifelse(poor == 1, \"poor\", \"not poor\"))\n\n\n\n\n\n9.9.11 Confusion Matrix\nWe can now use the table() function to create a confusion matrix of our actual and predicted classifications:\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(Prediction = res$class, Actual = res$actual)\n\n          Actual\nPrediction not poor poor\n  not poor     1047  313\n  poor          128  287\n\n\n\n\n\n\n9.9.12 Reorder factor levels (optional)\nNote that in the confusion matrix above the negative class (not poor) is on top of the positive class (poor). This is because the table() function goes in alphabetical order. If we wanted the table the other way around we would need to reorder the variables. We can do this with the fct_relevel() function.\nIn the code below, I create a new data frame (called res2) which is the same as the previous, just with the variables reordered so that ‘not poor’ goes before ‘poor’.\n\n\n\n\n\n\nR Code\n\n\n\n\nres2 &lt;- \n  res |&gt; \n  mutate(actual = fct_relevel(actual, \"poor\", \"not poor\"),\n         class = fct_relevel(class, \"poor\", \"not poor\"))\n\n\n\nWe can now rerun the table() function with the new data (res2):\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(Prediction = res2$class, Actual = res2$actual)\n\n          Actual\nPrediction poor not poor\n  poor      287      128\n  not poor  313     1047\n\n\n\n\n\n\n9.9.13 Calculating FPR / FNR\nFrom our table, we can see determine:\n\nTrue Positive = 287\nTrue Negative = 1047\nFalse Positive = 128\nFalse Negative = 313\n\nAnd, we could use R as a calculator to determine various metrics, for example:\n\n\n\n\n\n\nR Code\n\n\n\n\n# Overall Accuracy\n(287+1047) / (287+313+128+1047)\n\n[1] 0.7515493\n\n\n\n# Overall Error\n(128+313) / (287+313+128+1047)\n\n[1] 0.2484507\n\n\n\n# FPR (Type I Error)\n128/(128+1047)\n\n[1] 0.1089362\n\n\n\n# FNR (Type II Error)\n313/(313+287)\n\n[1] 0.5216667\n\n\n\n\nIn practice, it would be better to store these values as objects in our global environment. For example we can save the confusion matrix as an object (in the code below saved as tbl) and then extract out each cell of the table (based upon their position within the table).\n\n\n\n\n\n\nR Code\n\n\n\n\ntbl &lt;- table(Prediction = res2$class, Actual = res2$actual)\n\n# extract and save the top left cell and save it as 'TP'\nTP &lt;- tbl[1]\n\n# extract and save the bottom left cell and save it as 'FN'\nFN &lt;- tbl[2]\n\n# extract and save the top left cell and save it as 'FP'\nFP &lt;- tbl[3]\n\n# extract and save the bottom left cell and save it as 'TN'\nTN &lt;- tbl[4]\n\n\n\nNow, we can set up our formulas to calculate the different error terms using the stored objects, rather than individual numbers. Check that these values match with what we had previously.\n\n\n\n\n\n\nR Code\n\n\n\n\n# Overall Accuracy\n(TP + TN) / (TP + FP + FN + TN)\n\n[1] 0.7515493\n\n\n\n# Overall Error\n(FP + FN) / (TP + FP + FN + TN)\n\n[1] 0.2484507\n\n\n\n# FPR (Type I Error)\nFP/(FP+TN)\n\n[1] 0.1089362\n\n\n\n# FNR (Type II Error)\nFN/(FN+TP)\n\n[1] 0.5216667\n\n\n\n\nBut why is this important?\nFrom a coding perspective, the key advantage of setting things up this way is that we don’t need to manually re-enter numbers every time we change the threshold.\nBecause the calculations are written in terms of TP, FP, FN, and TN, the code does not change — only the threshold does. This makes the analysis:\n\nreproducible,\nless error-prone,\nand easy to extend (e.g. looping over many thresholds to see how FPR and FNR change).\n\nAs an example, I’m copied and pasted the same code from earlier in the section below. The only difference is that I’ve changed the value of the threshold to 0.40.\n\n\n\n\n\n\nR Code\n\n\n\n\nres &lt;- \n  test |&gt; \n  mutate(preds = predictions) |&gt; \n  mutate(class = ifelse(preds &gt; 0.4, \"poor\", \"not poor\")) |&gt; #Threshold changed\n  mutate(actual = ifelse(poor == 1, \"poor\", \"not poor\"))\n\nres2 &lt;- \n  res |&gt; \n  mutate(actual = fct_relevel(actual, \"poor\", \"not poor\"),\n         class = fct_relevel(class, \"poor\", \"not poor\"))\n\ntbl &lt;- table(Prediction = res2$class, Actual = res2$actual)\nTP &lt;- tbl[1]\nFN &lt;- tbl[2]\nFP &lt;- tbl[3]\nTN &lt;- tbl[4]\n\n\n# Overall Accuracy\n(TP + TN) / (TP + FP + FN + TN)\n\n[1] 0.7616901\n\n\n\n# Overall Error\n(FP + FN) / (TP + FP + FN + TN)\n\n[1] 0.2383099\n\n\n\n# FPR (Type I Error)\nFP/(FP+TN)\n\n[1] 0.2025532\n\n\n\n# FNR (Type II Error)\nFN/(FN+TP)\n\n[1] 0.3083333\n\n\n\n\nNotice how by simply changing one value (0.5 to 0.4), and then running the code again, quickly computes these metrics for us? This is the advantage of setting up our code in a reproducible format! Try it with different thresholds.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#summary",
    "href": "09-Classification-and-prediction.html#summary",
    "title": "9  Classification and Prediction",
    "section": "9.10 Summary",
    "text": "9.10 Summary\nIn this chapter, we extended the modelling ideas you already know from linear regression into the world of classification, where the outcome is not a number but a category (often coded as 0/1). While the goal changes—from predicting a continuous value like margin or income to predicting a class like win/loss or poor/not poor—the workflow remains familiar: we select predictors, fit a model using training data, and then evaluate how well that model performs on new observations.\nWe also introduced an important mindset shift: explain versus predict. Explanatory modelling focuses on interpreting coefficients to understand relationships, whereas predictive modelling focuses on generating accurate forecasts for unseen cases. This distinction becomes especially important in classification because a model can look “good” on the data it was trained on, but still perform poorly on new data. That is why we emphasised splitting data into training and testing sets, and evaluating performance using the test set.\nTo take our first step into classification modelling, we used a familiar tool in an unfamiliar way: the linear probability model (LPM). By coding outcomes as 0 and 1, the fitted values from a linear regression can be interpreted as predicted probabilities. This makes the LPM intuitive, because coefficients can be read as changes in probability (e.g., a one-unit increase in a predictor increases the chance of being in class 1 by some number of percentage points). At the same time, we highlighted why the LPM has limitations—most notably that it can produce probabilities below 0 or above 1—motivating the need for logistic regression in the next chapter.\nFinally, we focused on how probabilities become decisions. A predicted probability is useful, but many practical problems require a hard classification—so we choose a threshold (such as 0.5) to convert probabilities into predicted classes. Changing the threshold does not change the model, but it does change the confusion matrix and therefore the balance of false positives (Type I errors) and false negatives (Type II errors). This is why we used confusion matrices and calculated metrics like FPR and FNR: they help us understand what kind of mistakes a model is making, not just how often it is wrong. From a coding perspective, once we store the confusion matrix counts (TP, FP, FN, TN) as objects, we can change thresholds quickly and recompute performance without retyping numbers—making the whole workflow reproducible and scalable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "09-Classification-and-prediction.html#exercises",
    "href": "09-Classification-and-prediction.html#exercises",
    "title": "9  Classification and Prediction",
    "section": "9.11 Exercises",
    "text": "9.11 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nIn a linear probability model (LPM) where the outcome is coded 0/1, what does a fitted value \\(\\hat{y}\\) represent, and how do we interpret a slope coefficient?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nIn an LPM, the fitted value \\(\\hat{y}\\) is interpreted as an estimated probability that the outcome equals 1 (e.g., \\(P(Y=1)\\)). A slope coefficient tells us how that probability changes when the predictor increases by one unit. For example, if \\(\\beta_1=0.02\\), then a one-unit increase in the predictor is associated with a 2 percentage-point increase in the predicted probability of \\(Y=1\\) (holding other predictors constant).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhy do we need to choose a threshold when the model outputs predicted probabilities? What is the difference between a probability prediction and a class prediction?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nA model often outputs a probability (a number between 0 and 1), which describes how likely an observation is to belong to class 1. But many real decisions require a hard classification (class 0 or class 1). A threshold is the rule that converts probabilities into class labels (e.g., classify as 1 if \\(p \\ge0.5\\)).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nSuppose we lower the threshold from 0.50 to 0.40. What do you expect to happen to predicted positives, false positives, and false negatives (in general), and why?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nLowering the threshold makes it easier for an observation to be classified as positive (class 1). So:\n\nPredicted positives increase (more observations exceed the threshold).\nFalse positives tend to increase (more negative cases get labelled as positive).\nFalse negatives tend to decrease (fewer positive cases get missed).\n\nThis happens because changing the threshold shifts the decision boundary: you are trading off one type of error against the other.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nTwo models have the same overall accuracy. Explain why one model could still be preferred, using false positives vs false negatives.\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nAccuracy alone hides what kind of mistakes the model is making. Two models can have the same accuracy but very different balances of:\n\nFalse positives (Type I errors): predicting positive when the truth is negative\nFalse negatives (Type II errors): predicting negative when the truth is positive\n\nWhich model is better depends on the cost of errors in the real context. For example, in a medical screening context, a model with fewer false negatives may be preferred (missing a disease can be severe), even if its overall accuracy is the same as another model.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhy do we evaluate classification models on a test set instead of only using the training set? What does overfitting look like in classification?\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\nWe evaluate on a test set because it provides an estimate of performance on new, unseen data. A model can perform very well on training data simply because it has learned patterns that are specific to that sample, including noise.\nIn classification, overfitting looks like:\n\nvery high training accuracy (or very low training error),\nbut noticeably worse test accuracy (or higher test error).\n\nThis indicates the model has not learned general patterns that transfer well to new data\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nIt is commonly believed that young people are much more at risk of being involved in an accident than older drivers. In this exercise you will look at results based on a database with details of every death in road accidents in Australia over the period 1989 to 2020. Begin by downloading the accidents.csv file and load it into your RStudio.\n\n\n\n accidents.csv\n\n\n\n\nEstimate a regression model that explores the trend in proportion of young person fatalities over time using the following variables:\n\nDependent variable: Age (1 = young, 0 = not young)\nIndependent variable: t (time, where 0 is 1988)\n\n\nWhat is the average annual rate of decline in proportion of fatalities that are young people aged 17-25?\nUse the model to estimate the proportion of fatalities among young people in 1988.\nUse the model to predict the proportion of fatalities among young people in the year 2020\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\n\nmodel &lt;- lm(Age ~ t, accidents)\ncoef(model)\n\n (Intercept)            t \n 0.431778174 -0.006305964 \n\n\nThe beta coefficient for t is -0.0063. This tells us that the annual rate of decline is 0.633% per year.\nb.\nFrom the output in part a, the equation for this model is:\n\\[Age=0.4318-0.0063(t)\\]\nTherefore when t = 0 (which is the year 1988), \\(Age=0.4318\\). The estimated proportion of fatalities is 43.18%. Note: The intercept from the output also gives us the same answer.\nc.\nIf t = 0 is 1988, then t = 32 for the year 2020. Then:\n\\[Age=0.4318-0.0063(32)=0.23\\] Approximately 23% of fatalities attributed to young drivers.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nContinuing from the previous question, estimate a multiple regression model that explores the circumstances where a fatality is more likely to be a young person. Use:\n\nDependent variable: Age (1 = young, 0 = not young)\nIndependent variable:\n\nt (time, where 0 is 1988)\nMale (1 = male, 0 = otherwise)\nNight (1 = Accident occurred at night, 0 = otherwise)\nWeekend (1 = Accident occured on weekend, 0 = otherwise)\nSingle.Vehicle (1 = Accident involved only 1 vehicle, otherwise)\n\n\n\nCompare the R-Square values across the two models (question 6 and 7). Explain what R-Square is measuring, and comment on the differences between the two models.\nExplain how the interpretation of the coefficient of t is different in this model compared to the first model.\nDespite adding several variables to the model, the coefficient of t is very similar for both models. Can you think of a reason why this might be?\nRefer to the p-value column of values, Pr(&gt;|t|), and explain briefly what are the circumstances where the person killed is more likely to be a young person aged under 25.\nInterpret the coefficient of “Night”.\nInterpret the coefficient of “Weekend”\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\n\nmodel2 &lt;- lm(Age ~ t + Male + Night + Weekend + Single.Vehicle, accidents) \n\n\nsummary(model)$r.square\n\n[1] 0.01417989\n\nsummary(model2)$r.square\n\n[1] 0.05077797\n\n\nIn the first model, R2=1.41%, in the second model it is 5.08%, slightly bigger. R2 says the % of variation in Y that is explained by the set of X variables included in the regression model. The addition of several variables in the second model will increase the R2.\nb.\n\nsummary(model2)\n\n\nCall:\nlm(formula = Age ~ t + Male + Night + Weekend + Single.Vehicle, \n    data = accidents)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5784 -0.3547 -0.2457  0.5480  0.8817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.3164657  0.0054129   58.47   &lt;2e-16 ***\nt              -0.0061917  0.0002289  -27.05   &lt;2e-16 ***\nMale            0.0021519  0.0045819    0.47    0.639    \nNight           0.1165304  0.0043378   26.86   &lt;2e-16 ***\nWeekend         0.0583264  0.0043115   13.53   &lt;2e-16 ***\nSingle.Vehicle  0.0911346  0.0042440   21.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4624 on 51196 degrees of freedom\nMultiple R-squared:  0.05078,   Adjusted R-squared:  0.05069 \nF-statistic: 547.7 on 5 and 51196 DF,  p-value: &lt; 2.2e-16\n\n\nHere, we hold other X’s constant. i.e. Compare two accidents with same gender, day or night, weekday or weekend, single or multiple vehicles, the coefficient of t says the average annual decline in percent of fatalities aged 17-25 (i.e. young driver) is 0.00619.\nc.\nThe other variables have not changed much over time, and not systematically with age, so leaving them out of the first model does not affect the coefficient of t.\nd.\nMuch more likely at night, on weekend, and in single vehicle accident.\ne.\nIf an accident occurs at night time, there is an estimated 11.65% higher chance the person killed is a young person compared to daytime, holding other variables constant.\nf.\nIf an accident occurs on a weekend, there is an estimated 5.83% higher chance the person killed is a young person compared to on a weekday, holding other variables constant.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 8\n\n\n\nIn this exercise you will look at the issue of the “Youth Bulge” in a neighbouring developing country, Timor-Leste. The Youth Bulge describes a situation where there are many young people completing education and looking for work, but very few jobs for them. Your focus will be on young people aged 25-29, the age where most have finished their education, and in the early stages of their employment.\nBased on the 2015 Census, each of these young people have been put into one of 5 categories:\n\nFormal: working in a formal sector job\nFarmer: the person’s main work is as a self-employed farmer\nInfnonfarm: the person’s main work is as a self-employed informal sector activity\nUnemployed: the person is unemployed (doesn’t have a job, and wants a job)\nNot in Labour Force: the person is not working or looking for work – mostly students or fulltime parents\n\nThe Census also provides the following information about each person in the dataset:\n\nEnglish = 1 if the person can read, write and speak English\nPrimary = 1 if the highest level of education achieved is completing primary school\nSecondary = 1 if the highest level of education achieved is completing secondary school\nHigherEd = 1 if the highest level of education achieved is completing higher education\nMale = 1 if the person reports as male\nMarried = 1 if the person is married\nMother = 1 if the person is a mother\nDili = 1 if the person lives in the capital city, Dili\nNumdis = the number of disabilities the person has (people can have up to 4 disabilities: seeing, hearing, mobility, mental)\n\nFirst, download and load the data from the file “youthbulge.csv” into R and save it in a data frame labelled youthbulge.\n\n\n\n youthbulge.csv\n\n\n\n\nEstimate a multiple regression model with the dependent variable being formal, which takes the value 1 if the person has a formal sector job, and equals 0 if they do not. Include english, primary, secondary, highered, male, married, mother, dili and numdis as the independent variables.\n\nWhat do you learn from the p-values and coefficients on the 3 education dummy variables (NB remember the base category is a person with no education, or only some primary education). Explain carefully.\nBased on the model results, how much more likely is a married male to have a formal sector job compared to an unmarried male?\nBased on the model results, compare the likelihood having a formal job for a single female (not a mother) with that likelihood for a married female who is a mother. Explain your calculations briefly.\nUsing the coefficient of Numdis, what is the difference in likelihood of having a formal job between a person with 2 disabilities and a person with no disabilities, if all their other characteristics are the same?\nUse the model to predict the probability of formal employment for two types of people (just approximate prediction is fine, say 2 decimal places):\n\nan unmarried young woman with no children, no education, cannot speak English, has one disability, and lives outside Dili.\na married male with higher education qualifications, and can speak English, has no disabilities and lives in Dili.\n\nComment on the differences in probabilities you find here.\nA famous Timorese politician recently said “Yes, 27% of young males have formal sector jobs and only 16% of females, but this is not because of sexism. It is because males usually have higher levels of education. If we make the education system fairer, that will eliminate the gender differences in employment”. Use the multiple regression results here to carefully critique this claim. Explain your reasoning\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nmodel1 &lt;- lm(formal ~ english + primary + secondary + highered + male + married\n+ mother + dili + numdis, youthbulge)\n\nsummary(model1)\n\n\nCall:\nlm(formula = formal ~ english + primary + secondary + highered + \n    male + married + mother + dili + numdis, data = youthbulge)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51661 -0.25630 -0.13027 -0.03428  1.07519 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.034282   0.004240   8.086 6.22e-16 ***\nenglish      0.082441   0.003294  25.024  &lt; 2e-16 ***\nprimary      0.007282   0.004308   1.690   0.0910 .  \nsecondary    0.066413   0.003774  17.600  &lt; 2e-16 ***\nhighered     0.098652   0.004724  20.882  &lt; 2e-16 ***\nmale         0.059929   0.003622  16.545  &lt; 2e-16 ***\nmarried      0.115282   0.003059  37.689  &lt; 2e-16 ***\nmother      -0.085704   0.004304 -19.913  &lt; 2e-16 ***\ndili         0.126026   0.002926  43.073  &lt; 2e-16 ***\nnumdis      -0.023768   0.011718  -2.028   0.0425 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3932 on 92393 degrees of freedom\nMultiple R-squared:  0.08185,   Adjusted R-squared:  0.08176 \nF-statistic: 915.2 on 9 and 92393 DF,  p-value: &lt; 2.2e-16\n\n\na.\np-values are significant for Secondary and HigherEd, but NOT for Primary. So, evidence is that having Secondary or HigherEd significantly increases the chances of having a Formal sector job, but there is no evidence that Primary makes you more likely to have a Formal sector job than no education.\nThis is confirmed by the fact that the coefficient on Primary is very small – even if we had found significant evidence that primary education increases Pr(Formal), the effect is very small – estimated difference is 0.7%.\nSecondary educated person has a 6.6% greater chance of getting a formal sector job compared to someone with no education or less than primary school.\nPerson with HigherEd has a 9.9% greater chance of getting a formal sector job compared to someone with no education or less than primary school.\nBoth have strong positive effects on chances of a formal sector job, but HigherEd is clearly superior.\nb.\n11.5%, the coefficient on married, assuming other characteristics are the same.\nc.\nCoefficient of Mother + Married = 0.115 – 0.086 = 0.029. So, a married mother is 2.9% more likely to have a formal sector job compared to a single female who is not a mother\nd.\n-0.024 × 2 = -0.048. A person with 2 disabilities is 4.8 percentage points less likely to have a formal sector job, compared to someone with no disabilities with the same other characteristics.\ne.\n\nUnmarried woman Pr(Formal) = 0.034 – 0.024 = 0.01, 1% chance\nMarried male, etc Pr(Formal) = 0.034 + 0.082 + 0.099 + 0.060 + 0.115 + 0.126 = 0.51 approximately, or 51%.\n\nHuge difference, 51% compared to 1%. The female has multiple disadvantages that cumulate across them. The male is opposite, has all the benefits of location, education, no disability, gender, etc. This highlights how unequal the opportunities are.\nf.\nThe coefficient of Male is 0.06, so if we compare a male and a female with the same other characteristics, including education level, the male is 6 percentage points more likely than the female to have a formal sector job. This is a statistically significant difference (small p-value).\nSo, this contradicts the politician’s claim. Even when we compare people with the same education level, males are still much more likely to have a formal sector job. So, eliminating differences in educational attainment won’t eliminate the gender differences.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\n\nThe data file below contains a simulated data set containing information on 1,645 customers. The aim here is to predict which customers will default on their credit card debt.\n\ndefault: A factor with levels 0 = No and 1 = Yes indicating whether the customer defaulted on their debt\nstudent: A factor with levels No and Yes indicating whether the customer is a student\nbalance: The average balance that the customer has remaining on their credit card after making their monthly payment\nincome: Income of customer\n\n\n\n\n default.csv\n\n\n\n\n\nDownload and load the data file into R as ‘default’.\nSplit the data into training and testing sets. Use the first 1000 observations for training and the remaining for testing.\nConstruct a LPM for default with student, balance and income as independent variables.\nUse your model to make predictions on the test set.\nUse a threshold of 0.40 to classify your predictions into “No” and “Yes” for defaulting.\nConstruct a confusion matrix for this scenario.\nUsing your confusion matrix, compute the False Positive and False Negative rates.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\n# a. \ndefault &lt;- read.csv(\"data/Default.csv\")\n\n# b.\ntrain &lt;- default |&gt; slice(1:1000)\ntest &lt;- default |&gt; slice(1001:1645)\n\n# c.\nmodel_default &lt;- lm(default ~ student + balance + income, train)\n\n# d.\nclass_pred &lt;- predict(model_default, test)\n\n# e. \nres &lt;- test |&gt; \n  mutate(predictions = class_pred) |&gt; \n  \n  mutate(class = ifelse(predictions &lt; 0.40, \"No\", \"Yes\"),\n         actual = ifelse(default == 0, \"No\", \"Yes\")) |&gt; \n  \n  mutate(class = fct_relevel(class,\"Yes\",\"No\"),\n         actual = fct_relevel(actual,\"Yes\",\"No\"))\n\n# f.\ntbl &lt;- table(Predictions = res$class, Actual = res$actual)\n\n# g.\nTP &lt;- tbl[1]\nFN &lt;- tbl[2]\nFP &lt;- tbl[3]\nTN &lt;- tbl[4]\n\nFPR &lt;- FP/(FP+TN)\nFNR &lt;- FN/(FN+TP)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Classification and Prediction</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html",
    "href": "10-Logistic-regression.html",
    "title": "10  Logistic Regression",
    "section": "",
    "text": "10.1 Introduction\nIn the previous chapter, we explored classification and prediction, focusing on how to use a linear probability model (LPM) to handle categorical outcomes. In this approach, we treated the outcome (for example, win vs. lose, or success vs. failure) as a binary variable and fitted a standard linear regression model. Using this model, we generated predicted values and then applied a classification rule (such as assigning outcomes greater than 0.5 to “success” and those less than 0.5 to “failure”). While this approach gave us a first glimpse into prediction for categorical outcomes, we also discussed its limitations:\nThese issues make the linear probability model less reliable, especially when we want to use the model for decision-making or inference.\nIn this chapter, we will learn about logistic regression, a method specifically designed for categorical outcomes. Logistic regression overcomes the key problems of the linear probability model by ensuring that predicted probabilities always fall between 0 and 1 and by modelling the relationship between predictors and the outcome in a more flexible, non-linear way. It is one of the most widely used models in data science, business, and the social and health sciences for analysing binary and categorical data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#introduction",
    "href": "10-Logistic-regression.html#introduction",
    "title": "10  Logistic Regression",
    "section": "",
    "text": "Predicted probabilities from the LPM can fall outside the valid range of 0 to 1.\nThe relationship between predictors and the outcome is assumed to be linear, which is often unrealistic for probability data.\nThe errors in the model are heteroscedastic (the variance changes with the level of the predictors), violating a key assumption of linear regression.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#log-odds",
    "href": "10-Logistic-regression.html#log-odds",
    "title": "10  Logistic Regression",
    "section": "10.2 Log-odds",
    "text": "10.2 Log-odds\nLogistic regression models the log of the odds (log-odds) as a linear function of predictors. By working with odds, rather than probabilities directly, we can map any probability between 0 and 1 onto the entire real number line. This transformation removes the hard boundaries that caused problems for the linear probability model and allows us to use familiar linear modelling ideas in a setting where the outcome is binary.\nThis also changes how we interpret model coefficients. Instead of describing changes in probability, coefficients in a logistic regression describe how the odds of an outcome change when a predictor increases. While this may feel less intuitive at first, odds have a key advantage: they respond smoothly and consistently across the entire range of possible outcomes. Small changes near probabilities of 0 or 1 behave differently from changes near 0.5, and odds naturally capture this non-linear behaviour.\nIn the sections that follow, we will build up this idea step by step—first by understanding probability, odds and log-odds on their own, and then by showing how they form the foundation of a regression model that is better suited to classification problems than the linear probability model.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#recap-probability-and-the-complement-probability",
    "href": "10-Logistic-regression.html#recap-probability-and-the-complement-probability",
    "title": "10  Logistic Regression",
    "section": "10.3 Recap: Probability and the Complement Probability",
    "text": "10.3 Recap: Probability and the Complement Probability\nBefore we dive into logistic regression, it is useful to revisit some core probability concepts that underpin the model. Logistic regression is not built directly on probabilities, but instead on odds and log-odds (logits). Understanding these ideas will help us see why logistic regression is structured the way it is.\n\n10.3.1 Probability\nThe probability of an event measures the likelihood that the event will occur. It is always a value between 0 and 1:\n\\[P(\\text{Event})=\\frac{\\text{N outcomes where Event occurs}}{\\text{Total N of possible outcomes}}\\]\nFor example, the probability of drawing a clubs card from a standard deck of playing cards would be:\n\n\n\n\n\n10.3.2 Complement Probability\nThe complement probability (\\(A^c\\) or \\(A'\\)) represents the likelihood that the event does not occur. Using our example from above:\n\n\n\nThis can also be expressed as:\n\\[\n\\begin{aligned}\nP(\\text{not clubs}) &= 1-P(\\text{clubs}) \\\\\n   &= 1 - \\frac{13}{52} \\\\\n   &= \\frac{39}{52} \\\\\n   &= \\frac{3}{4}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#odds-and-the-odds-ratio",
    "href": "10-Logistic-regression.html#odds-and-the-odds-ratio",
    "title": "10  Logistic Regression",
    "section": "10.4 Odds and the Odds Ratio",
    "text": "10.4 Odds and the Odds Ratio\nWhile probabilities are intuitive and easy to interpret, they are not the only way to describe uncertainty. Another closely related way of expressing likelihood is through odds. Instead of measuring how likely an event is to occur on a 0–1 scale, odds compare the chance that an event occurs to the chance that it does not occur.\n\n10.4.1 Odds\nThe odds of an event are the ratio of the probability the event happens to the probability it does not happen (which are two terms we just recapped in the section above):\n\\[\n\\begin{aligned}\nOdds(\\text{Event}) &= \\frac{P(\\text{Event})}{1-P(\\text{Event})} \\\\\n\\end{aligned}\n\\]\nSo, in our current example:\n\n\n\nThis means the odds of drawing a clubs card are 1 to 3.\n\n\n10.4.2 Odds Ratio\nAn odds ratio (OR) compares the odds of an event occurring in one group to the odds of it occurring in another group:\n\\[OR=\\frac{Odds(A)}{Odds(B)}\\] For example, suppose two basketball teams have the following free-throw success probabilities:\n\n\\(P(\\text{Team}_A)=0.80\\)\n\\(P(\\text{Team}_B)=0.60\\)\n\nThe first step is to calculate the odds for each team:\n\n\\(Odds(A)=\\frac{0.80}{1-0.80}=4\\)\n\\(Odds(B)=\\frac{0.60}{1-0.60}=1.5\\)\n\nThen the Odds Ratio (OR) would be:\n\\[OR=\\frac{4}{1.5}=2.67\\]\nThis means Team A’s odds of scoring a free throw are 2.67 times higher than Team B’s.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#the-logit-function",
    "href": "10-Logistic-regression.html#the-logit-function",
    "title": "10  Logistic Regression",
    "section": "10.5 The Logit Function",
    "text": "10.5 The Logit Function\nLogistic regression is built on the idea that, rather than modelling probability directly, we model the log of the odds of an event occurring. This transformation is called the logit function, and it solves the major limitations we faced with the linear probability model.\n\n10.5.1 The Problem With Modeling Probabilities Directly\nRecall that a probability, \\(p\\), always lies between 0 and 1. But a linear regression model does not respect this restriction — it can produce predictions less than 0 or greater than 1. For example, if we tried to model “probability of winning a game” with a linear function like:\n\\[p=\\beta_0+\\beta_1X\\]\nthere is no guarantee that the predicted p will stay in the 0–1 range.\n\n\n\n\n\n\n\n\n\n\n\nWe need a function that:\n\nKeeps predictions in the range (0, 1).\nStill allows us to use a linear equation for estimation and interpretation.\n\nThis is where the logit function comes in.\n\n\n10.5.2 From probabilities to odds and log-odds (the logit)\nA probability \\(p\\) is always between 0 and 1, but a linear model can produce predictions outside that range. To address this, logistic regression does not model \\(p\\) directly. Instead, it models a transformed version of \\(p\\) that can take any real value.\nStep 1 — Convert a probability to odds\nThe odds of an event compare “success” to “failure”:\n\\[\n\\text{Odds} = \\frac{p}{1-p}\n\\]\nYou can read odds as:\n\n“How many times more likely is success than failure?”\n\nSome examples:\n\nIf \\(p = 0.50\\),\n\\[\n\\text{Odds} = \\frac{0.50}{1-0.50} = \\frac{0.50}{0.50} = 1\n\\]\nOdds = 1 means success and failure are equally likely (a 1:1 ratio).\nIf \\(p = 0.80\\),\n\\[\n\\text{Odds} = \\frac{0.80}{1-0.80} = \\frac{0.80}{0.20} = 4\n\\]\nOdds = 4 means success is four times as likely as failure (a 4:1 ratio).\nIf \\(p = 0.20\\),\n\\[\n\\text{Odds} = \\frac{0.20}{1-0.20} = \\frac{0.20}{0.80} = 0.25\n\\]\nOdds = 0.25 means success is one quarter as likely as failure — equivalently, failure is four times as likely as success (a 1:4 ratio).\n\n\nKey property: odds can take any value from \\(0\\) to \\(+\\infty\\).\n\nThis is an improvement over probabilities, but odds are still bounded below at 0 and are often highly skewed, which makes them unsuitable for direct linear modelling.\nStep 2 — Take the log of the odds (log-odds)\nTo remove the lower bound and spread values more symmetrically, we take the natural logarithm of the odds:\n\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\n\\]\nThis is known as the logit (or log-odds) transformation.\nThis transformation has several important properties:\n\nAs \\(p \\rightarrow 0\\), \\(\\text{logit}(p) \\rightarrow -\\infty\\)\nAs \\(p \\rightarrow 1\\), \\(\\text{logit}(p) \\rightarrow +\\infty\\)\nWhen \\(p = 0.5\\),\n\\[\n\\text{logit}(0.5) = \\ln\\left(\\frac{0.5}{0.5}\\right) = \\ln(1) = 0\n\\]\n\nSo:\n\nNegative logit values correspond to probabilities below 0.5\nPositive logit values correspond to probabilities above 0.5\nA logit of 0 corresponds to a 50–50 chance\n\nThe logit transformation maps the probability range \\((0,1)\\) onto the entire real line \\((-\\infty, +\\infty)\\). This allows us to use a linear model for the transformed outcome, while still guaranteeing that predicted probabilities (after transforming back) remain between 0 and 1.\nStep 3 — The logistic regression model\nIn logistic regression, we assume that the logit of the probability is a linear function of the predictors:\n\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\n= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k\n\\]\nThis looks just like an ordinary linear regression equation — except the left-hand side is not the outcome itself, but the log-odds of the probability of the outcome.\nIn words:\n\nLogistic regression assumes that the log-odds of success change linearly with the predictors.\n\nBy modelling log-odds rather than probabilities directly, logistic regression avoids impossible predictions and provides a principled way to connect linear models with binary outcomes.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#the-anti-logit-inverse-logit",
    "href": "10-Logistic-regression.html#the-anti-logit-inverse-logit",
    "title": "10  Logistic Regression",
    "section": "10.6 The Anti-Logit (Inverse Logit)",
    "text": "10.6 The Anti-Logit (Inverse Logit)\nIn the previous section, we introduced the logit function, which transforms a probability \\(p\\) into the log-odds of success. This transformation is what makes logistic regression possible, because it allows us to model a linear relationship on an unbounded scale.\nHowever, once we have estimated a logistic regression model, we usually do not want to interpret results in terms of log-odds. Probabilities are far more intuitive. To convert model outputs back to probabilities, we use the anti-logit (or inverse logit) function.\n\n10.6.1 The anti-logit function\nThe anti-logit function converts log-odds, denoted by \\(\\eta\\), back into a probability:\n\\[\np = \\frac{e^{\\eta}}{1 + e^{\\eta}}\n\\]\nHere, \\(\\eta\\) represents the linear predictor from the logistic regression model:\n\\[\n\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k\n\\]\nThis transformation ensures that, regardless of the value of \\(\\eta\\), the resulting probability is always valid.\n\n10.6.1.1 Properties of the Anti-Logit\nThe anti-logit function has several important properties that make logistic regression well suited for modelling binary outcomes.\n\n\n10.6.1.2 Range restriction\nNo matter what value \\(\\eta\\) takes (from \\(-\\infty\\) to \\(+\\infty\\)), the resulting probability always satisfies:\n\\[\n0 &lt; p &lt; 1\n\\]\nThis guarantees that predicted probabilities never fall outside the valid range.\n\n\n10.6.1.3 S-shaped (sigmoid) curve\nWhen plotted against \\(\\eta\\), the anti-logit function produces an S-shaped curve, also known as the logistic or sigmoid curve.\nIts behaviour at key values is:\n\nAs \\(\\eta \\rightarrow -\\infty\\), \\(p \\rightarrow 0\\)\nAs \\(\\eta \\rightarrow +\\infty\\), \\(p \\rightarrow 1\\)\nAt \\(\\eta = 0\\),\n\\[\np = \\frac{e^0}{1 + e^0} = \\frac{1}{2} = 0.5\n\\]\n\n\n\n10.6.1.4 Symmetry\nThe logistic curve is symmetric around \\(\\eta = 0\\).\nThis point corresponds to a probability of 0.5 and represents the location where the curve is steepest.\nTogether, these properties explain why logistic regression is so powerful: probabilities change smoothly with predictor values while never straying outside valid bounds.\nGraphically, the anti-logit transformation produces the logistic curve, which has three key regions:\n\nAt very low values of \\(\\eta\\), the curve flattens near 0\n\nAround \\(\\eta = 0\\), the curve is steepest, meaning small changes in predictors have the largest effect on probability\nAt very high values of \\(\\eta\\), the curve flattens near 1\n\n\n\n\n\n\n\n\n\n\n\n\nThis behaviour reflects many real-world processes. Once an outcome is almost impossible or almost certain, small changes in predictors have little impact. Logistic regression naturally captures this pattern while still allowing us to use a linear model on the log-odds scale.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#the-binomial-distribution",
    "href": "10-Logistic-regression.html#the-binomial-distribution",
    "title": "10  Logistic Regression",
    "section": "10.7 The Binomial Distribution",
    "text": "10.7 The Binomial Distribution\nSo far, we have focused on modelling binary outcomes — events that either occur or do not occur (such as pass/fail, win/loss, success/failure). To understand how logistic regression connects to probability theory, we now introduce the binomial distribution.\nThe binomial distribution describes the probability of observing a certain number of successes when:\n\nEach trial has only two possible outcomes (success or failure),\nThe probability of success, \\(p\\), is the same for every trial,\nThe trials are independent.\n\nThese conditions closely match many real-world situations, such as:\n\nPassing or failing an exam question,\nMaking or missing a free throw,\nWinning or losing a game.\n\n\n10.7.1 Bernoulli Trials\nBefore introducing the binomial distribution, it is helpful to start with the simplest case: a Bernoulli trial.\nA Bernoulli trial is a single experiment with:\n\nProbability of success \\(p\\),\nProbability of failure \\(1 - p\\).\n\nWe often code outcomes as:\n\n\\(Y = 1\\) for success,\n\\(Y = 0\\) for failure.\n\nIn this setting:\n\n\\(P(Y = 1) = p\\),\n\\(P(Y = 0) = 1 - p\\).\n\nLogistic regression models the probability \\(p\\) of success for a single Bernoulli trial.\n\n\n10.7.2 From Bernoulli to Binomial\nNow suppose we repeat a Bernoulli trial \\(n\\) times under identical conditions.\nLet:\n\n\\(n\\) = number of trials,\n\\(X\\) = number of successes observed.\n\nThen \\(X\\) follows a binomial distribution, written as:\n\\[\nX \\sim \\text{Binomial}(n, p)\n\\]\nThe binomial distribution gives the probability of observing exactly \\(k\\) successes out of \\(n\\) trials:\n\\[\nP(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\n\\]\nHere:\n\n\\(\\binom{n}{k}\\) counts the number of ways to choose \\(k\\) successes from \\(n\\) trials,\n\\(p^k\\) represents the probability of \\(k\\) successes,\n\\((1 - p)^{n - k}\\) represents the probability of \\(n - k\\) failures.\n\n\n\n10.7.3 Interpreting the Binomial Distribution\nThe binomial distribution answers questions such as:\n\nIf the probability of success is \\(p\\), how likely am I to observe \\(k\\) successes out of \\(n\\) attempts?\n\nFor example:\n\nIf a student has a 60% chance of answering any question correctly,\nAnd they answer 10 independent questions,\n\nthe binomial distribution tells us how likely it is they will get exactly 6 correct, 7 correct, or any other number.\nAs \\(n\\) increases:\n\nThe distribution becomes more concentrated around its average,\nRandom variation still exists, but extreme outcomes become less likely.\n\n\n\n10.7.4 Mean and Variability\nThe binomial distribution has two important summary measures:\n\n10.7.4.1 Expected number of successes\n\\[\n\\mathbb{E}(X) = np\n\\]\nThis represents the average number of successes we would expect if we repeated the experiment many times.\n\n\n10.7.4.2 Variance\n\\[\n\\text{Var}(X) = np(1 - p)\n\\]\nThe variability depends on both:\n\nThe number of trials \\(n\\),\nHow uncertain the outcome is (largest when \\(p = 0.5\\)).\n\n\n\n\n10.7.5 Why the Binomial Distribution Matters for Logistic Regression\nLogistic regression is built on the binomial distribution.\n\nEach observation is treated as a Bernoulli trial,\nThe probability of success \\(p_i\\) can vary across observations,\nThe logit link connects \\(p_i\\) to a linear predictor.\n\nRather than modelling the outcome directly, logistic regression models the probability parameter of a binomial process:\n\\[\nY_i \\sim \\text{Bernoulli}(p_i)\n\\]\n\\[\n\\text{logit}(p_i) = \\beta_0 + \\beta_1 X_{i1} + \\cdots + \\beta_k X_{ik}\n\\]\nIn this way:\n\nThe binomial distribution provides the probabilistic foundation,\nThe logistic model explains how predictors influence the probability of success.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#extending-to-r-from-lm-to-glm",
    "href": "10-Logistic-regression.html#extending-to-r-from-lm-to-glm",
    "title": "10  Logistic Regression",
    "section": "10.8 Extending to R: From lm() to glm()",
    "text": "10.8 Extending to R: From lm() to glm()\nUp to this point, we have fitted models in R using the lm() function. This function is designed for linear regression, where the outcome variable is continuous and normally distributed, and where predictions can take any real value.\nHowever, when our outcome is binary (such as pass/fail, win/loss, success/failure), linear regression is no longer appropriate. As we have seen, modelling probabilities directly with a linear model can lead to predicted values outside the valid range of 0 to 1.\nTo handle binary outcomes properly, R provides a more general modelling function: glm().\n\n10.8.1 Generalised Linear Models (GLMs)\nThe glm() function fits a generalised linear model. A GLM has three components:\n\nA random component\nThis specifies the distribution of the outcome variable (e.g. normal, binomial).\nA systematic component\nThis is the linear predictor: \\[\n\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\n\\]\nA link function\nThis connects the mean of the outcome distribution to the linear predictor.\n\nLogistic regression is a special case of a GLM.\n\n\n10.8.2 Logistic Regression in R\nFor logistic regression:\n\nThe outcome follows a binomial distribution\nThe link function is the logit\nThe model is fitted using glm() with the family argument\n\nThe basic syntax is:\n\n\n\n\n\n\nR Code\n\n\n\n\nglm(\n  formula = outcome ~ predictor,\n  family = binomial,\n  data = your_data\n)\n\n\n\nHere, family = binomial tells R two things:\n\nThe outcome is binary (or binomially distributed)\nThe logit link function should be used by default\n\n\n\n10.8.3 Comparing lm() to glm()\nIt is helpful to see how similar the two functions look:\n\n\n\n\n\n\nR Code\n\n\n\n\n# Linear regression\nlm(\n  formula = y ~ x1 + x2, \n  data = your_data\n  )\n\n# Logistic regression\nglm(\n  formula = y ~ x1 + x2,\n  data = df, \n  family = binomial\n  )\n\n\n\nThe formula syntax is identical. The key difference is that glm() requires us to specify the distribution of the outcome via the family argument.\nAlthough the syntax looks similar, the models behave very differently:\n\nlm() models the outcome directly:\n\\(Y=\\beta_0+ \\beta_1X\\)\nglm() with family = binomial models the log-odds of success:\n\\(\\text{logit}(p)=\\beta_0+ \\beta_1X\\)\n\nInternally, glm():\n\nEstimates coefficients on the log-odds scale\nUses the anti-logit function to convert fitted values back into probabilities\nGuarantees that predicted probabilities lie between 0 and 1",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#a-worked-example",
    "href": "10-Logistic-regression.html#a-worked-example",
    "title": "10  Logistic Regression",
    "section": "10.9 A worked Example",
    "text": "10.9 A worked Example\nSuppose we wanted to build a model to predict distance ran (km) based on fitness level. A study was conducted which involved 12 adults. Suppose each participant completed a fitness test where higher scores represent greater levels of fitness. Each participant was then asked to run as far as they could in one session (assume under a controlled testing environment).\n\n\nIDFitnessDistance11102323514410851310615771716820109211210141311257123017\n\n\nWe could visualise this with a scatterplot and trendline:\n\n\n\n\n\n\n\n\n\nSuppose we also had information on whether or not the participants belonged to a running squad. And instead of examining distance covered, we were predicting squad membership (where Non-squad members are denoted with 0s and Squad members are denoted with 1s):\n\n\nIDFitnessDistanceSquad Member11100232035140410805131016157071716182010092112110141311125711230171\n\n\nBecause Squad member only has 0s and 1s, out plot will look something like:\n\n\n\n\n\n\n\n\n\nAnd therefore, we can fit a logitic regression model to this data:\n\n\n\n\n\n\n\n\n\nIn R, we would use the glm() function to fit this model:\n\n\n\n\n\n\nR Code\n\n\n\n\nmodel &lt;- \n  glm(\n  formula = Squad ~ Fitness,\n  data = df_sprint2,\n  family = binomial\n)\n\nsummary(model)\n\n\nCall:\nglm(formula = Squad ~ Fitness, family = binomial, data = df_sprint2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -3.6789     2.2948  -1.603   0.1089  \nFitness       0.2528     0.1465   1.726   0.0844 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.636  on 11  degrees of freedom\nResidual deviance: 10.342  on 10  degrees of freedom\nAIC: 14.342\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n10.9.1 Making Predictions\nWe can use this output to interpret the linear predictor component in a similar way to how we have done previously (i.e. use the estimate column to determine our coefficients):\n\\[\\eta=-3.68+0.25(\\text{Fitness})\\]\nAnd we can use the properties of the logit model to calculate the probability of each person belonging to a running squad, based upon this model.\n\\[\n\\begin{aligned}\np &= \\frac{e^{\\eta}}{1 + e^{\\eta}} \\\\\n&= \\frac{e^{-3.68+0.25(\\text{Fitness})}}{1 + e^{-3.68+0.25(\\text{Fitness})}} \\\\\n\\end{aligned}\n\\]\nAnd if we apply this to our original data:\n\n\nIDFitnessDistanceSquadp111000.03123200.051351400.081410800.2355131010.394615700.5177171610.6398201000.7899211210.82810141310.4551125710.92912301710.979\n\n\nNow depending on what threshold we use, we might classify these probabilities differently (see Chapter 9). As an example, if we used a threshold of 0.50:\n\n\nIDFitnessDistanceSquadpClassification111000.0310 = Non-Squad23200.0510 = Non-Squad351400.0810 = Non-Squad410800.2350 = Non-Squad5131010.3940 = Non-Squad615700.5171 = Squad7171610.6391 = Squad8201000.7891 = Squad9211210.8281 = Squad10141310.4550 = Non-Squad1125710.9291 = Squad12301710.9791 = Squad\n\n\nOverall, the model did a pretty good job - correctly classifying all participants except ID 5 and ID 10, and would have resulted in the following confusion matrix:\n\n\n\n           Actual\nPred        Squad Non-Squad\n  Squad         4         2\n  Non-Squad     2         4\n\n\n\nIn this case:\n\nOut of 6 ‘Squad’ predictions, 4 were correct and 2 were incorrect\nOut of 6 ‘Non-Squad’ predictions, 4 were correct and 2 were incorrect\n\nThis means that the FPR and FNR would both be 0.3333, and the overall accuracy would be 0.6667.\n\n\n10.9.2 Explaining the Regression Coefficients\nIn logistic regression, the estimated coefficients do not represent changes in probability. Instead, they represent changes in the log-odds of the outcome. To interpret them meaningfully, we must connect the coefficients back to odds and odds ratios.\nRecall that logistic regression models:\n\\[\n\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\n= \\beta_0 + \\beta_1 X\n\\]\nIn our model predicting squad selection from fitness:\n\\[\n\\text{logit}(p) = -3.68 + 0.25 \\times \\text{Fitness}\n\\]\n\n10.9.2.1 The Intercept\nThe intercept estimate is:\n\\[\n\\hat{\\beta}_0 = -3.68\n\\]\nThis represents the log-odds of being selected when Fitness = 0.\nConverting this to odds:\n\\[\n\\text{Odds} = e^{-3.68} \\approx 0.025\n\\]\nThis means that, at Fitness = 0, the odds of squad selection are approximately 0.025 to 1, or about 1 selection for every 40 non-selections.\nIn practice, Fitness = 0 may not be a meaningful value, so the intercept is often best viewed as a baseline reference point rather than a quantity of direct interest.\n\n\n10.9.2.2 The Fitness Coefficient\nThe estimated coefficient for Fitness is:\n\\[\n\\hat{\\beta}_1 = 0.25\n\\]\nThis means:\n\nFor a one-unit increase in Fitness, the log-odds of squad selection increase by 0.25.\n\nBecause log-odds are not intuitive, we usually exponentiate this coefficient to obtain an odds ratio.\n\n\n10.9.2.3 Odds Ratios\nExponentiating the Fitness coefficient gives:\n\\[\n\\text{Odds ratio} = e^{0.25} \\approx 1.28\n\\]\nThis has a clear interpretation:\n\nFor each one-unit increase in Fitness, the odds of being selected for the squad are multiplied by 1.28.\n\nEquivalently:\n\nThe odds increase by 28% for every additional unit of Fitness.\n\nThis interpretation holds regardless of the current probability — odds ratios describe multiplicative changes in odds, not additive changes in probability.\n\n\n10.9.2.4 Why Probabilities Change Non-Linearly\nAlthough the coefficient implies a constant multiplicative change in odds, the corresponding change in probability depends on where you are on the logistic curve:\n\nWhen probabilities are near 0.5, small changes in Fitness lead to large changes in probability\nWhen probabilities are near 0 or 1, the same change in Fitness has a smaller effect on probability\n\nThis explains why logistic regression produces an S-shaped curve rather than a straight line.\n\n\n10.9.2.5 Statistical Significance\nThe p-value for Fitness is:\n\\[\np = 0.084\n\\]\nThis provides weak evidence that Fitness is associated with squad selection at the 10% level, but not strong evidence at the conventional 5% level. Importantly, statistical significance does not affect how the coefficient is interpreted — it only affects our confidence in the estimated relationship.\n\n\n\n10.9.3 Multiple Predictors\nLet’s have a look at what happens when we include an additional predictor to the model. Here we want to see if we can predict Squad membership based upon Fitness Score and Distance ran:\n\n\n\n\n\n\nR Code\n\n\n\n\nmodel2 &lt;- \n  glm(\n  formula = Squad ~ Fitness + Distance,\n  data = df_sprint2,\n  family = binomial\n)\n\nsummary(model2)\n\n\nCall:\nglm(formula = Squad ~ Fitness + Distance, family = binomial, \n    data = df_sprint2)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -9.9075     6.3359  -1.564    0.118  \nFitness       0.3016     0.1773   1.700    0.089 .\nDistance      0.5197     0.4155   1.251    0.211  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.6355  on 11  degrees of freedom\nResidual deviance:  7.7566  on  9  degrees of freedom\nAIC: 13.757\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nFrom the output:\n\\[\\eta=-9.91+0.30(\\text{Fitness})+0.52(\\text{Distance})\\]\nAnd therefore:\n\\[\np = \\frac{e^{-9.91+0.30(\\text{Fitness})+0.52(\\text{Distance})}}{1 + e^{-9.91+0.30(\\text{Fitness})+0.52(\\text{Distance})}}\n\\]\nIf we once again use a threshold of 0.50, the predictions become:\n\n\nIDFitnessDistanceSquadpClassification111000.0120 = Non-Squad23200.0000 = Non-Squad351400.2440 = Non-Squad410800.0600 = Non-Squad5131010.3080 = Non-Squad615700.1460 = Non-Squad7171610.9711 = Squad8201000.7841 = Squad9211210.9331 = Squad10141310.7411 = Squad1125710.7741 = Squad12301711.0001 = Squad\n\n\nThis time around, our confusion matrix is:\n\n\n\n           Actual\nPred        Squad Non-Squad\n  Squad         5         1\n  Non-Squad     1         5\n\n\n\nOverall, by adding Distance to our model:\n\nOut of 6 ‘Squad’ predictions, 5 were correct and 1 was incorrect\nOut of 6 ‘Non-Squad’ predictions, 5 were correct and 1 were incorrect\n\nThis means that the FPR and FNR would both be 0.1667, and the overall accuracy would be 0.8333 (these metrics improved from the previous model which only used Fitness as a predictor).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#practice-with-r",
    "href": "10-Logistic-regression.html#practice-with-r",
    "title": "10  Logistic Regression",
    "section": "10.10 Practice with R",
    "text": "10.10 Practice with R\nLet us use the same data from the previous Chapter (timor.RDS), but instead of running a LPM, we will construct a logistic regression.\n\n10.10.1 Download the data\nThe data set is stored in a R data format file named timor.RDS. Note: this is not an excel file, so you won’t be able to open it in Excel. Begin by downloading the file and moving it your working directory.\n\n\n\n timor.RDS\n\n\n\n\n\n10.10.2 Load the data in RStudio\nNext, let us load the data into our working environment in RStudio. Because the data file is a .RDS file, we will use the readRDS() function. In the code below, I am saving the file as ‘timor’.\n\n\n\n\n\n\nR Code\n\n\n\n\ntimor &lt;- readRDS(\"timor.RDS\")\n\n\n\n\n\n10.10.3 Load the tidyverse\nUse the library() function to load the tidyverse package. This will provide us with all of the tools we need to complete this exercise.\n\n\n\n\n\n\nR Code\n\n\n\n\nlibrary(tidyverse)\n\n\n\n\n\n10.10.4 Inspect the data\nRun the str() function to display the structure of the data frame:\n\n\n\n\n\n\nR Code\n\n\n\n\nstr(timor)\n\n'data.frame':   5916 obs. of  5 variables:\n $ poor     : num  0 1 0 0 0 1 1 0 0 0 ...\n $ hhsize   : int  7 7 8 2 4 10 5 1 6 4 ...\n $ district : chr  \"Aileu\" \"Baucau\" \"Dili\" \"Viqueque\" ...\n $ urban    : num  1 0 1 0 0 1 0 0 1 0 ...\n $ dirtfloor: num  0 0 0 1 0 0 1 1 1 1 ...\n\n\n\n\nThere are 5916 observations and five variables:\n\npoor: whether a household officially classifies as poor; 1 indicates poor, and 0 otherwise\nhhsize: household size\ndistrict: the district where a household locates\nurban: whether a household is from an urban area; 1indicates yes, and 0 otherwise (rural)\ndirtfloor: whether a household has a dirt floor; 1 indicates yes and 0 otherwise\n\n\n\n10.10.5 Training and Testing sets\nLet’s now create the model that can be used to predict whether a household is poor, based on these four characteristics.\nFirst, we will split the data into two subsets: a training set and test set. There are 5916 households in the data set. It’s common practice to randomly split your data into the training and testing sets, however in this exercise we will specify the first 70% (4141) households as training data, and the remaining 30% for testing.\nWe can use the slice() function to slice our data into parts. Remember that our data set has 5916 households. therefore:\n\nHouse 1 to House 4141 will be allocated to the training set\nHouse 4142 to House 5916 will be allocated to the testing set\n\n\n\n\n\n\n\nR Code\n\n\n\n\ntrain &lt;- timor |&gt; slice(1:4141)\ntest &lt;- timor |&gt; slice(4142:5916)\n\n\n\nTo make sure you have done this correctly, check that you have two objects (train and test) stored in your global environment.\n\n\n10.10.6 Building the model\nLast time, we fitted a LPM to this data. This time around, we will fit a logistic regression. We will run both models so that we can compare the results.\n\n\n\n\n\n\nR Code\n\n\n\n\n# lpm\ntimor_lpm &lt;- lm(poor ~ hhsize + district + urban + dirtfloor, train)\n\n# logistic\ntimor_logreg &lt;- glm(poor ~ hhsize + district + urban + dirtfloor, train, family = 'binomial')\n\n\n\nWe can now use the summary() function to inspect this model\n\n\n\n\n\n\nR Code\n\n\n\n\n\n\nCall:\nglm(formula = poor ~ hhsize + district + urban + dirtfloor, family = \"binomial\", \n    data = train)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -4.98564    0.24862 -20.053  &lt; 2e-16 ***\nhhsize            0.54931    0.02065  26.602  &lt; 2e-16 ***\ndistrictAinaro    0.29263    0.24920   1.174 0.240292    \ndistrictBaucau   -0.10712    0.23589  -0.454 0.649766    \ndistrictBobonaro  1.31676    0.23389   5.630 1.81e-08 ***\ndistrictCovalima  1.62966    0.23847   6.834 8.27e-12 ***\ndistrictDili      0.24797    0.22753   1.090 0.275788    \ndistrictErmera    0.56752    0.23570   2.408 0.016049 *  \ndistrictLautem   -0.18987    0.25665  -0.740 0.459437    \ndistrictLiqui?a   0.90086    0.24171   3.727 0.000194 ***\ndistrictManatuto  0.37061    0.25518   1.452 0.146404    \ndistrictManufahi  0.89162    0.24643   3.618 0.000297 ***\ndistrictOecussi   1.93570    0.23325   8.299  &lt; 2e-16 ***\ndistrictViqueque -0.25373    0.24754  -1.025 0.305367    \nurban            -0.77748    0.09572  -8.122 4.57e-16 ***\ndirtfloor         1.24735    0.09150  13.632  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5212.4  on 4140  degrees of freedom\nResidual deviance: 3824.8  on 4125  degrees of freedom\nAIC: 3856.8\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n10.10.7 Making predictions\nSimilar to last time, let’s use the predict() function on our two models with the test data:\n\n\n\n\n\n\nR Code\n\n\n\nNote: By default, predict() returns fitted values on the log-odds scale. Setting type = \"response\" applies the anti-logit transformation, converting log-odds into probabilities between 0 and 1.\n\n\n\n\n10.10.8 Combining Actual and Predictions\nWe can now create a data set that combines our actual values (the poor variable) with the predictions we generated in the previous step.\n\n\n\n\n\n\nR Code\n\n\n\n\nres_lpm &lt;- \n  test |&gt; \n  mutate(preds = predictions_lpm) |&gt; \n  mutate(class = ifelse(preds &gt; 0.5, \"poor\", \"not poor\")) |&gt; \n  mutate(actual = ifelse(poor == 1, \"poor\", \"not poor\"))\n\nres_logreg &lt;- \n  test |&gt; \n  mutate(preds = predictions_logreg) |&gt; \n  mutate(class = ifelse(preds &gt; 0.5, \"poor\", \"not poor\")) |&gt; \n  mutate(actual = ifelse(poor == 1, \"poor\", \"not poor\"))\n\n\n\n\n\n10.10.9 Reorder factor levels (optional)\n\n\n\n\n\n\nR Code\n\n\n\n\nres_lpm &lt;- \n  res_lpm |&gt; \n  mutate(actual = fct_relevel(actual, \"poor\", \"not poor\"),\n         class = fct_relevel(class, \"poor\", \"not poor\"))\n\nres_logreg &lt;- \n  res_logreg |&gt; \n  mutate(actual = fct_relevel(actual, \"poor\", \"not poor\"),\n         class = fct_relevel(class, \"poor\", \"not poor\"))\n\n\n\n\n\n10.10.10 Confusion Matrix\nWe can now use the table() function to create a confusion matrix of our actual and predicted classifications:\n\n\n\n\n\n\nR Code\n\n\n\n\ntable(Prediction = res_lpm$class, Actual = res_lpm$actual)\n\n          Actual\nPrediction poor not poor\n  poor      287      128\n  not poor  313     1047\n\n\n\ntable(Prediction = res_logreg$class, Actual = res_logreg$actual)\n\n          Actual\nPrediction poor not poor\n  poor      294      133\n  not poor  306     1042\n\n\n\n\nFrom our output above, we can calculate the False Positive rate for both models are:\n\\[FPR_{LPM}=\\frac{128}{128+1047}=0.1089\\]\n\\[FPR_{LogReg}=\\frac{133}{133+1042}=0.1132\\]\nLikewise, the False Negative rates are:\n\\[FPR_{LPM}=\\frac{313}{313+287}=0.5217\\]\n\\[FPR_{LogReg}=\\frac{306}{306+294}=0.51\\]\nHere, the logistic regression model had a lower false negative rate (FNR) compared to the linear probability model (0.51 versus 0.52). However, it also exhibited a higher false positive rate (FPR) (0.113 versus 0.109). With that said, the preference would still be to use the logistic regression model because it is theoretically appropriate for binary outcomes, guarantees valid predicted probabilities between 0 and 1, and models the relationship between predictors and the outcome in a statistically principled way. The differences in FPR and FNR are also dependent on the chosen classification threshold; by adjusting this threshold, logistic regression allows practitioners to explicitly trade off between false positives and false negatives in a way that the linear probability model does not naturally support. As a result, even if performance metrics are similar or mixed at a single threshold, logistic regression provides a more flexible and reliable framework for both prediction and inference in classification problems.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#summary",
    "href": "10-Logistic-regression.html#summary",
    "title": "10  Logistic Regression",
    "section": "10.11 Summary",
    "text": "10.11 Summary\nIn this chapter, we introduced logistic regression as a modelling approach designed specifically for binary outcomes (such as win/loss, pass/fail, or squad/non-squad). We began by revisiting the linear probability model (LPM) and highlighting why it can be problematic for classification tasks: linear models can produce predicted values below 0 or above 1, and the assumed linear relationship between predictors and probabilities is often unrealistic. Logistic regression resolves these issues by ensuring predicted probabilities always remain in the valid range, while still allowing us to build a model using familiar regression ideas.\nThe key shift was moving from probabilities to odds and then to log-odds (logits). Odds compare the chance of success to the chance of failure, and the logit transformation maps probabilities from the bounded interval \\((0,1)\\) to the entire real line \\((-\\infty, +\\infty)\\). This is crucial because it allows us to write a regression-style equation on the log-odds scale, while the anti-logit (inverse logit) converts model predictions back into probabilities. Graphically, this produces the familiar S-shaped logistic curve, which captures an important real-world feature: changes in predictors tend to have the biggest impact on probability when the outcome is uncertain (around \\(p=0.5\\)), and smaller impact when the outcome is already very unlikely or very likely.\nWe also linked logistic regression to its probabilistic foundation: the binomial distribution. Each observation in a logistic regression model can be viewed as a Bernoulli trial (0 or 1), and the model estimates the probability of success for each observation. This framework provides a principled way to model classification problems, and it explains why logistic regression is so widely used in business, health, economics, and data science.\nFinally, we extended these ideas to R. While we used lm() for linear regression, logistic regression is fitted using glm() with family = binomial. In our sprint example, we used the fitted model to generate predicted probabilities, classify individuals using a probability threshold, and evaluate model performance using a confusion matrix. We also introduced coefficient interpretation: logistic regression coefficients describe changes in log-odds, and exponentiating them gives odds ratios, which describe multiplicative changes in odds. In the next chapter, we will build on this foundation by looking more closely at prediction quality, threshold choice, and how to assess and compare classification models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-Logistic-regression.html#exercises",
    "href": "10-Logistic-regression.html#exercises",
    "title": "10  Logistic Regression",
    "section": "10.12 Exercises",
    "text": "10.12 Exercises\n\n\n\n\n\n\nQuestion 1\n\n\n\nSuppose we are modelling whether a student passes an exam (1 = pass, 0 = fail).\n\nExplain two reasons why a linear probability model (LPM) fitted using lm() is not ideal for this task.\nBriefly explain how logistic regression addresses these issues.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\n\nTwo key problems with the linear probability model are:\n\nInvalid predicted probabilities: Linear regression can produce predicted values below 0 or above 1, which are not meaningful probabilities.\nUnrealistic functional form: The LPM assumes a linear relationship between predictors and probability, even though probability changes are often non-linear (e.g., diminishing returns at high or low probabilities).\n\nLogistic regression addresses these issues by:\n\nModelling the log-odds of the outcome as a linear function of predictors, rather than modelling probability directly.\nUsing the anti-logit transformation, which guarantees that predicted probabilities always lie between 0 and 1 and naturally produces an S-shaped (logistic) curve.\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nA basketball player has a probability of making a free throw of \\(p=0.75\\).\n\nCalculate and interpret the odds of making the free throw.\nCalculate the log-odds (logit).\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\n\\[Odds=\\frac{p}{1-p}=\\frac{0.75}{0.25}=3\\]\nThe odds are 3 to 1 in favour of success (making a free throw).\nb.\n\\[\\text{logit}(p)=ln(Odds)=ln(3)=1.10\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nConsider the logistic regression model:\n\\[\\text{logit}(p)=-2.5+0.4X\\]\n\nInterpret the coefficient 0.4 in terms of log-odds.\nConvert this coefficient to an odds ratio and interpret it.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\nThe coefficient 0.4 means that for a one-unit increase in X, the log-odds of success increase by 0.4.\nb.\nThe odds ratio is \\(e^{0.4}=1.49\\). For each one-unit increase in X, the odds of success are multiplied by approximately 1.49 (an increase of about 49%).\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nA logistic regression model for squad selection is:\n\\[\\text{logit}(p)=-3+0.5(Fitness)\\]\n\nCalculate the probability of selection when Fitness = 6.\nWould this individual be classified as a squad member if a threshold of 0.4 were used?\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\nFirst calculate the linear predictor:\n\\[\\eta=-3+0.5(6)=0\\]\nConvert to probability using the anti-logit:\n\\[p=\\frac{e^0}{1+e^0}=\\frac{1}{2}=0.5\\]\nb.\nUsing a threshold of 0.4, this individual would be classified as a squad member, since \\(p \\ge0.4\\)\n\n\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nYou are given a data frame df containing a binary outcome Pass (0 = fail, 1 = pass) and a predictor Hours.\n\nWrite the R code to fit a logistic regression model predicting Pass from Hours.\nWrite the R code to obtain predicted probabilities from this model.\nExplain why type = “response” is needed.\n\n\n\n\n\n\n\nClick for Solutions\n\n\n\n\n\na.\n\nmodel &lt;- glm(\n  Pass ~ Hours,\n  data = df,\n  family = binomial\n)\n\nb.\n\npredict(model, type = \"response\")\n\nc.\nBy default, predict() returns fitted values on the log-odds scale. Setting type = “response” applies the anti-logit transformation, converting log-odds into probabilities between 0 and 1.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  }
]